[ { "title": "Understanding the CSS Cascade&#58; How Styles are Applied in Web Development", "url": "/posts/understanding-the-css-cascade/", "categories": "web development", "tags": "CSS, cascade, specificity", "date": "2023-10-20 00:00:00 +0200", "snippet": "Cascading Style Sheets (CSS) is a fundamental technology used to style the layout and appearance of web pages. One of the core concepts that developers need to understand when working with CSS is the CSS cascade. The cascade refers to the mechanism by which the browser determines which CSS rules to apply when multiple conflicting rules are defined. This process is central to ensuring that your web pages are styled correctly and consistently.In this article, we’ll explore what the CSS cascade is, how it works, and how developers can use it effectively in their web development projects.What is the CSS Cascade?The CSS cascade is essentially the set of rules that determines the final styles applied to an element on a web page when multiple CSS rules target the same element. When more than one rule affects the same element, CSS has to decide which rule should take precedence. This decision is based on the order of the rules, their specificity, and whether the rule is marked as important.There are three primary factors that influence the cascade: Source Order (the order in which styles are defined in the CSS). Specificity (the weight of a selector that targets an element). Importance (rules marked with !important override other styles).1. Source OrderCSS rules are applied in the order in which they appear in the stylesheet, or in the order they are linked in the HTML document. If two rules have the same specificity, the rule that appears last in the code will take precedence.For example, if we have the following two CSS rules: p { color: blue; } p { color: red; }The text color of all &lt;p&gt; elements will be red because the second rule appears after the first one. The later rule overrides the previous one due to its position in the source code.2. SpecificitySpecificity determines the “weight” of a CSS selector. It is a measure of how specific a rule is in selecting an element. The more specific a selector is, the higher its specificity value, and thus, the more likely it is to be applied when there are conflicting styles.CSS specificity is calculated based on a few different factors, primarily focusing on how the selector targets the element. The specificity hierarchy is as follows: Inline styles: style=”…” — highest specificity. IDs: #id — second highest. Classes, pseudo-classes: .class, :hover — lower specificity. Elements and pseudo-elements: div, h1, ::after — lowest specificity.Here’s an example:/* Element selector (lowest specificity) */p { color: blue;}/* Class selector */.text-blue { color: red;}/* ID selector (highest specificity) */#main-text { color: green;}Given the following HTML: &lt;p class=\"text-blue\" id=\"main-text\"&gt;Hello, world!&lt;/p&gt;The ID selector #main-text will be applied first, as it has the highest specificity, so the text will be green, overriding the class and element styles.3. Importance (!important)The !important declaration is a special flag that forces a CSS rule to take precedence over other conflicting rules, regardless of specificity or source order. While it can be useful for applying critical styles, it should be used sparingly, as it can make your code harder to maintain and debug.For example:p { color: blue !important;}p { color: red;}In this case, even though the second rule specifies red, the !important declaration ensures that the blue color is applied, since it has the highest priority.However, using !important too frequently can lead to problems, such as difficulty overriding styles later in the code. It is often better to adjust the specificity or source order of your selectors rather than relying on !important.The Cascade in ActionLet’s consider a more complex example to see how the cascade works in real-world scenarios. Suppose you have the following CSS:/* Global rule */h1 { color: black;}/* Class rule */h1.header { color: blue;}/* ID rule */#special-header { color: red;}/* Inline style HTML&lt;h1 id=\"special-header\" class=\"header\" style=\"color: yellow;\"&gt;Hello, world!&lt;/h1&gt;*/In this case: The global rule applies to all h1 elements, setting their color to black. The class rule is more specific and overrides the global rule, setting the color of .header to blue. The ID rule has a higher specificity than the class, so it overrides the color to red. Finally, the inline style is the most specific and will override all previous styles, setting the color to yellow.So, the text will appear yellow because the inline style has the highest specificity and is applied last.Best Practices for Working with the CSS Cascade Minimize the Use of !important: While !important can solve conflicts, overusing it can lead to maintenance headaches. Try to rely on source order and specificity to control the cascade instead. Use Specific Selectors Thoughtfully: Be mindful of how you select elements. Avoid overly broad selectors that could be unintentionally overridden. Instead, use specific class and ID selectors when possible. Organize Your CSS: Keep your CSS well-structured. Group related styles together, and order your CSS rules logically. For example, you can place more general styles at the top and override them with more specific styles further down. Use the Developer Tools: Modern web browsers have excellent developer tools that allow you to inspect the applied styles on elements. This can be helpful for understanding how the cascade is affecting the rendering of your page.ConclusionThe CSS cascade is a powerful concept that governs how styles are applied to HTML elements when multiple conflicting rules are present. By understanding the factors that influence the cascade—source order, specificity, and importance—web developers can create more efficient, maintainable, and predictable stylesheets. Being mindful of the cascade and using it effectively ensures that your styles are applied in the way you intend and that your code remains clean and easy to manage." }, { "title": "Navigating tomorrow - How AI Will Reshape the Job Landscape in the Next 5 Years", "url": "/posts/How-AI-Will-Reshape-the-Job-Landscape-in-the-Next-5-Years/", "categories": "AI, job market", "tags": "AI, jobs", "date": "2023-10-10 00:00:00 +0200", "snippet": "The accelerated advancement of artificial intelligence (AI) has become an influential force shaping our world, particularly in the realm of employment. Over the next five years, the employment market is poised for a transformation that will be significantly driven by AI. From revolutionizing industries to redefining job roles, these changes are set to bring both opportunities and challenges.Automation and Job TransformationAI’s ability to automate routine tasks is already redefining industries. Repetitive jobs in manufacturing, data entry, and customer service are being automated, leading to concerns about job displacement. However, as some roles become obsolete, new opportunities emerge. For instance, AI necessitates the creation of positions focused on AI development, maintenance, and oversight. Human-AI collaboration will become the new norm, with individuals working alongside AI systems to enhance productivity and decision-making.Skill Evolution and Lifelong LearningThe demand for technical skills in data analysis, machine learning, and programming will soar. However, it’s not just technical skills that will be sought after. Soft skills like creativity, emotional intelligence, adaptability, and critical thinking will be invaluable, as they are areas where AI currently lacks prowess. Lifelong learning will become a necessity, as individuals will need to continually upskill and adapt to stay relevant in the evolving job landscape.New Professions and IndustriesThe rise of AI will spawn new professions and industries. AI ethicists, responsible for ensuring the moral and fair use of AI, will be in high demand. Cybersecurity experts will be essential to protect AI systems from potential threats. Healthcare, finance, agriculture, and education are just a few sectors that will experience significant AI-driven changes, creating opportunities for those with expertise in both AI and these fields.Remote Work and FlexibilityThe pandemic accelerated the adoption of remote work, and AI will further facilitate this trend. AI-powered tools and platforms will enable effective collaboration and productivity, making remote work more seamless. As a result, geographical boundaries will become less of a barrier, allowing for more flexible work arrangements and the potential for a more global job market.Challenges and Ethical ConcernsWith the rapid integration of AI, concerns about job displacement, bias in algorithms, and the ethical use of AI will continue to persist. Ensuring a balance between technological advancement and the well-being of the workforce will be a significant challenge. Policymakers, organizations, and educational institutions will need to collaborate to address these challenges and create frameworks that safeguard against negative consequences.ConclusionThe next five years will witness a pivotal transformation in the job market as AI continues to permeate various sectors. While some job roles will diminish, many new opportunities will arise, necessitating a shift in skills and approaches to work. Embracing these changes with a focus on continuous learning and adaptation will be crucial in navigating this evolving landscape.In this era of AI, the ability to blend technological expertise with uniquely human skills will define success in the job market. It’s not just about adapting to AI; it’s about shaping it responsibly and leveraging its potential to create a more innovative, efficient, and inclusive job market." }, { "title": "Review of the book - \"Learning Python\"", "url": "/posts/review-book-learning-python/", "categories": "books, python", "tags": "python, books", "date": "2023-04-04 00:00:00 +0200", "snippet": "During my initial familiarization with the Python and computing in general, I tried obtaining knowledge from multiple sources. Yet I always preferred books as the main knowledge source so hereby I present a short review of one of the most comprehensive books written on Python which I bought five years ago.“Learning Python” is a thick, heavy reference book which is mostly oriented towards advanced users. With a length of over a 1500 pages, it is a bit overwhelming to beginners as it dives deeply into the matter, explaining fine grains not only of the Python itself, but often deals with broader concepts in the computing science as well (as opposed to the book I previously reviewed ). It can be considered as a textbook too as it could be read from the start to beginning, but as this is a book on programming with a myriad of concepts that are best understood, memorized and utilized through actual usage (i.e. experimenting with the code), it is better suited as a reference book. It is definitively not a workbook.Skimming through contents reveals the structure of the syllabus: after a light introductory text on the Python itself, including history, installation and IDE, the focus goes to the Python types and supported operations. This chapter deals with the most important data structures and introduces the dynamic typing to the user. I read multiple programming books (besides on Python mostly on Java and C#) and this book provides one of the best explanations of many general programming concepts like conditional statements, functions and function scopes and even some parts of object-oriented programming. This could partially be explained due to the fact that the Python is significantly lighter in syntax and sometimes features than aforementioned programming languages, but on a lower level, reader really needs to understand more complex topics and often the author gives the concepts comparison with other languages along with the description of its function in Python, for instance take a look at those excerpts: “Readers with a background in C may find Python references similar to C pointers (memory addresses). In fact, references are implemented as pointers, and they often serve the same roles, especially with objects that can be changed in place (more on this later). However, because references are always automatically dereferenced when used, you can never actually do anything useful with a reference itself; this is a feature that eliminates a vast category of C bugs. But you can think of Python references as C “void*” pointers,which are automatically followed whenever used.” “If you’ve ever used C++ or Java, you’ll recognize that Python’s self is the same as the this pointer, but self is always explicit in both headers and bodies of Python methods to make attribute accesses more obvious: a name has fewer possible meanings.” “Like the unified try, chained exceptions are similar to utility in other languages (including Java and C#) though it’s not clear which languages were borrowers. In Python, it’s a still somewhat obscure extension, so we’ll defer to Python’s manuals for more details.” What I like is that the material is presented in logical order, starting from the basics and then going up to more complex matters. The book deals with the OOP only in the second half, which isn’t always the case in some other readings where the user is immediately bombarded with it. Note that I don’t have a thing against the OOP but as an advanced concept its comprehension should probably need be somewhat postponed and not rushed nor imposed onto the innocent reader 😊 The OOP chapter consists of some 300 pages which comprises of one fifth of the book and the content is presented in a detailed and resourceful manner. Still, I would like that the chapter on exceptions is presented before OOP, but if one sees this book as a reference rather than a textbook, the ordering of the chapters isn’t important and isn’t linear so my remark in that sense could be discarded.Chapters on functions, modules and exceptions are great too – they provide practical and theoretical explanations on modular software development through a Python’s prism. Yet they are very detailed with detailed overviews of those topics and advanced programmers will benefit a lot from those materials. The book concludes with advanced topics like byte and Unicode strings, decorators, metaclasses and some other interesting and useful things.Important notice is that the book is now a decade old, written in 2013. That is a long time in computing. Although one could say that the Python now is more popular and widely used than back then, there were multiple changes in Python itself and there are some complex issues stemming from the version change issues which author covers here. Yet, I believe this is still by far the most comprehensive book on Python and as such still very relevant. It is probably best suited to sit on the shelf next to the programmer’s desk along with some good workbook.This book definitively has stood the test of time. As Python today is one of the world’s most used programming languages (many sources put it into the top 5), I am convinced that “Learning Python” will (continue to) find its place among the users and that the author will eventually delight the numerous readers with the updated version of the text." }, { "title": "Review of the book - \"A Smarter Way To Learn Python\"", "url": "/posts/review-of-book-learn-python/", "categories": "books, python", "tags": "python, books", "date": "2023-02-23 00:00:00 +0100", "snippet": "If you are keen to learn programming, there are multiple learning paths available, as well as various tools that can be used in that regard. Regardless of your previous education and knowledge that you may possess, there is a myriad of dedicated learning websites, courses, mentorships, tuitions and books. In this article, we’ll assume that you are newbie with no programming experience interested in Python and wanting to learn from the book.“A Smarter Way To Learn Python” is an introductory Python book, comprehensible to the total beginners with no previous experience in the field whatsoever. It can be used by more seasoned programmers too as a quick reference to some simpler topics. The concept author used in the book is a presentation of various topics in the form of lessons followed by numerous online exercises that should solidify the obtained knowledge.ContentsIn total, there are 77 lessons or chapters as the author calls them, which could be grouped into following segments: introductory topics (14 chapters), including variables, basic expressions and if statements, data structures and miscellaneous related stuff, which can be further subdivided to lists (5 chapters); tuples (1 chapter); dictionaries (16 chapters); for loops (2 chapters); while loops (2 chapters), functions (10 chapters), classes and OOP (9 chapters), working with external data (15 chapters), including CSV and JSON files, debugging and general information topics (3 chapters).In addition, through 5 appendixes, some preparatory topics like Python installation, running and saving programs are explained too.The book comes under 240 pages and it can be quickly consumed, depending on one’s particular workload, in probably 2 weeks time together with the included exercises.What will you learn from the book are the basic concepts; you will get information about how things as done, such as taking input from user, concantenating variables, grouping code via functions and classes etc., especially in a syntactical regard. Presented information is a bare minimum to get started with the programming in Python. The explanations are sufficient to get a grasp on how things may be done. Yet, as one progresses, the questions will inevitably arise, as the book follows linear learning pattern and it does not discuss much out of the lesson’s scope. For instance, the reader may ask himself why use CSV when there is a dictionary and vice versa; could the OOP be replaced and/or supplemented with functions; and finally; what could one do with the obtained knowledge presented in the book. While it is understandable that it is impossible to dive into any deeper discussion in such a brief book, there is complete absence of any advices and recommendations when further learning path is concerned. Yet, many other programming books also fail in this domain.Examples of the exercisesThere are no projects in this book, which is also to expect. Instead, through the exercises after the each lesson, the user is pointed to the book’s accompanying website. There are numerous quiz-like exercises where user can type simpler commands and the page checks its validity. There are no review exercises so every exercise is mapped to the particular chapter in the book. Some examples of the questions are, taken in a random order: What is the keyword for adding a new element to the end of a list? Eliminate the element with an index of 2 from a list whose name is products. Rewrite this so you can make changes to the elements. sizes = (\"sm\", \"med\", \"lg\", \"xl\") Targeting the correct element in the following list, assign “fi” to the variable giant_syllable. giant_speak = [\"fee\", \"fi\", \"fo\", \"fum\"] Delete a dictionary item whose key is an integer. Make everything up. In the following code, what is the value of total if the function call doesn’t pass any arguments? def add_numbers(first_number = 2, second_number = 4): total = first_number + second_number This is the first line of a function definition. When optional arguments are passed, they’re assigned to x, which is a ____. def fill_dictionary(new_customer, first_name, last_name, **x): Code the first two lines of a function. Line 2 begins the definition of a class. Make everything up. Create an instance of this class. The name of the instance is geneva. The value is 144. class Lake(): def __init__(self, depth): self.depth = depth Complete the fourth line. 1 with open(\"cats.csv\") as f: 2 cat_content = csv.reader(f) 3 cats_list = [] 4 for each_line in ___________ In the end, after the book’s completion the reader will have a fair comprehension of basic Python’s features. From that point there are two possible learning paths: The user may continue to consume similar material, which puts emphasize on the syntax rather on underlying logic. That can lead to After this book, more advanced material may be consulted, which will provide answers not only to how, but to why as well. This route is longer and harder but it enables understanding of the fundamentals of programming which are not related to just one programming language. ConclusionThe “Smarter Way To Learn Python” is a decent read, which will fairly quickly put a complete beginner into a position of understanding basic concepts of coding in Python, and as such is worth of studying." }, { "title": "HP Notebook 15 review – computing on a budget", "url": "/posts/hp-notebook-15-review/", "categories": "hardware, laptop", "tags": "hardware, hp notebook", "date": "2023-02-15 00:00:00 +0100", "snippet": "Some 18 months ago I got as a present a 15,6-inch HP laptop. Didn’t need a laptop, but an acquaintance of mine provided me with a new machine for some service I provided to him. Since I do vast majority of my computing on a desktop PCs (one exemption to that being that stubborn little Thinkpad), I was curious how far a brand-new laptop can go in terms of performance, portability, connectivity and overall user experience. HP Notebook 15 gen. 2021SpecificationsSpecs-wise, I was surprised when realized that the machine sports an AMD-processor. To be precise, it is an AMD Ryzen 5 3500U quad-core with HT (8 threads) with discrete GPU Radeon Vega 8, with 15W TDP and 12nm lithography. That puts it somewhere in the lower-middle place on the market reserved for ultrabooks, mini PCs and budget laptops. There is 8 GB DDR4-2400 SDRAM (one piece, hence one-channel) and main storage 256 GB PCIe NVMe M.2 SSD. Sounds decent, although a bit tight. Given the tendency of soldering everything to the motherboard, I expected little-to-none upgrades possible; out of curiosity I immediately voided warranty (could afford being reckless as I have plenty of alternative PCs lying around) and unscrewed the back cover of machine to see the internals:The RAM memory is NOT soldered and there are two slots, according to specs sheet max. supported amount is 16 GB (but the machine could probably take 32 GB as well given the chipset supports it); furthermore, there is space for 2.5” device (HDD/SSD) but that comes with some complications, later about that. I was hoping to find at least one internal USB connector (to put an USB mouse/keyboard controller into), but that was unjustified – there is no such thing inside.Design, build quality, portsThe machine is not an ultrabook, but it is reasonably thin, so some scarcity of ports is expected. I supposed there will be an USB-C, but it is somehow missing (meaning no charging via phone charger and no video output through it), but there are 3 USB ordinary ports (type A), of which two are 3.1, third is 2.0 (all support only data transfer, no charging). Which means, laptop can’t charge other devices through USB and it can’t be charged through it. There is standard LAN port which is a fine addition (usually missing on newer laptops), a HDMI a/v out (standard size, not mini), usual headphone/mic combo connector and SD card reader. Kensington lock is present as well, should you need it. Basics are covered, nothing more, nothing less.BatteryOne Li-ion 3-cell 41 Wh battery powers the laptop, I thought it would be bad, but given the efficiency of components, the battery life is surprisingly good – depending on usage, one could get the duration of typical working day without topping-up. The charging takes about 2 hours. There is no space for second battery as on some models.Screen is 15.6” Full-HD (1920*1080) 16:9 anti-glare IPS panel which looks pretty good; the images are reasonably sharp and viewing-angles too. It sports a decent pixel density of 142 ppi. I haven’t tested the gamut and color profiling as I am not into that stuff. It can be a complete desktop-replacement solution even during the longer working times and is pleasant to look at.Input devicesKeyboard is decent, feedback is satisfying given the 2 cm height of the typing board (whole laptop is 2.25 cm but few millimeters go to the top cover which contains the screen). To my satisfaction, the dedicated numerical keyboard is present so you get almost a full-keyboard-alike experience, albeit there are some drawbacks, namely the usual placement of some non-essential keys is displaced all around (ins/del, home/end, pgup and pgdown keys as well as f-keys) – missing IBM/Lenovo classic keyboard. Yet, keyboard is not only satisfying, but actually good – could work long-term on it.Now the biggest drawback: the touchpad itself and especially buttons are REALLY bad. The touchpad feels somewhat clumsy and not precise enough but is usable; however the touchpad keys are so hard to press that the user must mentally prepare to apply additional force every time the mouse key need to be pressed which ruins whole user-experience BIG TIME. Fast solution would be to simply hook up the external mouse and to avoid touchpad buttons whenever possible.StorageThe system supports the NVMe SSD and 256 GB unit is installed. The disk benchmark immediately shows very slow transfer rates which indicate an issue of some kind. Apparently, although the disk is PCIE 3.0 x4 capable, the data bus is somehow capped to the only PCIE 1.0 x2, limiting the data transfer to 500 MB/s (which is basically only one-quarter to the max theoretical read and one-third of the write speed).The BIOS isn’t configurable so the user can’t change anything of importance in it. This is a serious drawback and puts HP on the throne of shame, as the disk transfer rates of such kind are now 15 years old.Going further, 8 GB of RAM is on the lower edge, especially given the fact that the GPU RAM takes 2 GB of it, so user has only the 6 GB at disposal. For the light, office use, browsing and similar tasks it’ll do the job, however, forget anything more demanding. Expansion to the 16 GB will prove a bit difficult, since one should buy the same memory module which is impossible to find on the market (two same modules would avoid eventual memory issues), so the most probable solution would be acquiring a 2*8 GB modules of DDR4 laptop RAM, which amount to 10% of the laptop’s worth. There is a possibility of replacing SSD as the faster ones are readily available, and, technically, it is possible to add another 2.5” HDD/SSD, however HP complicated things here and didn’t provide the connector which must be manually bought and installed. To be precise, three parts are actually needed: Hard drive bracket, part number L20455-001 (here you could also use some strong double-sided foamy tape instead) Hard drive connector board, part number L20454-001 Hard drive/solid-state drive cable, part number L20456-001 NVMe can also be upgraded to a larger one, which is probably best option when it comes to storage expansion.PerformanceAs usual, main CPU is soldered and not replaceable so you’re stuck with what you initially obtained in that regard. Even if the RAM is maxed-out, the CPU will probably remain a constraint if heavier software load runs on this machine. I compared the performance of rather old desktop i7 22-nm 3770 CPU with this 3500u Ryzen, both being 4-core, 8-threads, and the i7 just crushes it in terms of everything – it may be not a fair comparison given the different market segments but between those chips is 7 years-difference in favor of that Ryzen – Moore’s Law is stuck in rut that’s for sure. Still no replacement for the displacement.Thermals and miscellaneous devicesThe machine internal heat management is good – there is no significant heating and the thing stays reasonably cool for a prolonged period of time. The fan runs reasonably quiet and is tolerable even when on full speed. I have no complaints in terms of peripherals and miscellaneous equipment – speakers, webcam and connectivity work as supposed. Nice thing is that the wireless is 802.11 ac meaning 5 GHz networks are supported. Weighing 1.8 kg (charger excluded), it is somewhat heavier than ultrabooks – but if you want 0,3-0,4 kg weight reduce with the same specs prepare to pay at least twice as much as this laptop costs! The weight is manageable though, it is significantly lower than 2 kg and it is well balanced over the laptop’s area. I measured 1,75 kg without and 2,05 kg with the charger, respectively. The build quality is decent, there are occasional, rare squeaks which is expected given the thin plastic’s body which seems durable.Portability is average: ideally, you’d want a 14” laptop or less when carrying around, but the additional screen-estate you get with this display is worth if you want portable desktop replacement. It will fit in most backpacks though.ConclusionAlthough it is a decent machine in most departments, even if it would have had a full-speed NVMe drive I wouldn’t have bought it for the single reason that it does not have USB-C (meaning, you can ditch the original charger and just use your phone’s one to charge it – one thing less in the backpack!). Another thing is crappy touchpad which means that an average user will probably have to carry around external mouse. Third thing, if you are constantly on the move, you’d be better-off with a lower-screened laptop which you can hook-up to larger external screen if needed. If you can live with those things, or if you don’t require much portability, this machine is a solid choice especially if it sits on the desk and doesn’t move often.Update 16th of February 2023:A BIOS update has solved the issue with the SSD speed connectivity:" }, { "title": "C# Mini Project - Library Management System program detailed overview", "url": "/posts/library-management-system/", "categories": "c#, sql server", "tags": "c#, databases, windows forms, visual studio", "date": "2022-11-22 00:00:00 +0100", "snippet": "In this mini project, we will be creating a program in C# which serves as a Library Management System, enabling the basic functions of one such system. In particular, there are separate records of a books, books’ categories, authors, publishers, library members, as well as the records on book issues and returns. It is in fact a Windows Forms GUI program connected to the underlying database in MS SQL Server which uses similar logic as the Car Rental program.Database overviewThe underlying database consists of 7 tables, as follows: author contains data on authors of the books, book contains books’ records, category holds the books categories, issuebook records the issuance of the books, member contains the library’s members data, publisher saves the information on publishers, returnbook records the return of the previously issued books.Structure of the programThe program holds a number of .cs files, each for the previously created database tables, respectively. As usual, startup Program.cs runs Main.cs which is the main selection menu of the program.Category.cs, Author.cs, Publisher.cs, Book.cs, Member.csThose classes are similar to each other, in fact, in terms of both GUI elements and underlying code they consist of the same elements. In particular, they have in common DataGridView, ComboBox, TextBox elements and corresponding buttons for adding/updating the data. On the coding side, they all connect to the database, displaying its contents in aforementioned Windows Forms elements. The database can be updated as well. The database connectivity is ensured through the following code:SqlConnection con = new SqlConnection(\"Data source=DESKTOP-N5IF2SJ\\\\SQLEXPRESS; Initial Catalog=slibrary; Integrated Security=true;\");As in the Car Rental example, the classes itself are consisting of multiple methods which eliminate the need for multiple-code usage. In particular, those methods are load(), getid(string id). Additionally, there are two click events which define the action on user’s button clicks. Once is one class created with such a structure and tested, creating other classes is a matter of a work in Visual Studio and repeating the same code within the other classes, paying respect to their particular contents, GUI and database elements.Connection string:SqlConnection con = new SqlConnection(\"Data source=DESKTOP-N5IF2SJ\\\\SQLEXPRESS; Initial Catalog=slibrary; Integrated Security=true;\");load() method:public void load() { sql = \"select * from category\"; cmd = new SqlCommand(sql, con); con.Open(); dr = cmd.ExecuteReader(); dataGridView1.Rows.Clear(); while(dr.Read()) { dataGridView1.Rows.Add(dr[0], dr[1], dr[2]); } con.Close(); }getid(string id) method:public void getid(string id) { sql = \"select * from category where id= '\" + id + \"'\"; cmd = new SqlCommand(sql, con); con.Open(); dr = cmd.ExecuteReader(); while(dr.Read()) { txtname.Text = dr[1].ToString(); txtstatus.Text = dr[2].ToString(); } con.Close(); }button1 click event:private void button1_Click(object sender, EventArgs e) { string catname = txtname.Text; string status = txtstatus.SelectedItem.ToString(); id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); if (Mode==true) { sql = \"insert into category(catname,status) values(@catname, @status)\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@catname\", catname); cmd.Parameters.AddWithValue(\"@status\", status); cmd.ExecuteNonQuery(); MessageBox.Show(\"Category created\"); txtname.Clear(); //txtstatus.Items.Clear(); txtstatus.SelectedIndex = -1; txtname.Focus(); } else { sql = \"update category set catname=@catname, status=@status where id=@id\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@catname\", catname); cmd.Parameters.AddWithValue(\"@status\", status); cmd.Parameters.AddWithValue(\"@id\", id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Category updated\"); txtname.Clear(); //txtstatus.Items.Clear(); txtstatus.SelectedIndex = -1; txtname.Focus(); } }datagridview click event:private void dataGridView1_CellContentClick(object sender, DataGridViewCellEventArgs e) { if(e.ColumnIndex==dataGridView1.Columns[\"edit\"].Index &amp;&amp; e.RowIndex&gt;=0) { Mode = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); getid(id); } else if (e.ColumnIndex == dataGridView1.Columns[\"delete\"].Index &amp;&amp; e.RowIndex &gt;= 0) { Mode = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); sql = \"delete from category where id=@id\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@id\", id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Category deleted\"); txtname.Clear(); //txtstatus.Items.Clear(); txtstatus.SelectedIndex = -1; txtname.Focus(); con.Close(); } }LendBook.cs, ReturnBook.csThe class lendbook.cs is similar to previous, the new elements are issuing and return date elements. Nothing is computed here but passed to the database. The calculation of the days is done in the returnbook.cs class: similar to the Car Rental project, the calculation is done in SQL and passed back to the program:cmd = new SqlCommand(\"select book,issuedate,returndate,DATEDIFF(dd,returndate,GETDATE())as elap from issuebook where memberid='\"+ txtmid.Text+ \"'\",con);ConclusionThe presented program takes into consideration records of authors, books, books categories, members and publishers, and records the books’ issuing and return date. Upon return, it calculates the number of passed days, and if it greater than allowed, calculates the fine." }, { "title": "Adding functionality to Windows Sandbox", "url": "/posts/adding-functionality-windows-sandbox/", "categories": "windows, sandbox", "tags": "operating systems, virtual machines", "date": "2022-11-18 00:00:00 +0100", "snippet": "Adding functionality to Windows SandboxWindows Sandbox is a fast, functional and effective virtual environment introduced in 2019. It is mostly delivered as-is, albeit Microsoft added some configurability in 2020, with Windows 10 build 18342 and newer, or Windows 11.Configuration filesConfiguration files of Windows Sandbox have .wsb file extension, they are formatted as XML and enable control of the following sections: Audio input: Shares the host’s microphone input into the sandbox. Clipboard redirection: Shares the host clipboard with the sandbox so that text and files can be pasted back and forth. Logon command: A command that’s executed when Windows Sandbox starts. Mapped folders: Share folders from the host with read or write permissions. Exposing host directories may allow malicious software to affect the system or steal data. Memory in MB: The amount of memory, in megabytes, to assign to the sandbox. Networking: Enable or disable network access within the sandbox. Printer redirection: Shares printers from the host into the sandbox. Protected client: Places increased security settings on the RDP session to the sandbox. vGPU (virtualized GPU): Enable or disable the virtualized GPU. If vGPU is disabled, the sandbox will use Windows Advanced Rasterization Platform (WARP). Video input: Shares the host’s webcam input into the sandbox.Creating Shared FoldersAmong the customizable options, most useful one is probably the possibility of direct access to the folders on the host within the Windows Sandbox itself. Basically it is a matter of creating a textual file with extension named as .wsb with following content:&lt;Configuration&gt;\t&lt;MappedFolders&gt;\t &lt;MappedFolder&gt; \t\t&lt;HostFolder&gt;absolute path to the host folder&lt;/HostFolder&gt; \t\t&lt;SandboxFolder&gt;absolute path to the sandbox folder&lt;/SandboxFolder&gt; \t\t&lt;ReadOnly&gt;value&lt;/ReadOnly&gt; \t &lt;/MappedFolder&gt;\t&lt;/MappedFolders&gt;&lt;/Configuration&gt;After that, just run the created file and in desktop on virtual machine you’ll find the mounted shared folder. There are some additional options which are presented in detail here.There is possibility to configure some settings in Windows Sandbox by using other means like adding the files manually and editing the registry, as described here.ConclusionAlthough it is possible to add some functionality to Windows Sandbox by using aforementioned means, its configurability is still very limited especially when compared to some other third party solutions, like Sandboxie. Yet, since Windows Sandbox is included in the Windows itself, it is worth modding it a bit to better suit user’s needs." }, { "title": "Windows Sandbox overview - Enabling, Features, Usability, Security", "url": "/posts/windows-sandbox-overview/", "categories": "windows, sandbox", "tags": "operating systems, virtual machines", "date": "2022-11-13 00:00:00 +0100", "snippet": "Over the time, Windows 10 got some useful features that are not always obvious or even enabled. Many users are not even aware of their existence. When it comes to productivity, it turns out that the Windows for some time sports a fully-fledged virtualization engine (called Hyper-V) and an instant-sandbox called simply Windows Sandbox. In this article we will take a closer look at the Windows Sandbox, to see what it is all about.What it isWindows Sandbox is an instance of a fresh Windows installation that starts within Windows itself. It is a clean Windows virtual machine that doesn’t need to be set up or preinstalled like traditional virtual machine engines (like beforementioned Hyper-V). It just runs from scratch, enabling the user fully-fledged Windows environment which is totally separated from host. Being a sandboxed, it does not remember its state.How to enable itWindows Sandbox is not supported on Home versions of either Windows 10 or 11. Windows 10 supported versions (Pro, Enterprise, Education) need to be build 18305 or newer – Windows Sandbox was introduced in May 2019. so, every supported and updated Windows is good to go. Windows Sandbox is not enabled by default; therefore, it doesn’t show in the Start Menu. Prior to enabling it, the system needs to support a Virtualization option in BIOS: its state can be checked in the Task Manager:If the host is virtualized itself (i.e., run in the virtual machine itself), the Nested Virtualization is required to run the Windows Sandbox. Enabling it is a matter of starting PowerShell and entering following command:Set-VMProcessor -VMName &lt;VMName&gt; -ExposeVirtualizationExtensions $trueOnce the virtualization is enabled, the Windows Sandbox should be enabled by going to Control Panel &gt; Programs &gt; Turn Windows Features On or Off:After the system restarts, Windows Sandbox appears in the Start Menu and it can be started:FeaturesThe Windows Sandbox is a live, clean-slate image derived from the host operating system. That means it sports the same Windows version as host and it is already updated to the same state (in stark contrast to the virtualized Windows system which doesn’t share any traits to the underlying OS). Some important aspects of Windows Sandbox are: It is a part of the Windows – no separate download is necessary (in respect to the Windows build requirement) It is a one-time session, meaning that it does not remember the state after the restart (for instance installed programs and copied files will disappear after the restart or closing down the program), It is isolated from the host – the changes done inside will not impact the host operating system (with some remarks later explained), It runs rather quick even on some older hardware.Transferring contentThere are two possibilities to copy files from the host to the Windows Sandbox. The first option is usual copying through Copy (Ctrl+C) and Paste (Ctrl+V). Note that the files’ drag-and-drop is not supported. Other way is configuring a shared folder between host PC and Windows Sandbox which is covered here.Usability Software testing Windows Sandbox is perfect companion when it comes to software testing. Fire it up, install the program, run it and play with it, change parameters, evaluate it. Once the Sandbox is closed, nothing escapes it – no harm done. It is perfect for power users, developers, testers. Windows options exploration Inside Windows Sandbox, user can freely explore the myriad of Windows options (those options that do not require restart though), figure-out what they do, how they behave - all without fear of system failure. Browsing the Web Since Microsoft Edge comes preinstalled, user can freely surf the Internet without having to worry about cookies, ads and related stuff that may impact the system. Inside Windows Sandbox, there is no need to clear cookies ever.Note that the Internet connectivity within Windows Sandbox does not work if VPN on host system enabled. SecurityUsing Windows Sandbox in most cases is isolated from the host, yet there are two potential security issues with it: Windows Sandbox by default utilizes Windows Defender antivirus, which offers decent protection against malware. Due to the temporary nature of Windows Sandbox itself, the proper antivirus suite can’t be installed so the user is limited to a Windows Defender, which lacks some security features like behavioral analysis needed to detect zero-day (unpatched) threats. In that way, user can wrongly assume that the file is safe and use it on the host system, thus endangering the host and the network. Advice: use online virus scan, just to be sure. When it comes to networking, Windows Sandbox is protected by NAT (network address translation), preventing network devices to initiate access to it. However, in some cases there is possible to detect the subnet and therefore the network devices on the host system – enabling the scan for the opened ports on those devices, which poses a security risk. Hint: for true isolated experience, disable the network adapter on Windows Sandbox when testing for malware. ConclusionWindows Sandbox is a neat useful Windows feature which gives the look and feel of the fresh, unbloated system. Being very handy to use, it can give additional value to the user experience and a boost in productivity and security." }, { "title": "C# Mini Project - Car Rental program detailed overview", "url": "/posts/mini-project-car-rental/", "categories": "C#, sql server", "tags": "C#, databases, windows forms, visual studio", "date": "2022-11-08 00:00:00 +0100", "snippet": "In this article, we will be reviewing a simple Windows program which can be used to manage a car rental company. The program is synced to a MS SQL Server database and it features basic features like registration of cars and customers, tracking of rentals and returns.Front-end uses Windows Forms UI framework and the development is done in Visual Studio. The source code is available at this link.Database overviewThe database part is done in SQL Server and it consists of four simple tables: dbo.carreg is the table where rental cars are registered, dbo.customer is the table for customers records, dbo.rental tracks rentals with date records, dbo.returncar manages returns of cars, i.e., termination of rentals.Note: dbo prefix is automatically added on the creation of tables in SQL Server Management Console, they can be referenced with and without this prefix.Structure of the programA program consists of the login screen, main window and sub-windows for each option (car registration, customer data, rental and return records). The main Program.cs section calls the Form1.cs and so the program is being run. The program consists of multiple .cs files which are separate classes itself. They are designed using Windows Forms UI elements from the Visual Studio corresponding toolbar.Login screen (Form1.cs)This simple form consists of two textboxes and two buttons. There is a predefined username (testadmin) and password (123) which is checked upon the click on the Login button. If the username and password are correctly entered, the program then creates a new instance of the class Main and displays it.private void button1_Click(object sender, EventArgs e){ string uname = txtuname.Text; string pass = txtpass.Text; if (uname.Equals(\"testadmin\") &amp;&amp; pass.Equals(\"123\")) { Main m = new Main(); this.Hide(); m.Show(); } else { MessageBox.Show(\"Username or password do not match\"); txtuname.Clear(); txtpass.Clear(); txtuname.Focus(); }}Main window (Main.cs)This section simply creates objects (class instances of the corresponding classes) and displays them via the buttonX_click method:private void button1_Click(object sender, EventArgs e){ carreg c = new carreg(); c.Show();}private void button2_Click(object sender, EventArgs e){ customer c = new customer(); c.Show();}private void button3_Click(object sender, EventArgs e){ rental r = new rental(); r.Show();}private void button4_Click(object sender, EventArgs e){ returncar r=new returncar(); r.Show();}private void button5_Click(object sender, EventArgs e){ this.Close();}Car registration (carreg.cs)From the designer’s viewpoint, this form consists of multiple elements like textboxes, buttons, grids, etc. Obviously, user can enter a registration number, make and model, and to mark whether the auto is available or not. Those data is then entered into the database and displayed within the DataGridView element on the right side. Let’s examine the most important parts of this structures and their link to the underlying code.The connection to the SQL Server is established by following code:using System.Data.SqlClient; SqlConnection con = new SqlConnection(\"Data Source=DESKTOP-N5IF2SJ\\\\SQLEXPRESS; Initial Catalog=carrental; User ID=testadmin; Password=123;\");SqlCommand cmd;SqlDataReader dr;Notice that this code will be the same in the other parts of the program. The objects cmd and dr are instances of the SqlCommand and SqlDataReader classes; the first one provides the possibility of entering the SQL commands, another one executes SqlDataReader objects.The class consists of multiple methods which are repeated throughout the project. The method load() loads the data from the carreg table into the DataGridView element:public void load(){ sql = \"select * from carreg\"; cmd = new SqlCommand(sql,con); con.Open(); dr = cmd.ExecuteReader(); dataGridView1.Rows.Clear(); while(dr.Read()) { dataGridView1.Rows.Add(dr[0], dr[1], dr[2], dr[3]); } con.Close();}The method getid() displays the data from carreg table into the outlined textbox fields. This method is called later when editing the car data records.public void getid(string id){ sql = \"select * from carreg where regno='\" + id + \"'\"; cmd = new SqlCommand(sql,con); con.Open(); dr = cmd.ExecuteReader(); while(dr.Read()) { txtregno.Text = dr[0].ToString(); txtmake.Text = dr[1].ToString(); txtmodel.Text = dr[2].ToString(); txtavl.Text = dr[3].ToString(); } con.Close();}Notice how this line enables the calling of the records with different ID numbers by declaring string variable id and escaping the characters while doing the SQL call: sql = \"select * from carreg where regno='\" + id + \"'\";The data from the database is then parsed to the corresponding text boxes.button1 click eventThe button “Add” has internal name button1. When the user click add, the entered data from the textboxes are entered into the database. This button has also a second function, which is update of selected records within database. Via the variable Mode, the selection is being made whether the new data is inserted into the database, or the existing data is being edited. The variable is also being used by another method which can change the state.private void button1_Click(object sender, EventArgs e){ string regno = txtregno.Text; string make = txtmake.Text; string model = txtmodel.Text; string aval = txtavl.SelectedItem.ToString(); if(Mode==true) { sql = \"insert into carreg(regno, make, model, available) values(@regno, @make, @model, @available)\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@regno\", regno); cmd.Parameters.AddWithValue(\"@make\", make); cmd.Parameters.AddWithValue(\"@model\", model); cmd.Parameters.AddWithValue(\"@available\", aval); cmd.ExecuteNonQuery(); MessageBox.Show(\"Record added\"); txtmake.Clear(); txtmodel.Clear(); txtmake.Focus(); } else { sql = \"update carreg set make=@make,model=@model,available=@available where regno=@regno\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@make\", make); cmd.Parameters.AddWithValue(\"@model\", model); cmd.Parameters.AddWithValue(\"@available\", aval); cmd.Parameters.AddWithValue(\"@regno\", id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Record updated\"); txtregno.Enabled = true; Mode = true; txtmake.Clear(); txtmodel.Clear(); txtavl.Items.Clear(); txtmake.Focus(); } con.Close();}Method Autono()This method sets the car registration number field to the 00000 (if there is no data read from the database), else it increments it by 1, so that the cars entered have the unique reg. no.public void Autono(){ sql = \"select regno from carreg order by regno desc\"; cmd = new SqlCommand(sql,con); ; con.Open(); dr = cmd.ExecuteReader(); if(dr.Read()) { int id = int.Parse(dr[0].ToString())+1; proid = id.ToString(\"00000\"); } else if (Convert.IsDBNull(dr)) { proid = (\"00000\"); } else { proid = (\"00000\"); } txtregno.Text = proid.ToString(); con.Close();}Method dataGridView1_CellContentClick()This method enables editing or deletes the data displayed in dataGridView1. In fact, it is an event that reacts on an user clicking on the particular parts of the gridview.If the user clicks on Edit button, the variable Mode is set to false, which enables updating of the records via the button1_Click() method previously described. Moreover, it sets the textbutton textregno as disabled (since we are editing the particular row in the table); it then reads the id value which is the first field in the datagrid current row and it passes that value to the aforementioned getid() method which enters the other text fields with the corresponding data from the database.private void dataGridView1_CellContentClick(object sender, DataGridViewCellEventArgs e){ if(e.ColumnIndex==dataGridView1.Columns[\"edit\"].Index &amp;&amp;e.RowIndex&gt;=0) { Mode = false; txtregno.Enabled = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); getid(id); } else if (e.ColumnIndex == dataGridView1.Columns[\"delete\"].Index &amp;&amp; e.RowIndex &gt;= 0) { Mode = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); sql = \"delete from carreg where regno=@id\"; con.Open(); cmd = new SqlCommand(sql,con); cmd.Parameters.AddWithValue(\"@id\",id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Record deleted\"); con.Close(); }}Button2, which is a refresh button, loads two methods and clears the fields. In that way, the records are being displayed into the gridview. The class runs the same methods on startup as well:public carreg(){ InitializeComponent(); Autono(); load();}The difference being here that the refresh button enables reloading new records on change – the new records wouldn’t be displayed without refresh button until the program’s restart.private void button2_Click(object sender, EventArgs e){ load(); Autono(); txtmake.Clear(); txtmodel.Clear(); txtmake.Focus();}Customer RegistrationThis class follows the same logic as previous one, just the names are different.Rental RegistrationHere we have two new methods; other code is similar to previous classes. Two feats are new to explain. Every car needs to have an availability mark; such column exists in table dbo.carreg. Idea is that cars that are taken are marked as unavailable so that the same car can’t be booked twice in the same time period. Method txtcarid_SelectedIndexChanged() queries database with the following: select * from carreg where regno='\"+ txtcarid.Text+ \"' \" txtcarid is the designation for the ComboBox, so basically this query tries to get the data filtered by the given carid and to display it in the gridview. If the read from the database succeeds (meaning the data with the particular id exists), then this line: aval=dr[\"available\"].ToString(); gets the value of column available in the carreg table. If that value is no, meaning car is not available, then the other fields below the ComboBox are disabled because the data on the taken car can’t be changed. Else, those fields are editable. If the read from the database does not succeed, this means car with the particular carid isn’t entered into the database and therefore unavailable. private void txtcarid_SelectedIndexChanged(object sender, EventArgs e){ cmd = new SqlCommand(\"select * from carreg where regno='\"+ txtcarid.Text+ \"' \", con); con.Open(); dr = cmd.ExecuteReader(); if (dr.Read()) { string aval; aval=dr[\"available\"].ToString(); label9.Text = aval; if(aval==\"No\") { txtcustid.Enabled = false; txtcustname.Enabled = false; txtfee.Enabled = false; txtdate.Enabled = false; txtdue.Enabled = false; } else { txtcustid.Enabled = true; txtcustname.Enabled = true; txtfee.Enabled = true; txtdate.Enabled = true; txtdue.Enabled = true; } } else { label9.Text = \"Car not available\"; } con.Close();} txtcustid_KeyPress method is used when the user enters a value in txtcustid field(Customer ID) and presses key within it. If that key is a Return (KeyChar==13), then via SQL query the program fetches the data from the database. Else, a customer with entered ID does not exist. private void txtcustid_KeyPress(object sender, KeyPressEventArgs e) { if(e.KeyChar==13) { cmd = new SqlCommand(\"select * from customer where custid='\" + txtcustid.Text + \"' \", con); con.Open(); dr = cmd.ExecuteReader(); if (dr.Read()) { txtcustname.Text = dr[\"custname\"].ToString(); } else { MessageBox.Show(\"Customer ID not found\"); } con.Close(); } } ReturncarClass returncar takes into account CarID, CustomerID, Date of return, it counts the elapsed days and it calculates the fine if the number of rental days is bigger than defined in DueDate field in Rental Registration class. It has similar options to edit and delete data in the DataGridView. Let’s see how the fine is calculated.When the user enters the value into the CarID field and press an Enter, this code is being executed:cmd = new SqlCommand(\"select car_id,cust_id,date,due,DATEDIFF(dd,due,GETDATE()) as elap from rental where car_id='\"+txtcarid.Text+\"'\",con);This SQL command will fetch the wanted data and via a SQL function DATEDIFF will calculate the difference between actual date of car return and supposed return date. If that difference is larger than 0 (if it exists), the program will calculate a fee that is times 100 of the number of passed days, else the fee doesn’t exist.ConclusionIn order to develop a simple program like this, one should possess following knowledge: designing a database in SQL Server, connecting a SQL Server to a C#, writing basic SQL queries, using Windows Forms UI framework, manipulate the elements of the UI and write the corresponding code, finally, have a basic overview of Visual Studio in which the program is developed.Possible improvements: improve user login via using trusted connection (without passing an user id and password) - current solution is prone to SQL injection attack, use other way to connect to the database, without having the need to manually close the connection every time the data is fetched (open connections to the SQL server are security issues as well as performance hogs)." }, { "title": "Why (not) study IT – my thoughts", "url": "/posts/why-study-it/", "categories": "studies, education", "tags": "university, academia", "date": "2022-11-01 00:00:00 +0100", "snippet": "Having just earned my bachelor’s degree in computer science, I wanted to outline and summarize the pros and contras of pursuing IT studies (or any studies for that matter) in the 2022. This is my second bachelor so obviously I was inclined to study in the past, but for the somewhat odd reasons. Moreover, if I was today thinking of enrolling, I would probably not do it. Here is the quick recap of my reasoning.Pros Consistency This was the main reason for me to pursue the studies. I was aware from the beginning that the studies have streamlined path, with steps to conquer towards the end goal. This may seem obvious, but comparing it with a self-study (and especially having tried it as I did), it is a major advantage: you are forced to do your assignments, seminal papers over multiple exams. It is usually a linear process; you need to pass an exam and then move to other ones. When doing that over a longer period, one faces with an internal struggle in order to achieve the most important thing in professional life: consistency. Being consistent means doing small but continuous improvements on any matter, which are added-up over the time, enabling a mastery over given topic. The source of consistency in my case, apart from the curiosity and passion about the underlying tech matters, were the tuition costs: the fact that the money you invested in education could be spent elsewhere. Therefore, opportunity costs are pressing you to justify your current sacrifice (expressed in your decision to allocate your money to finance your studies). Of course, you are sacrificing your current income in order to achieve return on your investment in the future, which is the main reason for studying anyways (apart for some rare non-monetary incentives). Networking with the peers In my view, the second most important advantage of studying is meeting other people in similar position (studying the same thing) and connecting with them. Being constantly on the same place at the same time during number of years enables people to acquaint with each other, to connect at various levels respective to their primary (topics of study) and secondary (hobbies, similar views on other aspects of life) interests. This creates lasting personal connections that could be significant in later professional and sometimes even personal life. Unfortunately, because of the extraordinary events starting in 2020 my networking experience was very limited. Intellectual challenge In this world, any achievement is done by continuous effort through series of challenges. This is true for any type of achievement, being intellectual, physical or artistic. Studying will mentally challenge you, forcing you to obtain new knowledge and to adapt during the process of solving problems. You will fall and rise; do trial and error; until you either quit temporarily or permanently, or succeed. It can be frustrating but addictive too. Cons Dealing with academia I guess no one is fond of messing on a daily level with hierarchical bureaucracy that considers you as a number on the list: waiting for the enrollment while queuing in the students’ lines, admission to various tests, seminal papers, managing installments, deadlines of various things and other trivial and boring issues. It is as if you are employed just that instead of being paid, you are obliged to pay and to obey at all times. Then there is dealing with lecturers’ egos: taken on average, academia doesn’t encourage free thinking and exchange of ideas supported by arguments. On numerous occasions, student will be wrong about something and will have to accept the argument of the academics (which is the obvious thing to do and one of the reasons of studying); but when vice-versa situation occasionally happens, academics often just don’t have the habit of reevaluating their views. And of course, studying is not about rightness or wrongness per se, but it should include honesty and integrity, virtues which seems to be in a rapid decline in our times. If you are a maverick, you will have very hard time on academia. Wrong perception of curriculum It may happen that the enrolling student is passionate about one set of topics and that the given uni’s curriculum (and/or the execution of the curriculum) is just not focusing on those interests and passions enough. The end effect is losing motivation and your passion, because if you are passionate about Python or C# for example, and if your uni’s lectures on those topics are not broad enough as expected but instead focuses on VBA, you will feel like losing time and achieving wrong goals. To be fair, there is some flexibility on curriculum because on the later years of study students usually can choose their courses, but even then, I’ve found out that self-study and custom-tailored courses offer more insight and depthness on the selected topics than those served by academia. Unjustified costs Depending on one’s country of origin, enrollment possibilities and the choice of the uni, the studying costs will range greatly. Still, it is safe to say that taken in relative regard, they will be significant. The incurring costs need to be justified and if usual way of financing the studies is taking student loans that span over the multiple years, it will mean a significant reduction of a student’s living standard afterwards – until the breaking point is reached when the gains will overcome the costs. Therefore, the dilemma of justifying the costs of studies is whether the gains can be achieved without studying or not. I believe that, especially in IT field, studying is becoming more and more redundant. What future brings?The basic reasons of pursuing any studies just a decade ago would be to be able to learn from the recognized academics and industry’s experts: to attend the lectures in person, to actually see and hear the lecturers and interact with them. Since then, rapid changes took place as everything became digitalized and even freely available. Today, one can find loads of quality material online: myriad of courses, lectures and even complete studies are available freely or paid, in the video, text, audio and other forms (with additional material such as manuals, course books and exercises). Then there are specialized websites, apps, forums, social media groups and channels which are practically on the one’s fingertip.For those inclined to more traditional ways of learning, there are ratings and reviews of studies and courses; advices from the fellow students; and group collaboration tools that can be really useful when it comes to evaluating the decision about picking the right type of education.So, my guess is that the possibility of obtaining access to the freely shared knowledge will be preferred in the near future. While I am mostly thinking about IT, that will probably apply to other, non-regulated fields of human endeavors too, because in the end, knowledge is about passion, not necessarily about academic degrees.ConclusionAlthough I think that obtaining a diploma in IT still has some merits, it has some serious drawbacks. It all depends on the one’s personal situation and perception: if you want all-rounded, broad educational path with a conservative mindset of starting your career, then pursuing studies would be appropriate. For those eager to focus and specialize, other, modular learning paths are probably better option: they will lead to quicker employment or possibility of creating something on your own (maybe in entrepreneurial sense)." }, { "title": "List comprehensions in Python", "url": "/posts/list-comprehensions-python/", "categories": "python, list comprehension", "tags": "python, functional programming, iteration", "date": "2022-10-21 00:00:00 +0200", "snippet": "Iteration protocol in Python consists of multiple statements, of which a for loop is the usual way to iterate over corresponding data structure. For loop is not the only though. In this regard, Python borrowed from the other languages, particularly from the Haskel, providing something called comprehension iteration.Suppose we have a following code:L=[2,4,6,8]for x in range(len(L)): L[x]=L[x]**2print(L)# Outputs: [4, 16, 36, 64]It certainly works as planned, but there is more concise and even faster way to do the same:L=[x**2 for x in L]# Outputs the sameThis one liner replaces the for loop completely; it partially resembles the syntax of the for loop, only written backwards, while bringing some novelties in the process. Let’s explore its details.List comprehension basicsIf we closely look at the prior statement:L=[x**2 for x in L]it is clear that the statement inside constructs a new list, because it is written in square brackets. It begins with an arbitrary expression (x**2), followed by the ‘for’; ending with the iterable object (L). When this statement is run, an interpreter iterates the elements in the right side, applying the code on the left side of for; end result being creation of a new list with all its elements multiplied by 2, for every x in L.List comprehensions are optional, they are not required by any feature of Python, as they can be fully represented by for loops and lists. However, they are less verbose and very common in code, therefore their understanding is an important part of comprehending Python. Moreover, when it comes to code performance, they may have a significant boost compared to executing for loop statements, because comprehension iterations are performed inside Python interpreter at C language speed.List comprehensions apply an arbitrary expression to iterated items, rather than applying set of lined statements or a function. Therefore, similar to lambda, they can be put to places where multiple-lines statements wouldn’t be allowed. Comprehensions are a general term as they can be applied not only to lists, but to sets and dictionaries as well (albeit they are most widely used with lists). Their formal syntax is:ExamplesIt almost seems as they form a mini-language itself, especially when coupled with an if statement:res=[x for x in [2,3,4,5,6] if x%2==0]print(res)#Outputs : [2, 4, 6]which is the same as:res=[]for x in [2,3,4,5,6]: if x%2==0: res.append(x)print(res)# Outputs : [2, 4, 6]Quasi-nesting is also supported:res=[x+y for x in [2,3,4,5,6] for y in [10,20,30]]print(res)# Outputs: [12, 22, 32, 13, 23, 33, 14, 24, 34, 15, 25, 35, 16, 26, 36]which is the same as:res=[]for x in [2,3,4,5,6]: for y in [10,20,30]: res.append(x+y)The iterable part in comprehension can be any iterable type. For instance, it can iterate over strings as well:print([x for x in 'collection'])# Outputs: ['c', 'o', 'l', 'l', 'e', 'c', 't', 'i', 'o', 'n']Emulating concatenation:print([x+y for x in 'ab' for y in 'cd'])# Outputs: ['ac', 'ad', 'bc', 'bd']This code produces identical output (iterating over a list instead of string in the second part):print([x+y for x in 'ab' for y in ['c','d'] ] )To give more complex example, let’s start backwards and first define the usual, for loop statement:res=[]for i in range(10): if i%3==0: for j in range(4): if j%2==1: res.append((i,j))print(res)Output:[(0, 1), (0, 3), (3, 1), (3, 3), (6, 1), (6, 3), (9, 1), (9, 3)]Same code expressed as list comprehension with the same output:print([(i,j) for i in range(10) if i%3==0 for j in range(4) if j%2==1 ])Performance, readability and usability remarksAs we have seen, list comprehensions per se are not required syntactically, but their usage usually yields a performance advantage over the usual for loops. Due to the fact that they execute at the full C language speed (which is also true for the map iterations), they are in most cases significantly faster than for loop Python bytecode.Yet, there is certainly a tradeoff between list comprehensions’ conciseness, performance gain and lowered readability, because they bring less verbose but as well less understandable statements. So, when used, list comprehensions should be kept simple and neat; for more complex tasks, full statements are preferred. There is a reason why the for loop is the usual way of doing most tasks; one should be aware of other ways too which is the purpose of this article.ConclusionList comprehension enables creation of effective and fast one-liners in Python which replace for loops. They are expressions, meaning they can be inserted in places where multiple lined and nested code couldn’t. Iteration through comprehension is widely used in practice, so if kept simple, it is a neat and elegant coding solution." }, { "title": "Iterating over combinatorics", "url": "/posts/iteration-over-combinatorics/", "categories": "python, algorithms", "tags": "python, itertools, combinations", "date": "2022-10-19 00:00:00 +0200", "snippet": "Very often, there is a need for implementing combinatorics (permutations, combinations and variations) and iterating over the generated values in a list for example. While it is possible to create user-tailored solution to it, there is module in Python that provides solutions to those problems.The itertools module provides four methods for implementing combinatorics: product() itertools.permutations() itertools.combinations() itertools.combinations_with_replacement()Examples:product('ABCD', repeat=2)Output:AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DDpermutations('ABCD', 2)Output:AB AC AD BA BC BD CA CB CD DA DB DCcombinations('ABCD', 2)Output:AB AC AD BC BD CDcombinations_with_replacement('ABCD', 2)Output:AA AB AC AD BB BC BD CC CD DDThe user can choose one of those four methods depending on the type of combinatorics required. Basically, the product() method can be configured to output variations with repetition – basically all possible ways to combine a given set of data. The purpose of other three methods is self-explanatory. The first method itertools.permutations() takes a list or another collection and produces a sequence of tuples which generate all permutations’ possibilities. For example:collection=['a','b','c']from itertools import permutationsfor x in permutations(collection): print(x)Output:('a', 'b', 'c')('a', 'c', 'b')('b', 'a', 'c')('b', 'c', 'a')('c', 'a', 'b')('c', 'b', 'a')As demonstrated in the first example, it is possible to set a length of an output data:for x in permutations(collection,2): print(x)Output:('a', 'b')('a', 'c')('b', 'a')('b', 'c')('c', 'a')('c', 'b')Going further, itertools.combinations() produce a sequence of combinations from the input items. For example:# (using the code from the previous example)for x in combinations(collection,3): print(x)Output:('a', 'b', 'c')Those are combinations without repetition (i.e. repetitive characters are not produced as they are considered the same). Combinations with repetition can be generated as follows:from itertools import combinations_with_replacementfor x in combinations_with_replacement(collection,3): print(x)Output:('a', 'a', 'a')('a', 'a', 'b')('a', 'a', 'c')('a', 'b', 'b')('a', 'b', 'c')('a', 'c', 'c')('b', 'b', 'b')('b', 'b', 'c')('b', 'c', 'c')('c', 'c', 'c')The itertools module contains other interesting tools, like infinite iterators and terminating iterators. Certainly, when it comes to combinatorics and aforementioned other topics, it is the quickest way of implementing solutions to those problems." }, { "title": "10 essential free, open-source programs for Windows and Linux", "url": "/posts/ten-essential-free-programs/", "categories": "software, open-source", "tags": "software, freeware", "date": "2022-10-16 00:00:00 +0200", "snippet": "This list comprises of 10 great freeware, open-source programs which may not be an obvious find for everyone. It includes the useful utilities for every-day general use and productivity and most are available for both Windows and Linux platforms. 7-Zip (file archiver) 7-Zip When it comes to file archiving and compression, 7-Zip is very capable archiver supporting a wide-range of formats (ZIP, RAR and many more) as well as its native 7z format which has open architecture as well. The program supports any Windows version since 2000 (supporting both GUI and command-line versions), while it functions as a command-line-only tool in Linux. Archives can be encrypted with the 256-bit AES. The Windows version can be used as a file manager as well and it can be associated into the shell context menu. The program can calculate the checksums of files in the CRC or SHA formats. The only con to the program from my viewpoint is an absence of the automated update check-ups. Audacity (audio editor) Audacity If you need to record or digitize, edit, mix or post-process audio files, chances are that you’ll be able to do it with Audacity. The program was introduced over 20 years ago with a premise to offer basic audio editing, but since then became widely known digital audio files editor. Audacity features include live audio recording through a microphone or mixer, or digitizing recordings from other media; post-processing through adding of various effects; digital multitrack editing; third-party plug-ins support; support for variety of audio formats and more. The program support both Windows and Linux. Bitwarden (password manager) Bitwarden On average, a user with an active digital life probably has over 100 passwords to remember. If they are manually managed, it could lead to a password overload which means using the same or slightly altered passwords for multiple logins, which poses a significant security risk. Better way to manage passwords is using a password manager, which protects the encrypted password vault with one-login-to-remember, relieving the user of mismanaging the dozens of web page logins, PINs, and what not. Bitwarden is one of the best password managers out there. Being open-source, it is constantly publicly reviewed and frequently updated. It can be used not only as a personal password tool; it supports a plethora of features in business environment like multi-user support, API access, user groups, SCIM support, SSO integration, encrypted sharing, encrypted file attachments, and more. Bitwarden can sync passwords between multiple devices and store them in a protected vault. The vault is by default stored at a cloud but it can be saved locally as well – in both cases it is encrypted by strong 256-bit AES. It supports practically every platform available, both desktop and mobile. Other security features include two-factor (2FA) authentication, password generator, password sharing, password auditing and breach monitoring. Worth mentioning is that only the basic version of Bitwarden is free, other ones come with a cost. Notable mention: Keepass is worthy alternative to the Bitwarden if you want only basic, locally stored vault and password management. Its basic configuration can be upgraded with numerous plugins though, although this customization does require a bit of a user effort. BleachBit (disk-space cleaner) BleachBit Temporary files created by operating system and other software can clog-up the system, affecting performance. They can pose a security risk as well. Although modern operating systems have embedded some basic cleaners, third-party apps still do a much better job in this regard. BleachBit is a free program that can clean digital trash from multiple sources. It detects the major browsers’ left-overs like web cache, cookies, session data and more; it can find left-over and safe-to-delete files in the operating system directories as well. Furthermore, it offers a file shredder.It has a couple of draw-backs though: it does not clean all the trash (like operating system update left-overs) and it does not have a scheduler. Bulk Crap Uninstaller (program uninstaller) Bulk Crap Uninstaller Software admins, testers and computer enthusiasts install a number of programs which at some point need to be removed. It can be done manually either through operating system or by particular program’s uninstaller. Sometimes there is no uninstaller and the program needs to be manually deleted, folder by folder, and even if uninstaller is present, it is mostly likely that the left-overs will be scattered throughout the drive that it will be very hard and time consuming to fully remove all of them.There is variety of program uninstallers on the market, both paid and free. Bulk Crap Uninstaller (BCU) is viable option due to the fact that is pretty powerful and configurable, allowing multiple uninstallations and automatic folder cleanup afterwards, all that with a minimal user input during the uninstall process. Multiple uninstallation or batch uninstall means that the user will simply mark the number of programs which the BCU then quietly uninstalls. Furthermore, it can deinstall programs which don’t have the uninstaller, searching the directories and doing the dirty work for the user. It has a system cleaner which can remove system updates and preinstalled bloatware and as such, it can be a perfect companion to the aforementioned BleachBit. Other useful features are startup manager, application rating, creation of uninstall presets and much more. GIMP (image editor) GIMP GIMP is a bitmap image editor that has a reputation as a closest free thing to the Adobe Photoshop. It mimics in many ways the tools of the Photoshop and it has the rich feature set; however, the user interface and the workflow require adjusting. GIMP supports multiple image filters and brushes, as well as transformation, selection, layer and masking tools. There are vast number of available plugins and it supports scripting and macros which allows automating tasks. GIMP supports all available platforms including Windows, Linux and more. MEGAsync (cloud drive) MEGAsync MEGAsync is an end-to-end encrypted cloud drive with generous 20 GB of storage for a free account (paid plans available too). It uses a 128-bit AES (paid versions utilize 256-bit AES). It supports all common software platforms, as well as mobile ones, enabling seamless data syncing among multiple devices. One possible limitation is somewhat lacking online collaboration and integration into office suites, due to its security features. Notable mention: Sync.com offers end-to-end encryption as well, and its free plan includes 5GB of cloud space. However, it does not offer a Linux client. Notepad++ (text editor) Notepad++ This one probably needs no introduction. Whether you’re just looking for a more advanced text editor than Notepad or you need almost a fully-fledged IDE, Notepad++ gets you covered. Standard option comes with a ton of useful options and its capabilities can be expanded by numerous plugins. It supports displaying and editing of nearly any programming language; additionally, it can run most of them directly (with some forking). Its only drawback is that is Windows based only. Notable mention: Visual Studio Code is either al pari to Notepad++ or better (especially when it comes to debugging and testing), and it supports Linux and other platforms too. ShareX (screen-capture) ShareX ShareX is probably the best screen capture utility for Windows. It can seamlessly capture static screen images or animated ones; it offers many utilities ranging from color-picker, QR scanning, OCR processing, to task automation including taking captures, applying some basic edits (like adding a watermark) and uploading to online service, thumbnail creation and more. It is Windows-only application. Shotcut (video editor) Shotcut Shotcut is a capable video editing software available on all major platforms. It uses internal codecs so it isn’t dependable upon system ones. Supports wide range of video formats as well as still images; audio and video capture from various sources, multi-format timeline editing, video effects, capable audio editing options and filters etc. It supports a lot of output formats, while it lacks in number of transition effects offered. Its user interface is pretty good too. " }, { "title": "Text clean-up and string sanitization with Python", "url": "/posts/text-cleanup-sanitization-python/", "categories": "python, clean-up, string sanitization", "tags": "python, text parsing", "date": "2022-10-11 00:00:00 +0200", "snippet": "Text sanitizing means cleaning-up the entered text according to the previously established parameters or rules. Those rules usually involve text parsing and handling.A whole set of tools could be used to clear-up such an input. For example, basic tools like string replacement method - str.replace() or regex – re.sub() can achieve removing or changing a specific character sequences. Additionally, there is Unicode normalization. Consider following strings:s1='Menue\\u00f1o' # Menueños2='Menuen\\u0303o' # MenueñoThey represent the same text, and are represented by multiple characters therefore the strings itself are different:print(s1==s2) Output:FalseObviously comparing those strings will be a problem so this needs to be fixed somehow. When considering Unicode, Python sports the unicodedata module with method normalize() which can do the trick:fix_s1=unicodedata.normalize('NFC', s1)fix_s2=unicodedata.normalize('NFC', s2)print (fix_s1==fix_s2) # trueNFC argument means fully composition of the characters, so if we apply it to the both strings, they will be comparable. There is another argument which depicts decomposition NFD, which fully decomposes the use of the underlying characters:fix_s1=unicodedata.normalize('NFD', s1)fix_s2=unicodedata.normalize('NFD', s2)print (fix_s1==fix_s2) # trueTo see the difference between NFC and NFD, let’s ascii print the first string after applying NFC and NFD parameters, respectively:print(ascii(fix_s1)) # NFC applied outputs 'Menue\\xf1o'print(ascii(fix_s1)) # NFD applied outputs 'Menuen\\u0303o'Furthermore, when trying to clean-up the text, the various strip() methods could be used for processing. They can be used to get rid of whitespace and characters on the either left or right side of the string, but not in the middle:s=' python is open source 's=s.strip()print(s)Output:python is open sourceHere the s.strip() method trims leading and trailing whitespace, but it can’t remove the inside ones. To do that, we could use the replace() method, as follows:s=s.replace(' ','')Which outputs:pythonisopensourceGoing further, there are other, more advanced options to the text clean-up. Suppose a chaotic string like this:\"ṗŕögrặmṃìng\\fis\\t great\\r\".There is a way to process whole ranges of characters or strip diacritical marks by using str.translate() method. Let’s first fix the whitespace:s='ṗŕögrặmṃìng\\fis\\t great\\r'process={ ord('\\t'): None, # deleted ord('\\f'): ' ', # replaced with an empty space ord('\\r'): None # deleted}s=s.translate(process)print(s) Output:ṗŕögrặmṃìng is greatBasically, it uses a translation table which is then applied to the given string. One can build a larger table and include all the fixes inside. Adding this code to the previous block:import unicodedataimport syscombine=dict.fromkeys(c for c in range(sys.maxunicode) if unicodedata.combining(chr(c)))b=unicodedata.normalize('NFD', s)b=b.translate(combine)print(b) Output:programming is greatIn this example, a dictionary mapping every Unicode character (through comprehension iteration) is created from the dict.fromkeys(). The original input is then normalized into decomposed form using normalize() with a NFD parameter. Afterwards, translate() method is used to delete all the accented characters.Ending note is that the larger data set, the more possible performance issues the complex code may have. Therefore, performance-wise, it is sometimes better to repeat simpler code than to forge out an one-solution-fits-all. In any case, a testing will reveal what is the most acceptable solution in that regard." }, { "title": "Retro computing - review of the Lenovo Thinkpad X201", "url": "/posts/thinkpad-x201-review/", "categories": "hardware, laptop", "tags": "retro, hardware, thinkpad", "date": "2022-10-09 00:00:00 +0200", "snippet": " Lenovo Thinkpad X201A blast from the past, a machine that has been in active service for staggering 12 years and still rocking is certainly worth reviewing of. It is a 2010 fine specimen of the ultra-portable X-series line, which traces its roots back to the year 2000. when IBM introduced the X20. Thinkpads need no introduction, as they are iconic machines that set standard some 30 years ago, as a complete desktop-replacement solution which was able to mimic the look and feel, as well as performance, of a classic desktop PC - which was unheard for the era. OverviewThis particular model was introduced in 2010 (see initial review here). It originally sported a mobile version of the first i5 generation, coupled with a 4 GB RAM and usual suspects like radio module (Bluetooth 2.1 and a WiFi 4 a/g/n adapter). It has a 3G module as well, which was an optional feature. Surprisingly enough, it has a modem (trully blast from the past!) The initial asking price in Europe was 1500-1800 EUR (depending on the configuration) which was a lot of the money back then, especially considering its form-factor. The 15.6” top-of-the-line Dells and Toshibas were sold for equal or less amount. I bought it refurbished in 2014 At the time I just discarded my old Thinkpad T60 (which was legend itself back in the time) and needed something newer - dual-core hyperthreading CPU was fine. The X201 came with a preinstalled Windows 7, 160 GB HDD, and a wiggly radio adapter which soon stopped working. As I recall, the BIOS came locked-out, meaning I could enter it and see the configuration, and at most, change boot-order, but everything else was inaccessible. Which meant no WAN card swaps. I tried to flash BIOS that accepted unlisted WAN cards (Lenovo locked-out unapproved adapters), but that never worked properly. Then I installed small mPCIe to USB adapter and hooked an USB WiFi adapter with a detachable antenna, which worked fine for a couple of years but that died as well. From that point on, I used external USB WiFi adapters and wired connection. As Windows 7 was relatively snappy with 4 GB RAM, the significant limiting factor was a spinning hard drive. Somewhere around 2016 I upgraded it with a SSD, which brought new life into this small machine. Although X201 supports only SATA II interface (max. 3 GB/s), the performance gain is vastly improved due to much shorter random seek-time in comparison to mechanical disk. Windows 7 functioned really fine, especially when I maxed-out the RAM to 8 GB, adding one additional 4 GB module. As Microsoft ended long-term support for Windows 7 in January 2020, I could either switch to Linux, or try to install and use Windows 10. At that moment I was still Windows-oriented, and have had not fully explored alternatives to it, so I was keen enough and installed Windows 10 on the machine, which was 10 years old at the time. After 6 months, this showed to be a mistake, as Windows 10 hogged up laptop’s resources especially with its frequent updates and fixes. During that time, I found alternatives to mostly all the software I use and switched to Linux. After trying a couple distros, I finally settled on a Linux Mint. Less demanding MATE desktop environment runs really fine; in comparison to Windows 10 it flies. It is continuously updated hence the machine can be used online (but the update process is much more smooth, simpler and faster in comparison to the Windows 10); it is snappy and feels happy with the 8 GB RAM and it heats up the case way less than Windows 10. So, even after 12 years, the machine can still be used in a productive manner which is awesome and it justifies the high initial investment. Pros Build quality and design Although being small, this is the classic Thinkpad: a bit uninspiringly-looking to some but very sturdy. It does not bend, does not squeal, if water is spilled on the keyboard, it is drained without side-effects. The case is made of magnesium alloys and the inner side of screen is of plastics that can bend but not much. Thinkpads were never flashy machines, they signify functionality, usability and practicality over attentive showiness. Keyboard and touchpoint The keyboard is the famous 7-row classic Thinkpad one: it is considered the best in the business. Sadly, Lenovo switched it later to chicklet-style starting from X230 onwards (which does have some advantages) and the position of the keys from that model onwards no longer resembles the classic PC keyboard’s look. Here, you can be in full control just by touching the group of keys and sensing the particular key or group (without looking down) as for instance Insert, Delete, Home, End, PgUp and PgDown are separated (rather than in-line integrated). The big Enter button (not halved-one)is really an enjoyment to use as well. The touchpoint enables spotless movement of the mouse cursor and the user has the two groups of mouse keys to its disposal, which taken together with the touchpad brings outstanding control options. The feedback from the keyboard’s keys is great: one just needs to try typing on any newer laptop and then compare the experience. Thinklight keyboard backlight lights-up the keyboard, enabling typing in the dark conditions. Battery life This particular model has a heavy 9-cell non-original, replacement battery which was purchased 5 years ago. The battery life ranged from 8-ish hours when it was new, it degraded pretty fast since then though, so it currently gives around 2 hours of life. Additional point with the 9-cell battery which protrudes on the back side is the feat that the user can hold the laptop with one hand safely (even inverted). Smaller and lighter variants of the battery (4 and 6-cell) were also available. Upgradable Just like an ordinary desktop PC, this line of laptops was made upgradable. I think that the CPU itself is not soldered on this particular model; in addition, the memory slots are easily accessible; user can replace a hard-disk as well and add other devices through the PCMCIA/aforementioned mPCIe/memory card slot, as well as through ordinary USB ports. A docking station called Ultrabase adds plethora of ports including Display Port, 4 USBs and CD/DVD unit which essentially transforms this tiny laptop into the fully-fledged PC. Cons Screen The only significant downside to this machine is the very faint and incapable display. The resolution is passable for the time; however the brightness and the viewing angles are really bad and the color representation is not inspiring to say at least. This was not the laptop for the image/video editing. The screen could be upgraded to IPS which solved much of those problems, but it didn’t provide higher resolution. No HDMI or Display Port Thick and heavy for today’s standards Multimedia features are weak Speakers, microphone and web-cam were mediocre even back in the 2010. They are usable, but the engineers’ focus was obviously elsewhere. ConclusionThis ancient machine has won many computing battles and earned my stripes through reliable, hassle-free, consistent functioning over the 8 years in my possesion. Thanks to Linux, it is still a viable machine for most of my use-cases and I intend to use it for a foreseeable future. Hats off to classic Thinkpads!" }, { "title": "Connecting Python to MySQL and tips to avoid SQL injection attacks", "url": "/posts/connect-python-to-mysql-avoid-injection/", "categories": "python, mysql, database", "tags": "python, databases, mysql, injection attack, security", "date": "2022-10-08 00:00:00 +0200", "snippet": "To quick-connect Python program with a MySQL, one needs to consider following: MySQL driver needs to be installed, connection object needs to be created, cursor object needs to be created, the query should be executed.Of course, Python can connect to most databases, yet it seems that the most-preffered database is MySQL, for the reason of being powerful, free and open source, just like Python is.Steps to followMySQL driver connects MySQL database/server and Python program. It is installed either during MySQL install process, or separately by using PIP:python -m pip install mysql-connector-python In any case, to check whether the MySQL is installed or not, following is typed into Python prompt (no error after executing means it is working properly, otherwise re-run the installation):import mysql.connector In order to link the database and the Python program, the mysql.connectir provides the connect() method:Connection_object= mysql.connector.connect(host = &lt;hostname&gt;, user = &lt;username&gt;, passwd = &lt;password&gt;) Hostname: localhost or IP address Username: name of the user that can access the database Password: password that has been associated with the particular user Database: name of the database to connect toPurpose of the connection object is to provide multiple working environments using same connection to the database:import mysql.connector db = mysql.connector.connect(host=\"localhost\", user=\"root\", passwd=\"admin123\", database=\"testdatabase\")The cursor() function creates the cursor object which then further uses methods like execute and rollback. Inside those methods the SQL queries are run:cursor=db.cursor()cursor.execute(\"CREATE TABLE Food (name VARCHAR(50), price int UNSIGNED, foodID PRIMARY KEY AUTO_INCREMENT)\")cursor.execute(\"DESCRIBE Food)Display the data by looping through the cursor object:for x in cursor: print(x)This is pretty straightforward and the syntax is similar for other databases. However when writing SQL code one should be aware of SQL injection attacks which are very common as they use SQL code to manipulate databases in an unwanted way. Usually, attacker has the access to the program’s code or part of it, but it can’t access the database directly, so it is trying to gain unwaranted access through the program’s (in this case Python’s code).Preventing SQL injection attacksAlthough this is a broad topic, the aim of this article is to show how to connect to a database from Python and give a few important tips that can prevent SQL attacks. Sanitization is a common termn meaning typing such a code that prevents itself from being exploited. For instance, using execute() method with %s in place of each variable while passing the value by the list or tuple as the second parameter of execute():cursor=db.cursor()max_price=[10]cursor.execute(\"\"\" SELECT flour FROM Food WHERE price &lt;%s \"\"\", (max_price))This would be insecure:cursor=db.cursor()cursor.execute(\"SELECT flour FROM Food WHERE price &lt;10 \")Because the variable is encoded separately in a list or tuple, it can’t be changed by manipulating SQL code, and the characters are excaped properly, using triple quotes (\"\"\") as well as the comma character instead a %. Of course, there are a lot of other ways to prevent SQL injections but maybe the best ones are done when writing the code." }, { "title": "Determining the most frequently occuring item in a sequence", "url": "/posts/determining-most-occuring-item/", "categories": "python, algorithms", "tags": "python, functions, arguments, parameters, counter", "date": "2022-09-28 00:00:00 +0200", "snippet": "A sequence is a positionally ordered collection of items. It consists of elements which can be refered to using the index number. In the most programming languages, Python included, the elements’ numbers are in the range 0-(n-1), meaning if the sequence has 10 elements (or its length is 10), starting index is 0 and the last is 9.In Python, most common sequence types are: lists (mutable), strings (immutable), typles (immutable).Now, very common problem is, given the sequence of items, to determine the most frequently occuring items in it.song=[\"Been\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Been\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Keep\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Keep\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\",\"Tell\", \"me\", \"why\", \"are\", \"we\", \"so\", \"blind\", \"to\", \"see\",\"That\", \"the\", \"ones\", \"we\", \"hurt\", \"are\", \"you\", \"and\", \"me\",\"Tell\", \"me\", \"why\", \"are\", \"we\", \"so\", \"blind\", \"to\", \"see\",\"That\", \"the\", \"ones\", \"we\", \"hurt\", \"are\", \"you\", \"and\", \"me\"]from collections import Counterword_counts=Counter(song)top5=word_counts.most_common(5)print(top5)Output:[('spending', 4), ('most', 4), ('our', 4), ('lives', 4), ('Living', 4)]Python has a solution in a form of embedded collections.Counter class which does exactly what we are looking for. The method most_common() returns the most frequent occurences. It can count any sequence of hashable input items, since a Counter class is a dictionary itself that maps the items to the number of occurences.print(word_counts['spending']) # outputs 4For additional set of data:additional=[\"I\", \"guess\", \"they\", \"can't\",\"I\", \"guess\", \"they\", \"won't\", \"I\", \"guess\", \"they\", \"front\", \"guess\", \"guess\"]for word in additional: word_counts[word]+=1top5=word_counts.most_common(5)print(top5)Output:[('guess', 5), ('spending', 4), ('most', 4), ('our', 4), ('lives', 4)]There is an update() method which does the same thing:word_counts.update(additional)We can use addition and subtraction operations:a=Counter(song)b=Counter(additional)print(a)Output:Counter({'spending': 4, 'most': 4, 'our': 4, 'lives': 4, 'Living': 4, 'in': 4, 'a': 4, \"gangsta's\": 4, 'paradise': 4, 'me': 4, 'are': 4, 'we': 4, 'Been': 2, 'Keep': 2, 'Tell': 2, 'why': 2, 'so': 2, 'blind': 2, 'to': 2, 'see': 2, 'That': 2, 'the': 2, 'ones': 2, 'hurt': 2, 'you': 2, 'and': 2})print(b)Output:Counter({'guess': 5, 'I': 3, 'they': 3, \"can't\": 1, \"won't\": 1, 'front': 1})c=a+bprint(c)Output:Counter({'guess': 5, 'spending': 4, 'most': 4, 'our': 4, 'lives': 4, 'Living': 4, 'in': 4, 'a': 4, \"gangsta's\": 4, 'paradise': 4, 'me': 4, 'are': 4, 'we': 4, 'I': 3, 'they': 3, 'Been': 2, 'Keep': 2, 'Tell': 2, 'why': 2, 'so': 2, 'blind': 2, 'to': 2, 'see': 2, 'That': 2, 'the': 2, 'ones': 2, 'hurt': 2, 'you': 2, 'and': 2, \"can't\": 1, \"won't\": 1, 'front': 1})d=a-bprint(d)Output:Counter({'spending': 4, 'most': 4, 'our': 4, 'lives': 4, 'Living': 4, 'in': 4, 'a': 4, \"gangsta's\": 4, 'paradise': 4, 'me': 4, 'are': 4, 'we': 4, 'Been': 2, 'Keep': 2, 'Tell': 2, 'why': 2, 'so': 2, 'blind': 2, 'to': 2, 'see': 2, 'That': 2, 'the': 2, 'ones': 2, 'hurt': 2, 'you': 2, 'and': 2})ConclusionCounter class is very useful tool when it comes to counting the data in sequences. It is an implemented solution which should be preferred to user-written, manual solutions involving dictionaries." }, { "title": "Functions' arguments in Python", "url": "/posts/function-arguments-python/", "categories": "python, functional programming", "tags": "python, functions, arguments, parameters", "date": "2022-09-27 00:00:00 +0200", "snippet": "Arguments, as pieces of information passed to a function provide a way to insert values into function’s mechanism. In Python, function arguments directly depend on object reference model, as there is a stark difference in behavior of immutable and mutable arguments. At this point, a linguistical remark is needed to differentiate between argument and parameter: although those two terms are usually used interchangeably, strictly speaking, when declaring function we use parameters, while upon a function’s call we pass arguments.Python uses number of matching rules when differentiating passed arguments. Those rules are, as follows: positional arguments are matched from left to right. This is the most widely used case, for example: def func(a,b,c): print((a+c)/b)func(3,9,12) # 1.6666666666666667func(3,12,9) # 1.0 keyword arguments are matched by given name, instead of position. For instance: def func(b=20,a=5,c=22): print(a,b,c) func() # 5 20 22func(3,12,9) # 12 3 9 arguments with defaults specify default values if the function’s call doesn’t contain all specified arguments (parameters) def func(a, b=10, c=20): print(a,b,c)func(12) # 12 10 20func(12, 15, 22) # 12 15 22 star expressions (varargs): arguments preceeded with * or ** characters to collect an arbitrary number of arguments def func(*args): print(args)func(10) # 10func(1,2,3,4) # 1,2,3,4 ** feature works for keywords and collect them into a new dictionary: def func(**args): print(args)func(x=5, y=10) # {'x': 5, 'y': 10} varargs unpacking. Passing arbitrarily number of positional or keyword arguments def func(a,b,c,d): print(a,b,c,d)args =(0,1)args+=(3,5)func(*args) # 0 1 3 5 Using the same function but providing a dictionary instead and **: def func(a,b,c,d): print(a,b,c,d)args={'a':0, 'b':5, 'c':10, 'd':15}func(**args) # 0 1 3 5 keyword only: arguments that must be passed by name def func(a, *b, c): print(a, b, c)func(1,2,3) \"\"\" TypeError: func() missing 1 required keyword-only argument: 'c'\"\"\"func(1,2,c=3) #1 (2,) 3 Now let’s explain difference between immutable and mutable arguments. Consider the following code:def func(a): a=10b=20func(b)print(b) # 20Here the variable a is assigned the object with the value od 20 on the function’s call. Although inside a function’s definition the object a changes to 10, changes don’t have effect on function’s call as they only reset the local variable to another object. In contrast with C/C++, Python lacks aliasing; meaning that assignment to a parameter name defined in function (a=10) does not change variable b upon the function’s call. Since arguments are, in fact, pointers to the objects, they can’t share same objects permanently – when it comes to immutables (see this article).However, when mutable objects (lists, dictionaries, class objects) are passed as arguments, in-place changes will probably occur, impacting the callers as well. For instance:def func(x,y): x=5 y[0]=\"test\"X=1C=[2,4]func(X,C)print(X,C) # 1 ['test', 4]In the upper code, the func function assigns values to a immutable and mutable variables. Since x is a local variable in the function’s code scope, x=5 has no effect upon function’s call as previously explained – all it does is changes the local variable a to another’s object reference. Y is a local variable but since the list is the mutable object (see explanation), statement y[0]=”test” is the in-place change which impacts the value of the corresponding list’s element. This is the reason why this change outlives the function’s call and impacts the list C. If those in-place changes are not wanted, we could copy the list so the original one isn’t affected:def func(x,y): x=5 y=y[:] y[0]=\"test\"Summarizing Python’s argument matching rules, here is the complete list of possible cases: def func(name): matches arguments by position or name def func(name=value): defines default argument value (if not passed in function’s call) def func(*name): matches and collects remaining, possibly multiple, positional arguments in a tuple def func(**name): matches and collects remaining, possibly multiple, keyword arguments in a dictionary def func(*other, name): defines arguments that must be passed in calls by keywords func(value): matching by position in function’s call func(name=value): matching by name in function’s call func(*iterable): pass all objects in iterable as individual positional arguments func(**dict): pass all key/value pairs in dict as individual keyword arguments" }, { "title": "Functions and scopes in Python, Part 2", "url": "/posts/functions-scopes-python-pt2/", "categories": "python, functional programming", "tags": "python, functions, scopes, def, return, nonlocal, global", "date": "2022-09-01 00:00:00 +0200", "snippet": "The usual way to get an output from function is a return statement. As we have seen in the part 1 of this article, once the interpreter reaches the return statement, it returns the value and ends the processing of the function’s body. At that point, all scope internal to the function is being destroyed and nothing after that, with an exception of return value, is being retained or remembered.Yet, there is another way to code functions that would return values in a consecutive way: those are generator functions and are available in many programming languages, Python included. Such generator function’s distinction is an output type: instead of single value, they return a generator object which can be iterated. Let’s examine more closely how they function.Yield statementWhile usual functions return value and are done with any computation or memorization afterwards, generator functions can remember their state (scope and variables’ values); thus, they can be stopped and later resumed. Therefore, when being suspended, they yield the value, temporarily stop the execution which can be resumed because the state is saved. Yield can be seen as a consecutive return because it returns the value in a similar manner, but it can be resumed. This allows the function to output a series of values throughout the function’s run-time, in a contrast to returning all the work immediately and exiting like return functions do. There are particular usage-cases for generator functions which we’ll later more closely examine.We mentioned that generator functions output generators which can be iterated. It turns out that they share common traits with the iteration protocol. In Python, iterator objects implement a next method which iterates one step forward until the end of iteration range is reached, raising an StopIteration exception to mark the end of iteration. All iteration methods use this iteration protocol which is also used by generator functions, making them in a sense a variation of iterators. The generator object which is the output of generator function supports the iteration protocol and aforementioned next method, which is being run until either the next yield value or StopIteration exception. To illustrate this, consider following example:def gen_func(a): print(\"First iteration\") yield a a+=1 print(\"Second iteration\") yield a a+=1 print(\"Last iteration\") yield ab=gen_func(3)next(b)next(b)next(b)Output:First iterationSecond iterationLast iterationAs we can see, by using yield returns, the local scope is remembered, so when the next method is used on a generator function object, it actually iterates one step further, as if it would in any iterative process (in order to output yield value to the console print method should be used). When next reaches end of iterative range, it finishes and it can’t be called again using the same function’s call.If we try to print out the object:print(b)The output we’d get is:&lt;generator object gen_func at 0x000002595D926420&gt;To get and display the yielded values from the object we need to iterate them and use print:for i in gen_func(3): print(i)Output:First iteration3Second iteration4Last iteration5We can put them in a list as well:print(list(gen_func(3)))Output:First iterationSecond iterationLast iteration[3, 4, 5]We could get the same results as in the previous example by using a standard function with returned list of generated values. However, when it comes to processing larger data sets, they may have an advantage in terms of memory use and performance since they don’t execute in the moment and completely but rather in consecutive steps. By doing so they actually distribute the resources (computational power which equals time as well as the memory space) over a number of iterations.Consider this example:import timeitdef sq(n): lista=[] for i in range(n): lista.append(i**2) return listadef gen_sq(n): for i in range(n): yield i**2normft=timeit.timeit('sq(1000000)', globals=globals(), number=5)genf=timeit.timeit('gen_sq(1000000)', globals=globals(), number=5)print('Exec time of a normal function: ', normft)print('Exec time of a generator function: ', genf)print(\"Exec time of a generator function is faster on average \", normft//genf, \" times\")Output:Exec time of a normal function: 1.7032738999696448Exec time of a generator function: 4.499917849898338e-06Exec time of a generator function is faster on average 378512.0 timesFunctions sq(n) and gen_sq(n) do the same thing, which is computing a range of numbers’ squares. The first function makes all the computation and saves the results in the list while another creates generator which does the same computing but without using memory, hence the enormous execution time difference. Therefore, generators really shine when processing large data input.Another possible usage is infinite streams. As generator functions don’t create any data structure it is possible to generate infinite iteration stream within a function. In the following example we demonstrate the way to generate infinite stream outside and inside the function, using generators. When executed, both codes will loop infinitively, but calling the function gives more flexibility, for instance using it with if branching, or inside another function or class, etc.while True: a=0 a+=1 print(a)def inc_nums(): n = 0 while True: yield n n += 1Another example of using generator functions would be reading a very large textual file (.txt, .csv, .json and similar formats where data is separated via a newline) to extract a number of rows. We could use a normal function to do so:def text_reader(file_name): file = open(file_name) result = file.read().split(\"\\n\") return resultrow_count = 0text_gen = text_reader(\"some_file.txt\")for row in text_gen: row_count += 1print(\"Row count is \", row_count)This will function fine as long as the input data’s size is modest at most because the program processes whole input file at once. If we convert it to generator function, it will be able to process input files regardless of the file size, as the generator would process the data in a sequence manner sending back only specific value:def text_reader(file_name): file = open(file_name) for row in file: yield rowrow_count = 0text_gen = text_reader(\"some_file.txt\")for row in text_gen: row_count += 1print(\"Row count is\", row_count)LambdaMany popular programming languages, Python included, provide another way to create functions. That is lambda expression. It is used to create small, inline functions which are unnamed (although a name could be assigned but it’s not mandatory) and as such, executed without a call. Lambda is a single-liner, opposed to a block of statements when using def. Lambda is not a statement but an expression and as such is possible to appear inside places where def couldn’t (as an argument in function call, for instance).This code returns the same values:a=lambda x,y: x*yprint(a(5, 10))def mt(x,y): return x*yprint(mt(5,10))Output:50General form of lambda is:lambda [arg1, arg2,… argN]: expression Names assigned inside lambda retain their scope, as seen from the following example:def names(): first_name='John' full_name=lambda x: first_name+' ' + x return full_namefirst_client=names()name_surname=first_client('Smith')print(first_client)print(name_surname)Output:&lt;function names.&lt;locals&gt;.&lt;lambda&gt; at 0x0000023F61CCA4D0&gt;John SmithAs a shortened version of def, lambda expression is mostly used where statement can’t be inserted but expression can, as in jump tables:# this code uses lambda as a list literal, where def can't golista=[lambda x: x**2, lambda x: x**3, lambda x: x**4] for a in lista: print(a(3))# code below outputs the same yet uses def# def square(x): return x**2# def cube(x): return x**3# def quad(x): return x**4# lista2=[square,cube,quad]# for a in lista2:# print(a(3))Output:92781In Python, lambda is often used in connection to higher-order functions; more precisely lambda is used as an argument along with functions like filter() and map().Using filter():lista=[-10, -9, -8, -7, -6, 6,7,8,9,10]lista2=list(filter(lambda x: (x&lt;0), lista))print(lista2)Output:[-10, -9, -8, -7, -6]Using map(): lista=[-10, -9, -8, -7, -6, 6,7,8,9,10]lista2=list(map(lambda x: (x**2), lista))print(lista2)Output:[100, 81, 64, 49, 36, 36, 49, 64, 81, 100]ConclusionIn this article series we’ve touched upon the basic premises of functional programming in general, as well as some of the basic functional elements in Python. Generator and anonymous functions are more advanced concepts with which a programmer should be at least familiar, because they can be used either to shorten, or in some cases to optimize the code and as such, it is useful to get acknowledged about their existence, structure and common usage cases." }, { "title": "Sum of intervals Codewars challenge", "url": "/posts/Sum-intervals-codewars-challenge/", "categories": "python, overlapping intervals", "tags": "python", "date": "2022-08-29 00:00:00 +0200", "snippet": "Sum of intervals is the Codewars challenge that is about counting overlapping intervals. Let’s examine the problem’s description and see if we can devise a valid solution to it. Figure 1: Screenshot of the Codewars challenge Description Write a function called sumIntervals/sum_intervals() that accepts an array of intervals, and returns the sum of all the interval lengths. Overlapping intervals should only be counted once. Intervals Intervals are represented by a pair of integers in the form of an array (in Python it will be a list of tuples). The first value of the interval will always be less than the second value. Interval example: [1, 5] is an interval from 1 to 5. The length of this interval is 4. Overlapping Intervals List containing overlapping intervals:[[1,4], [7, 10], [3, 5]]In Python:[(1,4), (7, 10), (3, 5)]The sum of the lengths of these intervals is 7. Since [1, 4] and [3, 5] overlap, we can treat the interval as [1, 5], which has a length of 4.Examples:sumIntervals( [[1,2],[6, 10],[11, 15]] ) =&gt; 9sumIntervals( [[1,4],[7, 10],[3, 5]] ) =&gt; 7sumIntervals( [[1,5],[10, 20],[1, 6],[16, 19],[5, 11]] ) =&gt; 19sumIntervals( [[0, 20],[-100000000, 10],[30, 40]] ) =&gt; 100000030Tests with large intervals Your algorithm should be able to handle large intervals. All tested intervals are subsets of the range [-1000000000, 1000000000].Thinking about the problemIt seems that there are two ways to approach this problem. First one is iterating through the intervals, putting them into a suitable data structure, filtering out the duplicates (or processing it with some sort of unique-outputting function) and counting the occurrences. This is easier to write but will surely be slow to execute. Another way would be to consider given intervals’ limits, using them as a comparing points between intervals. This requires only iteration and comparison of the start and end points (instead of iterations of whole range of values), which will take a bit more code but will be much faster to run.Brute-force solution (slow performance)def sum_of_intervals(intervals): lista=[] for x in intervals: for y in range(x[0],x[1]): lista.append(y) return len(set(lista))This iterates the input list two-folds; firstly it iterates the tuples in the list, after that it iterates the range within the tuples and puts them into a list. The code outputs the length of a set — list is converted to set to filter out the duplicate values. It is a fast solution to write but it will be slow with larger input values as the time to execute is O(n²). See here for explanation about a Big-Omega notation:Performance-efficient solutionIn this case, enhancing performance means avoiding iteration through the whole range of intervals. Instead, using the intervals’ limit values and comparing them should solve the problem much more efficiently.def sum_of_intervals(intervals: list) -&gt; int: sum=0 intervals=sorted(intervals) a=intervals[0][0] b=intervals[0][1] sum += b - a if len(set(intervals)) == 1: return sum else: for j in intervals[1:]: if j[0] &gt; b: sum += j[1] - j[0] a = j[0] b = j[1] elif j[0] &gt;= a and j[1] &lt;= b: continue else: sum += j[1] - b b=j[1] return sumThe key to devising this solution is detecting whether the input contains overlapping intervals, and if so, how to manage them. The list is sorted so that algorithm iterates through the intervals’ limits properly. After that, if the length of an input data with unique values (taking into account possible tests with repeating the same values) is one, a computation is easy and it returns the difference of the two values at the beginning and the end of interval. If that is not the case, we have three cases: if the second and other tuples’ first values are larger than second value of the first tuple (we split the interval using slicing): then simply subtract them and add the result to the sum variable, if the first value of second tuple onwards is larger than first value of first tuple and the second value of second tuple onwards is smaller than the first tuple’s second value, don’t do anything with this case as it needn’t be counted. For example, if the first tuple is (1,4) and another one is (3,2), it is enough to take into account only first one, as the second is irrelevant from the description of the problem, everything else represents overlapping intervals so the second value represents the boundary of the interval that needs to be compared and subtracted.This solution is much faster as it doesn’t care whether the input data is in the range of tens, hundreds or millions as it only takes into account end values of intervals: the subtracting operation has the same speed if subtracting 1 or 1 million. On the other hand, since we iterate here only the numbers in the tuples, not their range, only possible limitation would be if we’d have lists of millions of tuples to check out, which is a non-realistic scenario anyways." }, { "title": "Functions and scopes in Python, Part 1", "url": "/posts/Functions-scopes-Python-pt1/", "categories": "python, functional programming", "tags": "python, functions, scopes, def, return, nonlocal, global", "date": "2022-08-26 00:00:00 +0200", "snippet": "Functions are integral part of most programming languages. Together with classes in OOP-supported languages, they make the code more readable, concise, usable, maintainable thus greatly enhancing the program design. Programming without functions today would be impossible: developers would have to copy and paste code, to repeat it everywhere, creating so-called spaghetti code which would be unreadable and impossible to maintain. In fact, in Python, understanding how functions work is probably more important than classes comprehension, or at least it is a prerequisite to understanding OOP.Basic theory behind functionsA function is way to group statements into one coherent notation defined by its name, which can then be used and run multiple times later. It can actually do more than being run; it could return a value based on a processing of given inputs in some particular way. Furthermore, functions can be nested and called within themselves, which we’ll explore a bit later.Benefits of grouping the code into one statement are: Increase of readability and conciseness, as the code is written once and may be easily implemented (called upon) multiple times, Enhanced usability and maintainability, as changing the function in the one place produces changes everywhere else where is called, Minimizing redundancy thus shortening the code, Splitting complex code into manageable, well-defined parts leads to a better program design, achieving procedural decomposition.Diving into codeIn Python, functions are created using def statement:def function(par1, par2,… parN):\tsome code\treturn value # optionalThe def statement creates a function with dedicated, user-chosen name with one or more parameters that are optional. Function can have a return value, if it computes/produces something when being run. When the interpreter reaches a return statement inside a function, it ends the function’s call. If there is no return value, the interpreter supposes None and returns it. Other than return, the function can have yield statement, which will be discussed in pt. 2 of this article. When function is being created, it is not being run (called): function assignment does not imply its execution, which is a separate statement. On the function’s call, arguments are being passed to it (note that the parameter and argument are the same thing, with a commonly accepted linguistic distinction - parameter is what is defined on function’s declaration, while argument is being passed to the function when it is run):function(a,b):\treturn a+bIn Python, the def can be seen as assignment (=) as it assigns a name to the code block within. So, it can be used everywhere where it is possible to assign a variable (more precisely a name, because Python uses names referenced to underlying objects). Therefore, functions can be assigned within the particular module, or inside an if statement (but not inside an iteration statement: for that purposes a lambda can be utilized which will be explained in the part 2 of the article as well).A function can be assigned to a separate variable, for instance:def printing(x):\tprint(x)prnt=printing(“Hello World”)In this case function is being called through the assigned name.Consider this function and its calls:function(a,b):\treturn a*bprint(function(3, 4)) # outputs 12 (integer)print(function(3, 4.3)) # outputs 12.89999 (float)print(function(3,’a’)) # outputs ‘aaa’ (string)Since Python is dynamically typed, it does not care about the types but about interfaces. In the example above, multiplication means either mathematical multiplication, or repeating. The obvious question arises here: how Python knows which operation to apply? The answer is wide-spread polymorphism, which is everywhere in Python. The underlying object’s interfaces decide how to react on a given operation: if the object has the expected interface (or protocol), it will run. That is the reason why the multiplication runs without problems with various data types (integers, floats, strings), as they internally can manage it. On the other hand, if they don’t support it, the exception will be raised:def function(x,y): return x+yprint(function(3,'a'))Output:TypeError: unsupported operand type(s) for +: 'int' and 'str'ScopesLet’s consider a simple example:a=22def number(): a=23 print(a)number(a)Output:23Another one:a=22def number():a=23print(a)number()print(a)Output:2322In order to understand the output results of the previous examples, the concept of scope needs to be discussed. It is obvious that the declared (or in Python assigned) variables (names) have their ‘space’ of validity, that is, there exists an internal mechanism which tells the interpreter when to consider which variable. A certain hierarchy can be seen from the provided examples: it seems that the variables have their defined places of validity. That is scope: it is a system which prevents variables with same names interfering with one another. Scope is directly linked with the functions, as we may have the same-named variable in the root of our program (module) and in the function. Furthermore, as previously mentioned, we can nest multiple functions.Python approaches this problem by defining a namespace – literally a place in memory where names reside. The determining factor of how Python treat a name is the place of its assignation in program’s code. According to the places of assignation, variables in Python adhere a hierarchical order according to which they can be local, nonlocal (or enclosing), global and built-in. Figure 1: Scope hierarchy in PythonThis scheme gives the answer to the question on how Python knows what variable to use: If the variable (or rather name) is used (but not declared/assigned) within the function, Python will first seek the local scope, then local scope(s) of any enclosing function (nested functions), then global scope, and finally, built-in scope. The interpreter stops at the first place the name is found (if the name isn’t found at all, it returns an error back to the user). If the name is assigned within a function, Python creates the name in the local scope of that function, with exception if the name is declared global or nonlocal. Additionally, every class creates local scope so the names living there are considered local to that class. If the name is assigned outside of function, it is considered as a global (valid within the file/module), essentially local scope becomes global one. If the name does not exist in the first three categories, Python finally searches built-in names module (builtins). In the current version, there are 156 names inside it and they refer to some errors/exceptions as well as variable/data structures assignment names.Local and global scopeFrom the previous discussion it is clear that the local scope is already defined as name space inside either a function or a class. It is separated from other scopes and as such it is a narrow, limited one. Global scope is however native to the module, that is, to the root of the file itself:a=22def number(): a=23 return a print(number())print(a)Output:2322Here we assigned two names, one is global to the file (a=22), other is local to the function (a=23). When the function is called, it considers the first name which is found in the hierarchy, which here is a local one, hence output 23. Name a within the module itself remains unchanged so it outputs 22 as a second value.Now let’s introduce the global statement and slightly alter the previous code:a=22def number(): global a a=23 return a print(number())print(a)Output:2323The global statement overrides the name defined inside the module: here we implicitly tell Python to consider only name which is declared global, regardless of the fact that it has been already declared within the function. Names declared global are global within the module/file, which means their value is the same everywhere – bringing advantages but some disadvantages as well. Consider another variant of this code:def number(): global a a=23 return a print(number())print(a)Now we assigned name only once – but it is clear that we can access the name inside or outside the function, that is, anywhere in the module itself. Clear advantage is that we can use global statement when we want local scope (function) to be preserved and used throughout the module. Disadvantages may include unpredicted behavior and hard-to-trace errors, as global statement could be used multiple times:def number(): global a a=23 return a print(number())global aa=22print(a)Output:2322Therefore, there is a reason names inside functions have local scope, and names within module have global scope. Additionally, there is a good reason why global statement is implicit, as it produces the global consequences users should be aware of. One last remark about the global statement is that global name always is global inside one module: they can’t be global in more modules at the same time – ‘truly’ global variables in the C/C++ sense do not exist in Python.Enclosing (nested) scope and nonlocalIt is possible to define functions within functions, in that case multiple local scopes are being created:a=22def n1(): a=23 def n2(): a=24 print(a) n2()n1()Output:24In the example above, there are three variables with the same name, all within separate, different respective scopes. More precisely, we have one global scope (where a=22) and two enclosing ones (a=23 and 24, respectively). The output is 24 because second local scope is taken into account only, as the interpreter finds the name in that space first. The nonlocal statement allows scope to become global, but only one step in the upper hierarchy:a=22def n1(): a=23 def n2(): nonlocal a a=24 n2() return aprint(n1())Output:24Without nonlocal statement, this code would output 23, as return considers n1 function space. With nonlocal, it raises the scope one level up. So, it rather acts as a limited global statement, when we want to access/manipulate the value of the name declared within one function, calling it from a function that is one level higher.Summing it all upTaking aforementioned discussion into consideration, we will portray an example of functional design taking into account multiple scopes. Consider a task of finding multiple prime numbers between the given numbers’ interval. The task consists of choosing the start number, an end number and the advancement step, printing out the Boolean values and counting the number of primes found. For example, in the number range of 9 and 20 every number needs to be checked against a prime test. The algorithms here are not important (other than they function properly) nor the output formatting; it is the program design using functions that we want to depict here:import mathdef is_prime(num): if num&gt;1: for i in range(2,int(math.sqrt(num))+1): if(num%i)==0: return False else: return True else: return Falsedef choose_nos(start,end,step): global lista lista=[] for i in range(start, end, step): lista.append(i)def print_primes(lista): global lista2 lista2=[] for i in lista: print(is_prime(i)) lista2.append(is_prime(i))def count(lista): global countTrue countTrue=0 for i in lista2: if i==True: countTrue+=1 return countTruedef run(start, end, step): choose_nos(start, end, step) print_primes(lista) print(count(lista))run(9,20,1)Output:FalseFalseTrueFalseTrueFalseFalseFalseTrueFalseTrue4Without using functions, the code would be very messy, redundant, probably at least twice longer and pretty much hard to read. Instead, the code was grouped into 5 functions; every function relates to the segment of program that is dealing with particular sub-problem. The last function’s purpose calls previously assigned functions so the user must only write one line of code to execute it, passing the parameters and getting the result. Notice further how we used global statement on two occasions, enabling the name inside function to be visible to the rest of the code. If we were to change objectives of the code (what the code should output), it would be enough to change it on one place within the particular function and to use it as much times as needed, as the function calls needn’t be changed. Once set-up, we could import this module into another one and reuse the defined functions outside of the module where they’ve been assigned, which would bring us to the field of modular programming.ConclusionThis rather lengthy article outlines basics of functional programming in Python; discusses the elementary syntax and touches upon the rudimentary issues of program design. As successful implementation of functions implies understanding of scopes, the discourse had to be broadened with the explanations of name spaces, scopes and their hierarchy in Python. Rounding up the theory with appropriate examples of program code summarizes this article: its second part will review the yield and lambda statements." }, { "title": "Codewars review - lessons learned after solving 100 challenges", "url": "/posts/codewars-review/", "categories": "competitive coding, codewars", "tags": "competitive coding, codewars", "date": "2022-08-20 00:00:00 +0200", "snippet": "Coding, like any other skill, needs constant improvement, time investment, commitment and honing. Regardless of the knowledge level you’re at, if you don’t practice, your performance in the field will degrade because we humans tend to forget things we don’t repeat, and even if that’s not the case, your competition is certainly doing it, so you are forced to do it in order to keep the pace. While there are many ways to practice coding, one of the best ways is to do something that is: fun, instantly verifiable and comparable, motivating to endure and to keep coming back.Since the coding produces an immaterial output, it is perfect for collaboration, contemplating, sharing and discussing on the Internet. During the last 10-15 years, great online learning and collaboration communities emerged which simplified many of the aspects of dealing with the code, starting from learning, exchanging ideas, collaborating on projects and many more useful things. Figure 1: Codewars.com screenshotCodewars platformFrom the official documentation, Codewars is a “platform that helps you learn, train, and improve your coding skills by solving programming tasks of many types and difficulty levels”. It has elements of a social network because every user gets its user account with viewable code rank, can contact other users and collaborate with them, even can directly compete, solving some particular problem against the opponent. The core material of the platform is some 8000 coding challenges ranked by various criteria, of which probably the most important is the level of complexity. The user solves the challenges and gets rewarding points for every solved problem. Users are then ranked to each other by total number of points and is clearly visible how one stacks up against others. Solutions of the particular problem are not visible until user solves problem (although user can ‘unlock’ the solution and see the solutions forgoing at the same time the reward points). The harder the problem solved, more points are gained and the rank being higher and faster achieved. Users can also submit their own problems, which includes writing test cases for them, discussing solutions, contributing to code sharing platforms like GitHub, and much more. Figure 2: Screenshot of my Codewars profileProsIt is all about algorithms and data structures. Down to the core, coding is resourceful using of various algorithms and appropriate data structures. Reading the theory or watching live or video presentations of others explaining it is certainly good but not good enough to master the skill. Only when you do it itself through repeating, trial and error and correcting your mistakes you will progress. So, when user solves a particular challenge on Codewars he/she can’t see the solutions: in order to gain the points, user must successfully pass the tests which is considered the solving of the problem. Problems may range in complexity, but they all include using one or more algorithms, loops, conditional statements, maybe recursion, regex and so on. In fact, you are free to use whatever you want from the toolbox of the particular programming language in order to pass the tests. When it comes to data structures, you’ll obviously use everything that can solve the problem, including arrays, lists, dictionaries, maps, sets and whatever not. You will perform conversions of various data types, transform data structures, manipulate them in resourceful ways and you will learn A LOT about structures’ mutability, data types, possible uses and limitations, best practices, fastest algorithms and much more. That is true for the more advanced developers as well: regardless of the knowledge you may already possess, you WILL be challenged (Bjarne Stroustrup excluded).Very important feature of Codewars is seeing other people’s solutions. This is BIG, as through reading and analyzing code others have written you will IMENSELY learn. I often found myself in the situation of solving particular problem in a certain way which was at the moment maybe less sound to me, but unable to solve it using the preferred logic. After submitting, I skimmed through the others’ solutions and found the way to solve the challenge in a way that I wanted to. Reading solutions helps VERY MUCH because in you realize that a particular challenge can be solved in a wide variety of ways which opens doors to a new knowledge and points you in the directions you thought prior were impossible.\tTesting comes as a part of the package As every challenge includes a number of tests to check the solution for, you will soon realize that your code needs to pass every included test. If you want to write/submit your own challenges, you will have to include tests as well. Therefore, your understanding about the designing and usage of tests will skyrocket, as you will see various examples of test cases, how they’re structured, for what particular case they are testing and so on. A software testing is very sought skill on the market so by doing these challenges you will advance in that field as well.\tImportance of trial&amp;errorI would argue the attempts and failures in the process of solving something are the most important way of learning anything. Because of abundance of rules, you will fail often. It will surely be frustrating, but through every single fail you will learn, explore the issue, eventually figure out your mistakes and use that knowledge in the next iteration in you favor. You can’t learn to code only by passive engagement (for instance reading a book); that surely helps but does not provide the possibility for an user to actively solve a problem. Active engagement is the way to go, because only by example that YOU have to approach to, your mind starts actually to work, to think about the underlying knowledge and possible solutions. Codewars and similar platforms shine in this regard.\tEasily-verifiable challenges will activate your logic (thinking and reasoning) brain region(s)This obviously counts for every kind of a challenge, but even more if the challenges can be verifiable. For instance, if you have tried to challenge yourself by reading some algebra book and solving some problem found there, you could end up breaking your brain without any incentive on how actually to check where/why your incorrect solution is wrong. I am NOT saying coding challenges are comparable in some particular way to the mathematical ones, but they are certainly MUCH MORE EASILY verifiable: enter the code, click test, tests are run and you instantly get the results. If they are incorrect, you may rearrange your code to display that key variable, or even using an IDE to visualize the execution of your code. Of course, you’d do the same for mathematical problems using some computational technique (MATLAB or similar), which boils down again to verification through executing computer code. If you know you can easily evaluate your solution, you know your hardship will not be in vain as you WILL get the feedback which can gradually point you to the right direction in thinking and ultimately finding a correct solution. On a numerous occasion I caught my brain doing the problem-cracking in unexpected times, for instance during the night: subconscious being activated and pointed in the right direction (although I may stand corrected on that assertion but certainly it seems so to me).\tPersistence through healthy addictionMany say that the key to the success in every field is persistence in doing hard things. But hard does need to be boring at the same time. In fact, if we trick the brain to do the hard things in a fun way, it could be very appealing to us. Codewars achieves that, at least for me: it creates a healthy addiction of solving challenges often and on persistent basis. Firstly, everything runs in the browser, you don’t need to install anything, so you can use it everywhere. Secondly, you are being constantly challenged; since we are naturally curious, you ask yourself what awaits you next. Thirdly, you are starting to advance the concepts and to gain new knowledge, so in connection to the second point, you can always expect to learn something new. So, it creates a new habit, which is for coding enthusiast more valuable and fun than, say, watching TV (which may be occasionally fun but at least for me totally unproductive).Cons\tLacking big pictureAlgorithms and data structures are only an important but rather small subset of programming, or even taken broader, computer science. There are A LOT of equally important or very important things to fit the big picture in, like databases, software version control systems, GUI design, simulations, networking and system administration, cyber-security and encryption, design of systems, OOP and other programming paradigms, etc. Being good at algorithms and data structures is required, but far from being the ONLY condition for a professional developer. Even programmers with a broad range of technical know-how need to have some decent amount of interhuman and personal skills. Being stuck in a basement doing only coding challenges will NOT get you there.\tMiscellaneous I have one technical issue with Codewars: only basic tests are freely accessible. User can’t see additional tests which can be sometimes important, particularly when solving more advanced challenges. It is frustrating not to be able to see the test you’ve failed.My experienceIn a matter of a couple of months, I’ve solved 100 challenges, ranging in complexity from easy to middle ones. I used C# and Python, which are the languages I’m at the moment particularly interested for. Overall, I am satisfied with the results and plan to use the site in the future as well. My biggest gain was the persistence in the coding practice, which I couldn’t get from other sources.ConclusionIn this article I elaborated on the reasons pro et contra to usage of competitive coding platform Codewars. I liked the experience and its features, at the same time being aware that coding is much more than solving prefabricated problems. Like with any other field, it is the balanced approach that is the most fruit-bearing, so be sure to complement coding challenges to research of other topics computer science consists of." }, { "title": "Understanding recursion - Towers of Hanoi", "url": "/posts/understanding-recursion-towers-hanoi/", "categories": "python, recursion", "tags": "python, recursion, towers of hanoi", "date": "2022-08-09 00:00:00 +0200", "snippet": "A recursion is problem-solving method which calls itself in a range of calls until the base one. Only the base call is defined with clear solution; other ones are derived from it. A base call finally terminates the recursive function. Recursive techniques can be short and simple, but sometimes hard to figure out the proper setup. There is number of problems that are traditionally solved by recursion. Note that for every solvable problem it is possible to use either recursion or iteration. Here we present famous puzzle called “Towers of Hanoi”, explain the input premises, define logarithm in pseudo code, and finally craft an implementation of the algorithm (we will use Python). We conclude by evaluating the code, dissecting it step by step.Premises of Towers of Hanoi puzzleSuppose we have three rods, or towers, with a number of disks on one rod. Disks are sorted according to their diameter, from largest one being on the bottom, to smallest one on the top. The goal is to move those disks in the same sorted manner onto another tower, respecting the following rules: The disks are moved in turns. It is allowed to move only one disk per turn. The move consists of moving the one disk from one tower to another one. Towers are marked as a source, destination and auxiliary ones. Their order is not fixed, i.e., all of three can bear any of those three names. It is allowed moving either disk on an empty tower, or, if the tower contains disk(s), only smaller disk may be placed onto bigger one. Figure 1: Generalized depiction of the problem and solutionDiscussing the possible solutionsRecursive problems can be divided to subproblems of similar type. In our problem setup, we label the number of disks as n. The simplest subproblem would be when we have only one disk, i.e., when n=1. Figure 2: Solution where n=1It is obvious that, in the case of n=1, problem is solvable in one step, by simply moving one disk from the source to the destination tower of choice. Other rules are not taken into account as there is only one disk.Going up, if we have two disks (n=2), we have added some complexity to the problem. Figure 3: Solution where n=2The number of steps needed for solving is now three, as we now have to consider how disks are stacked onto each other.When n=3, complexity goes further up. Now the manipulation of disks really is starting to get complicated. Figure 4: Solution where n=3Number of steps now rises to 7.Observing those cases, we can draw some conclusions: the number of cases needed for solving the puzzle is 2^n-1 (which suggests the problem has the exponentially rising complexity), when number of disks is greater than 1, the problem corresponds to the moving of the n-th disk to the destination tower, while n-1 disks are firstly moved to the auxiliary tower in order to be decomposed into single disks which are further moved until they all end up sorted from the smaller to bigger onto a destination tower, the recursive nature of the problem is revealed in the same moving pattern of n-1 disks (regardless of the size of n), which is subdivided into smaller, recursive (sub)patterns of the same nature.Aforementioned conclusions can help us devise the algorithm in pseudo-code: if the n=1, it’s a base case, move n-1 disks from source to the auxiliary tower first, move nth disk from source to the destination tower, move n-1 disks from auxiliary to destination tower.Whenever the disk moves, algorithm should notify the user, ultimately giving the print-out of the steps taken in order to solve the puzzle. The total number of steps should be at minimum 2^n-1, which can be utilized in order to control the validity of the algorithm. The recursive calls should dismount the calls to itself all up to the base case, which is where n=1.Implementation of algorithm in PythonWe start by defining the function and its parameters. The parameters we will pass to the function will be, as previously noted number of disks, and the three towers. Using if check we can check for the base case. So, we literally translate the presented pseude-code to Python code:def hanoi(n, source, dest, aux): if n==1: print(source, \" to \", dest)Now we need to define the else cases. It will be the part where the recursive calls are found.else:Hanoi(n-1, source, aux, dest)print(source, \" to \", dest)Hanoi(n-1, aux, dest, source)Combining it all together:def hanoi(n, source, dest, aux): if n==1: print(source, \" to \", dest) else: hanoi(n-1, source, aux, dest) print(source, \" to \", dest) hanoi(n-1, aux, dest, source)Evaluation of the codeWe test the written function by calling it for cases where n=1 to n=4.hanoi(1,1,3,2)Output:1 to 3This is the base case, we get the expected result, as the disk is transferred from the source to destination tower in one step.hanoi(2,1,3,2)Output:1 to 21 to 32 to 3Here we see how the function is calling itself: Figure 5: Visualization of the code where n=2, using https://pythontutor.com/The total number of steps is 3, which is within the established limit of 2^n-1.hanoi(3,1,3,2)Output:1 to 31 to 23 to 21 to 32 to 12 to 31 to 3 Figure 6: Visualization of the code where n=3, using https://pythontutor.com/Again, the total number of steps is within the parameter’s range.hanoi(4,1,3,2)Output:1 to 21 to 32 to 31 to 23 to 13 to 21 to 21 to 32 to 32 to 13 to 12 to 31 to 21 to 32 to 3The total number of steps is confirmed for this case as well.ConclusionTowers of Hanoi is the famous puzzle which is introduced in the West at the end of the 19th century, yet it was known for centuries before in the Far East. It is one of the prime examples of recursion. If one truly dives deep enough, the beauty of the code calling itself is astonishing: in only a couple of code’s lines we have defined a universal solution to the problem regardless of the size of n. The iterative solution is more complex and not so intuitive like the recursion one." }, { "title": "Garbage collection in Python", "url": "/posts/garbage-collection-python/", "categories": "python, garbage collection", "tags": "python, garbage collection, reference counting", "date": "2022-08-03 00:00:00 +0200", "snippet": "When it comes to memory management, the vital task of any developer is taking care of a memory requirements of given program. This assumes allocating and dislocating the memory occupied by the variables and their data. In the early days of programming, it was a manual task so developer solely was responsible for the memory management. It was laborious job, because every single variable needed to be traced from the declaration/assignation to the point where it may be no longer needed in the code. This meant tracing variables’ attributes like scope, life and their interaction within the code, which all needed to be taken into consideration when removing it from the memory. The bugs occurred often and were sometimes hard to trace. This still stands for major languages like C/C++, although there are some add-ins that offer user-configurable memory management techniques.Garbage collection stands for an automatized memory reclaiming process and it is a standard feature (rather than optional add-in) in the high-level languages developed in the last 30 years. This feature relieves the developer from the manual work and applies one or more algorithms, which run automatically, independently from the user. Garbage collector identifies previously allocated memory spaces (therefore called garbage) which are no more referenced by the program’s variables, and removes them, thus freeing memory.Python’s garbage collector specificationsPython documentation states following:Garbage collection: The process of freeing memory when it is not used anymore. Python performs garbage collection via reference counting and a cyclic garbage collector that is able to detect and break reference cycles. The garbage collector can be controlled using the gc module.From the upper definition we can see that the garbage collection in Python is twofold: Reference counting algorithm takes care of number of references that each object through its life gets. It is an integral part of the Python, it is automatic, constant process running in the background and it can’t be managed from the user side. Cyclic garbage collector was added later up, when it was realized that the reference counting algorithm can’t detect some referencing cases such as reference cycles, where either object references itself or more objects are referencing each other. This garbage collector can be directly manipulated, it runs automatically periodically (but not constantly) in the background, and it can be manually switched off.Reference countingPython is structured in a such way that it does not “hold” any data in variables but rather use them just as names which reference to the actual objects that reserve or allocate actual memory space. Every object has a header field that records actual number of references to it. For the details about this model, see following texts:The joys of Dynamic Typing, Part One - ImmutablesThe joys of Dynamic Typing, Part Two - MutablesWhat reference counting algorithm does is deallocates objects which loose references to the variables (more precisely, when reference count equals zero).Consider following code:Example 1.x = {'price': 'GBP', 'quantity': 'pounds'}y = {'price': 'GBP', 'quantity': 'pounds'}print(x == y) # Trueprint(x is y) # Falseprint(id(x)) # 2295298897536print(id(y)) # 2295298897792We created two dictionaries with the same values. Equality check expectedly returns True, because the values are the same. Identity check however prints out False, which tells us that the objects behind those two dictionaries are not the same, which is confirmed if we print out their memory addresses via id() function. Now if we display reference count of those dictionaries, we get following:import sysprint(sys.getrefcount(x)) # 2print(sys.getrefcount(y)) # 2Number of references is 2 for each object, respectively. One reference is created on assignation, another reference is created by getrefcount() function itself.Example 2.import syslist=[]def func(x): print(sys.getrefcount(x))func(list) # returns 4print(sys.getrefcount(list)) # returns 2We created an empty list and defined a function which returns number of references for the passed argument. When we call the function and pass our empty list, the returned count of references is, maybe surprisingly, four: one reference for assignation of list, one reference for passing an argument, one reference for using function itself, one reference for getrefcount() functionOn the other side, the reference count for the list itself is two: one reference for assignment, one reference for getrefcount() function.Example 3.import sysx = []list = []print(sys.getrefcount(x)) # 2list = [x]print(sys.getrefcount(x)) # 3By assigning an object to another object the reference count increases. To sum it up, there are four ways to increase the reference count: assignment operator, argument passing, calling a function or class, inserting an object to another object.The reference count decreases when applying opposite concepts: reassigning the variable to another object, deleting the variable, exiting the function or class instance (Python destroys references with the variable scope local to that code block), deleting an object within another object.There are some things to be considered though. The variable’s scope is important. If the variable has the local scope (for example, lives only in the function or class), then on exiting from the code block, Python destroys variables with local scope and thus lowers the reference count of their objects in background. Objects are destroyed only when their reference count reaches zero, not on the decrementation of the reference count! The reference count decrementation of one object that is linked to other objects is mutually linked, so when complex object like list or dictionary (containing another objects) is deleted, the reference count for all containing objects is decremented as well. If, however, another object links one or more previously deleted objects, the interpreter will take that into account and update the current reference count of such objects.Variables outside of function/class scope that reside in the program’s “root” are global variables. Their reference count is shielded from the garbage collector as they are stored inside a dictionary and live until the termination of Python’s interpreter. The user can display global variables’ dictionary by using function globals().Cyclic garbage collectorA reference counting algorithm has one major drawback: it can’t handle reference cycles. If one variable references itself, or two or more objects are referencing each other, such situation is called a cyclic or circular reference. The reference counts stemming from cyclic references can’t be detected using reference counting algorithm, hence, additional garbage collector’s algorithm is needed to detect those and to remove them from the memory space.If we assign a variable to itself and delete it afterwards, we can’t access the reference count as the variable is deleted:Example 4.import sysx = []x.append(x)print(sys.getrefcount(x)) # reference count=3del xSince variable x is deleted, we can’t access it using the getrefcount() function to determine what actually happened after deleting and what is the count of its references. Therefore, we must dig deeper and use code that can access memory address of the deleted object directly. In order to justify usage of another garbage collection algorithm, we need to prove that the reference count algorithm does not detect cyclic references.Consider following situation depicted below.Example 5.import sysimport gcimport ctypesclass PyObject(ctypes.Structure): _fields_=[(\"refcount\", ctypes.c_long)]gc.disable() \"\"\" disable garbage collector to demonstrate that reference remains although we deleted variable\"\"\"x = []x.append(x) # x references itselfprint(sys.getrefcount(x)) # reference count=3x_address=id(x)del xprint(PyObject.from_address(x_address).refcount) \"\"\" reference count=1\"\"\"gc.collect()print(PyObject.from_address(x_address).refcount) \"\"\" reference count=0 \"\"\"In this example we used ctypes, a foreign function library which enables using C data types which we will need since we’ll going to access memory addresses directly. The default cyclic garbage collector was disabled, to prevent it from eventual cleaning of the reference counts. Then we assign a memory address of variable x to another variable. Afterwards, we delete the x and print out the reference count of the object that we accessed through its memory address. Since we get one reference returned following is obvious: it is a proof that object still exists, regardless of the prior deletion of the variable x, it is a proof that reference count garbage collector hasn’t detected the cyclic reference.When we manually start the cyclic garbage collector, we demonstrate that it works as it should because now the reference count that is returned equals zero.As previously noted, the cyclic garbage collector runs automatically but not constantly opposed to the reference count collector, it activates itself occasionally. Another difference is that it is completely manageable and configurable, as it can be accessed within the code by using the gc module. Basically, what cyclic garbage collector does is detection of cyclic references and creation of three generations of variables which have different conditions (thresholds). We won’t go deeper in this matter here, rather we will provide some insights of how useful the understanding of the garbage collection could be.Usage tipsBroadly speaking, it is best leaving all as it is, because the garbage collectors certainly justified their existence through evolutionary, incremental advancement steps during the couple of decades. They apply advanced algorithms which, on average, do satisfying job within the specified parameters. Yet, on some specific cases it is possible to tune the Python’s cyclic garbage collector, or to disable it altogether. Sometimes is favorable to avoid cyclic references completely, by using weak references instead. Another tip worth mentioning is to take a look on the global variables in the program’s code as they are untouched by the reference counting. As always, understanding the logic behind the garbage collector reveals a lot about Python’s internals and can help you to better understand the involved processes, ultimately envisioning better code for your needs.ConclusionThis article touches upon the memory management in Python; its automation and garbage collection process. We explained the reasons behind the both implementations of garbage collector in Python, together with valid examples. Additionally, we proved why there is a need for second garbage collector, and finally gave some tips worth considering when designing your code." }, { "title": "The joys of Dynamic Typing, Part Two - Mutables", "url": "/posts/dynamic-typing-pt2/", "categories": "python, dynamic typing", "tags": "python, interpreter, compiler, bytecode", "date": "2022-08-02 00:00:00 +0200", "snippet": "In the first part of this text, we have seen that the defining characteristic of the Python is the dynamic typing; the possibility of creating variables without their declaration but rather with immediate assignation to the value; the easy-going reassignation to another object type and value. We have made the distinction between immutable and mutable variables and explained the variations when managing immutable variables.Now it’s the time to refer to mutables and analyze their behavior to get a complete picture of assignment and reference model which is central in Python.Mutable typesMutable are considered following Python’s objects: Lists.List is a mutable object type in Python which allow storing multiple objects in ordered manner. They are indexed - meaning the order of objects inside list matters and is being tracked internally. Lists are not fixed in size (like static arrays in C languages which Python doesn’t support); they are changeable; allow duplicate values and are mutable. This ensures an excellent flexibility of their use as the internal manipulations of its contents is automated (for instance, list automatically inserts or deletes particular containing object, rearranges the remaining indexes and the length attribute). Mutability means they can change content in place without creating another list, as we will see in examples. Dictionaries.Dictionary is such data structure which allows storing values in key: value pairs. As in lists, order matters, they are changeable but they don’t allow duplicates – its keys are unique. Dictionaries are mutable as well and they are indexed as well, but through indexing one can access key or values, not both at once. Sets.Sets can store multiple items in a single variable which is different from lists and dictionaries which store multiple objects; they are not ordered and not indexed; and are mutable. Consider following statements:List1=[1,2,3,4]List2=List1List1=”Hello World!”This is an example of shared reference without changes inside a list. List1 and List2 link to the same object in the first two lines, then the List1 references another object (with another type) while List2’s reference remains unchanged. Note that we can access the lists’ elements through indexing so List1[0]=1, List[1]=2 and so on.Now we introduce the changes inside a list with shared reference:List1=[1,2,3,4]List2=List1L1[0]=5This code returns following:List1=[5,2,3,4] # List1 changed in place, there was no new object, the old one is being modifiedList2=[5,2,3,4] # List2 also changed since it references the same object Figure 1: List’s mutability, i.e. changes in place result with no additional object being createdIn the upper example we changed only object within the List1 which resulted in a change of the object’s value; therefore, no new object needs to be created. This is important to note because if there are multiple references to the list, they all will be changed as well, which may or may not be immediately understandable. So, when structuring the code, pay attention to the shared references as they will change upon the mutable object’s change.If we wanted different behavior, that is, only change directly affected variables, other with shared references to stay unchanged, we could use some workarounds. For instance, we may copy the value of the list when declaring another one:List1=[1,2,3,4]List2=List1.copy() # using the copy method creates another object# List2=List1[:] using list slicing also creates another object while copying the valuesList1[0]=5After those changes, the lists’ printout looks like:List1=[5,2,3,4] # took change in place, references the same objectList2=[1,2,3,4] # unchanged, references previously, through copying created object Figure 2: List’s mutability on in-place changeEquality and identity checks in PythonAs we create variables with references, some maybe shared between them, it is sometimes useful to check whether variables refer to the same object. Such a test could then be used when determining are two variables’ values and their objects are equal or not. The equality is the situation when variables’ values are the same; the identity comprises the fact if the variables point to the same object. Operator == tests for the equality while is operator tests for object identity. When comparing two variables, following is possible: two variables have the same values, or not, two variables have the same identities (point to the same object), or not, variables at the same time have same equality and identity, or not.Examples:A=[10, 20]B=[10, 20]A==B is true, A is B not true.Using checks on integers, there is one thing worth mentioning regarding identity check. Because Python’s optimizations, if we compare integers’ identity in range -5 till 256, they will return the same object (although they shouldn’t), as such smaller integers are cached and reused: Figure 3: Identity checks in Python’s default IDLE shellHowever, this is not always the case as it seems that in the IDE like Visual Studio Code, interpreter still does some checks as it returns the same identity for the variables with the value of 257, so bear in mind those small grievances when testing your code: Figure 4: Identity check printout in Visual Studio CodeConclusionIn this text we’ve shown the behavior of mutables in the sense of their linkages to objects. Firstly, the mutables were defined and their types in Python explained. Secondly, we have explored their basic traits when reassigning their values and summarized the background changes in object’s references. It is important to test variables for equality and identity so through couple of simple examples we have highlighted some rules and exceptions to those rules, which may be of use when analyzing or debugging the code." }, { "title": "The joys of Dynamic Typing, Part One - Immutables", "url": "/posts/dynamic-typing-pt1/", "categories": "python, dynamic typing", "tags": "python, interpreter, compiler, bytecode", "date": "2022-07-31 00:00:00 +0200", "snippet": "In traditional, statically-typed languages there is a data type classification which needs implicit declaration to every variable, function and OOP object. If the variable’s data type is for instance an integer, it needs to be marked as one prior to the value’s assignation. It is possible to declare a variable in one line, without assigning any value to it. It is however NOT possible to declare a variable and then to re-declare it implicitly as another type, or with another value. Static typing makes things more ordered, clear and defined for the compiler, which reduces the number of needed checks improving the bug-detection rate and compiling time. Downside is the syntactic complexity of a such written code which raises the costs of the development. On the other hand, dynamically-typed languages (of which two probably most used being currently Python and JavaScript) don’t declare variables, they assign them without mentioning of any data type whatsoever. If we instead ofint x=5;simply statex=5one may ask, how does the Python know that variable x is intended to be used as integer?Names, references, objectsIn Python, variable is created when we assign a value to it. There is no declaration of variables prior to the assigning value (btw. proper “pythonic” designation is name rather than variable, but we’ll stick to the latter for the sake of clarity, especially in comparison with another languages). A variable only has its name, the type of it doesn’t exist as the variables are generic in nature and only refer to the particular object created in memory for designated amount of time. This may be baffling to C/C++/Java/C# developers. When a variable is being created with an assignation to some value, the Python immediately creates an object and references a variable to that object. Unassigned variables can’t exist and an attempt to do so leads to an error. More specifically, the statement:x=5does the following in Python: interpreter checks the value, conduct some tests and make a conclusion about the type, object with proper type is created to represent the value 5, variable x is created, more specifically name, reference is created to link variable x with its object.Variables are separated from objects and stored in different parts of the memory while being linked together. Simple variables exclusively link to objects, while more complex objects (for instance lists and dictionaries) link to the objects they contain. References or links are strictly enforced; therefore, Python really uses only names for variables and for that matter a name can’t have a type, but object can. That is the answer to the question how Python knows the type of the variable: it doesn’t, but it does know the type of the object that variable is being referenced to. Objects are spaces in memory but more than that - they have two header fields: type designator, which actually identifies the type of the object, and reference counter, which holds a number of times the object is being referenced.In Python, references are pointers which point to some address in memory (similar to C pointers), but given the important fact that the Python has garbage collector, since the variables are being automatically referenced, they are automatically unreferenced on certain conditions as well – hence, there is no way to use them in such a manner - it is pointless.Because of such structure, it is possible to change the value type of the variable:x=5x=\"This is string\"x=1.01 Figure 1: The variable x can change values and types because in the background it only refers to various objects which are created and destroyedAs described, the variable x doesn’t really change its type but name x changes it’s referenced object which changes the type. Variable, i.e., name does not have the type, it is only an internal notion, but objects through type designator recognize the particular type. What happens when we change the variable’s assignation? Whenever a name being assigned to new object, if the object referenced prior to that loses all references, it is being automatically reclaimed – beforementioned garbage collector kicks in.x=5x=\"This is string\" # if number of references to it equals zero, object with value 5 is being reclaimed x=1.01 # “This is string\" object is reclaimedImmutability explainedThis discussion leads us to the notion of immutability. Broadly speaking, in programming jargon, a variable which value can’t be changed is considered immutable. In Python, immutability is achieved at object level and immutable are core types of objects (partially similar to primitives in C languages): integers, strings, tuples.They can’t be changed in place; therefore, on change of assignation we need a new object to represent the new value and/or data type.Mutable are considered following Python’s objects: lists, dictionaries, sets, most custom created OOP objects.Those can be changed in place as we can see in the part 2 of this article.Multiple (shared) referencesUntil now, we have been assigning values with some values, which creates objects in the background. There is another possibility - assigning variable to another variable:x=5y=xIn this case, there is only one referenced object, which has the value 5. When we say y=x, there is no new object, but what happens is creation of another reference to the existing object. Object with value 5 is shared among two variables, but those variables do not have any connection whatsoever (in stark contrast to C languages); via references they point to the very same object. Let’s broaden this contemplation a bit more, by saying:x=5y=xx=\"Hello World!\" Figure 2: If we assign one variable for another, those variables will not be linked but point to the same object. If we further assign a new value to the first variable, it will point to new object while second one will remain pointing to the previously referenced one.Here the variable x changes reference, so when the last line is typed, Python creates new object with the name “Hello World!” and new reference to it, while previous one is being dereferenced and removed from the memory (if there are no other references linking to it). Note here that the value of the y remains the same: object with the value 5 is being still referenced by y. The same holds true if we don’t change type, for instance:x=5y=xx=6 Figure 3: Same as Figure 2, but without change of the object’s typeThe object we initially created holds the value represented with integer 5, when we created new variable, we added another reference to the same object, and finally, as we assigned new value to the variable x, new object is created while first one remains referenced to variable y. Once again, variables in Python only point to objects and do not represent actual memory spaces, therefore it is impossible for immutable objects to change values.ConclusionDynamic typing is such an important characteristic of Python. It enables the great flexibility and freedom of expression, but it does have its drawbacks as the interpreter must constantly perform numerous checks on runtime, reducing the speed of execution. This article has shown the main behavioral traits of dynamic-typed code in terms of using immutable objects and the corresponding logic behind it. I would argue that while it certainly may be cumbersome to switch from static to dynamic-typed code, but going from dynamic to static scheme is much more demanding, requiring more precision and attention to details." }, { "title": "Behind Python's lines", "url": "/posts/behind-python-lines/", "categories": "python, bytecode", "tags": "python, interpreter, compiler, bytecode", "date": "2022-07-28 00:00:00 +0200", "snippet": "The final product of the user’s code in Python is .py file. On its execution, the source file is passed to a Python as an argument, compiled to the intermediary byte-code and then interpreted in the Python Virtual Machine. A lot of happenings, most of which is hidden to the user. Why care, if the code runs appropriately? There are a couple of reasons: Getting the fine-grains will hone your conceptualization and coding skills. As you go deeper to the more fundamental programming concepts, you will learn a lot; open your mind to the new concepts and broaden your views. Start thinking about the code’s complexity, efficiency and optimization, even if it may sound futile as Python’s engine is already many times optimized. Those are not only theoretical aspects, but can be practical, depending on the usage. For those who will care about the program speed, there could be possibilities to accelerate your program by an order of magnitude – by telling the Python which exactly optimization to apply (and maybe, which one to omit). Academic, professional or personal curiosity to study how things are made. Maybe you want to write your own programming language. Python blendsAs it turns out, Python has multiple implementations written in different languages. When we say Python, which actually is a specification, we usually think on its standard implementation called CPython and written in C and Python. It serves multiple roles: it is an interpreter, a compiler, a standard library, a parser and much more. There are many other variants, like Jython (Python implemented in Java), IronPython (.NET implementation), PyPy (fast-running Python, interpreted and compiled to optimized machine code), Cython (static-typing support, compiling to C code, really fast), Stackless (implemented in PyPy) and so on. In this text we will be considering CPython.Intermediary codeIn contrast to C/C++ which compile directly to machine code on a given platform and execute directly on CPU using its native instructions (therefore being fast as it gets, actually only thing that could be faster would be executing assembly instructions), Python compiles the source code into bytecode, which is assembly-alike, intermediary code with its syntax and semantics which is then interpreted in Python’s virtual machine (not on the CPU). So, Python adds one layer of abstraction between hardware and operating system, executing its bytecode internally on virtual software, achieving universal portability, but loosing speed: while C/C++ code must be compiled prior to run on any particular platform, Python (just like Java) runs platform-independent because it executes virtually. So basically, bytecode is a language hidden behind the Python’s code and developers usually do not bother with it nor the majority of users probably are aware that it exists. That is one of the reasons why Python is a high-level programming language: through automatization and optimization, it hides many steps from the developer, achieving clear, readable, human-language alike syntax which comes at the price of creating overhead and increasing complexity on its executing side which affects performance.Analyzing bytecodeThere is a way to disassemble bytecode by using the dis module. https://docs.python.org/3/library/dis.htmlThe dis module disassembles Python source code into internal, individual instructions which are executed by the interpreter. So, for this basic code:def Func(a): print(a)applyingdis.dis(Func)we get following output: Figure 1: Disassembling function Func()This is an example of disassembling one function, but it may be a complete .py file as well. This outputted code is the way the Python interpreter sees our code: it consists of internal instructions and their names and addresses which correspond to the original source code.import timeitimport disdef fast_compute(): return 100*12*30*24*60*60def slow_compute(): seconds_hourly=60*60 hours_monthly=30*24 months_yearly=12 years_century=100 return seconds_hourly*hours_monthly*months_yearly*years_centurydis.dis(slow_compute)dis.dis(fast_compute)print(timeit.timeit(slow_compute))print(timeit.timeit(fast_compute))Output: Figure 2: Disassembling functions slow_compute() and fast_compute()We defined two functions which compute number of seconds in a year, one function uses variables to store the corresponding time frames, while another computes it directly in the function’s return statement. We then tested the execution time of those functions and provided their corresponding disassembly code. The time of execution for the slow_compute function was 0,23 seconds comparing to the fast_compute one of 0,07 seconds (~3x slower). The slow_compute function has 16 lines of code while fast_compute has only two! The reason for this is that the interpreter knows that product of multiplication does not need to be reevaluated and computed again, while in the slow_function we used a number of variables, each of which needs to be checked in terms of their content as Python is dynamically-typed language which takes additional time when executed in virtual machine. By knowing this we could optimize our code for a three times faster execution, which is an outstanding gain.import timeitimport disdef fast_loop(): for i in range(100): passdef slow_loop(): i=0 while i&lt;100: i+=1dis.dis(slow_loop)print()dis.dis(fast_loop)print(timeit.timeit('slow_loop()', setup='from __main__ import slow_loop'))print(timeit.timeit('fast_loop()', setup='from __main__ import fast_loop'))Output: Figure 3: Disassembling functions slow_loop() and fast_loop()It is noticeable that the for loop is much quicker and with less internal instructions: 1.36 against 4.79 seconds is a huge time difference. This is due to the fact that it is internally optimized to run much faster C code, on the other hand while loop repeats much of its instructions and is interpreted which affects performance.Down to the bytesWe could go even deeper and disassemble the already disassembled intermediary code into the stream of bytecode by using the function.code. For instance:def calculate(): return 50**45+18*2/3print(calculate.__code__.co_code)returnsb'd\\x01d\\x02\\x13\\x00d\\x03\\x17\\x00S\\x00'What those bytes stand for? They are low-level specifications of the intermediate instructions we already seen, which are then being fed into the virtual machine and actually executed. So, your code is dismantled into byte representations and then run on virtual machine which does another level of abstraction by separating the bytecode from actual, native code which all CPUs need to be able to produce a valid output. Long way down from writing “Hello world”!ConclusionThis article gives a broad overview on Python’s interpreter internal dealings. It shows that Python, being a high-level language, consists of multiple layers of abstraction which hide many assembly details from the end user, providing an ease of use, readability, portability, automatic optimization and more. The only drawback is running time which, depending on one’s particular usage, may or may not be the issue." }, { "title": "Algorithms’ complexity analysis - Big-O examples with explanations", "url": "/posts/big-o/", "categories": "algorithms, complexity analysis", "tags": "big-o, algorithms", "date": "2022-07-26 00:00:00 +0200", "snippet": "As Big-O algorithm’s time complexity is very common theoretical concept, investing some time in understanding it can pay off when implementing solution to some problem, because finding time complexity presumes deep understanding of the code behind it. Important thing though is not to overdo it, because estimating time complexity of more complex algorithms can be more resource-consuming than the coding itself. Another worthy guideline is to first devise any solution to the problem, then according to the needs and priorities of the current project contemplate about optimization, not the vice-versa.In this article we discuss the Big-O time complexity analysis of 10 algorithm implementations in C#, Java, Python and pseudocode.Example 1.for ( int a=0; i&lt;arrA.length; a++) { for (int b=0; i&lt;arrB.length; b++) { System.out.println(a+\",\"+ b); } }Here we have two loops iterating over the two different arrays. Since we don’t know anything about those arrays, the total worse time of execution is a product of A and B, O(A*B). If we had two loops iterating within the same array, we’d had O(n^2). Now consider we have two separate loops that iterate over different arrays:for(int a=0; i&lt;arrA.length; a++)) { System.out.println(a); }for(int b=0; i&lt;arrB.length; b++) { System.out.println(b); }We simply add those loops and say O(A+B). Again, there is no way to simplify under that as the details on arrays A and B are unknown. Analogous, if we’d had two loops that iterate on the same array, we could then simplify and state: O(A+A) ~ O(A) – linear time.Example 2.def Func(n):\ti=1\tj=1\twhile j&lt;n:\t\ti+=1\t\tj+=i\t\tprint(\"#\") In order to calculate the Big-O time in this example, we should ask ourselves under what conditions the while loop exits and what is the number of those steps. Value of i is incremented in each iteration and value of j is sum of previous values of i. The loop terminates when j&lt;n, meaning the loop will terminate when sum of all iterations reaches and overcomes the n, so if we label a total number of iterations as y, we can state: 1+2+…+y&gt;n. We further simplify the statement using the known mathematical rule that the sum of the given numbers’ array is the mean of the product of its last number enlarged by one:y(y+1)/2 &gt; n =&gt; O(sq.root n).Example 3.search 7 in [1, 3, 4, 7, 10, 12, 22, 25, 30] if 7 equals middle of array (10) then return it else if 7 smaller than 10 search in [1, 3,4,7) if 7 is smaller than middle of sub-array((3+4)/2) search in [1,3] else if 7 is bigger than 3.5 search in[4,7] if 7 is smaller than 4 compare else compare to 7 return Here we examine a case of binary search which takes a sorted array (by the way, this is a fine example how sorting reduces problem’s complexity) and by comparing the search case to the middle array in the iterative turns ultimately finds it (or doesn’t). It starts with N elements to search in the first iteration, then N/2. N/4 and so on, until the value either is found (either by finding it in the middle of the array or by reducing it to the size of one element – the one that is searched for) or isn’t. So, the total running time is a number of steps until N goes to the 1 by its division by 2:N=9 // N/2N=4 // N/2N=2 // N/2N=1 // N/2This number is same as if we reverse this and ask how many times we need to multiply 1 by 2 until we reach N:N=1 // N2N=2 // N2N=4 // N2N=9 // N2That number of times is log, as we search the k in statement 2^k=N =&gt; log(2)16=k. We will discard here the basis of the log and the fact that in this particular example we start with an odd number, both of which isn’t important for the Big-O estimation. So, in the binary search, worst execution time is O(log n).Example 4.def Func(n): count=0 for i in range(int(n/2), n): # O(n/2) time a=1 while a+n/2&lt;=n: # O(n/2) time b=1 while b&lt;=n: # O(log n) count+=1 b*=2 # because of this a+=1 print(count)Here we have three nested loops, one outer and two inner ones. First two loops execute in n/2 times, third one, while b&lt;=n loop, because of the line b*=2 which reduces its time from linear to logarithmic (halving the number of iterations), is logn. We multiply the final outcome as those are nested loops and have: n/2 ^2 * log n =&gt; O(n^2 logn).Example 5.int Func(int n) { if (n&lt;=1) { return 1; } return Func(n-1)+Func(n-1); }Calling Func(3) corresponds to:Func(3-1)+ Func(3-1)=2(Func(2)Func(2)=Func(1)+Func(1)=2Therefore, Func(3)=2(Func(2))=4Func(4)=8, Func(5)=16 and so on – we have a clear example of geometric progression, i.e. this is a recursive function which generates multiple instances of itself which are progressing as a powers of 2, therefore the number of those calls defines its complexity, which is equal to 2^N =&gt; O(2^N).Example 6.def Func(n): count=0 if n&lt;=0: return 0 j=1 for i in range(0,n): # n times while j&lt;n: # n-j times const. time j+=1 count+=1 print(count)The function above executes in n*const. time =&gt; O(n).Example 7.void ArrayPrint(int[] arrayA, int[] arrayB) { for (int i= 0; i &lt; arrayA.length; i++) { for (int j = 0; j &lt; arrayB.length; j++) { for (int k = 0; k &lt; 100000; k++) { Console.WriteLine(arrayA[i] + \",\" + arrayB[j]); } } } }Here we have a variation of the example 1., with the addition of one more inner loop which iterates up to a constant number. Regardless of its size, the execution time is constant and we discard it so the total running time remains O(A,B).Example 8.def Func(n): for i in range(0, n//4): # executes n/4 times j=1 while j&lt;=n: # executes n/4 times j+=4 print(\"*\")Total execution time: O(n^2)Example 9.Which of the following statements have the time complexity of O(n)? O(n+p), where p&lt;n/2 O(2n) O(n+logn) O(n+m)Answer: Cases 1, 2 and 3 all have the O(n). Case 1: Since the p&lt;n/2, the contribution of p is guaranteed smaller than n, therefore we can exclude it.Case 2: We can exclude constant 2, as already mentioned.Case 3: logn will always be smaller than n, therefore we exclude it.Now the case 4 can’t be simplified, as we don’t know anything about n or m, so its complexity is as stated.Example 10.count=0def Logs(n): i=1 global count while i&lt;=n: j=n while j&gt;0: j=j//2 # logn count+=1 i*=2 # logn return countHere, the outer loop executes when I doubles, which halves the number of executions, giving us the logx times, as discussed in previous examples. Similarly, the inner loop also has logarithmic time. So, the total complexity is O(log^2n).ConclusionThe concept of Big-O notation may be somewhat too abstract to figure out, but through analyzing simple algorithm implementations one should get a grasp on its basic principles. Having acquainted us with a bunch of such examples, we should be in the position to draw conclusions about the worst time complexity of the given algorithm’s implementation and to compare it with another one. It is a term often found in the computer science books so developers should at least understand the basic concepts behind it." }, { "title": "Algorithms’ complexity analysis - the Big-O Notation", "url": "/posts/algorithm-analysis/", "categories": "algorithms, complexity analysis", "tags": "big-o, algorithms", "date": "2022-07-23 00:00:00 +0200", "snippet": "As there usually are multiple ways to solve the given problem, given the multiple algorithms’ implementation, one would find their comparation useful. This is what is algorithm analysis all about. Suppose we want to sum the first n integers: user inputs the number and all integers from 1 till n are summed up. We could do it via iteration like this:(pseudocode)create function sum(n){ declare integer sum=0 loop from integer I starting from 1 till n+1 in every iteration add to sum value of i return sum}Or using a mathematic shortcut that tells us the sum of first n numbers is one half of product of number n and its larger follower, n+1, like this:(pseudocode)create function sum(n){return (n*(n+1))/2}We don’t need some specialized knowledge in algorithm analyze to figure out that, in terms of simplicity and efficiency, second implementation will have the decisive difference: it will be faster and simpler to calculate the product of two numbers and to half that then to iterate through a list of numbers, adding each one and summing it up in the last iteration. We could easily measure the execution times in any programming language and we would realize the differences. However, it turns out that the execution time is not a good reference for comparing algorithms because it is dependable upon the computer hardware and software configuration. What we need is an algorithm comparison which is not reliant upon the given programming language, style of the coder, machine/operating system internal timer’s configuration, etc. Since we have already defined that the main reference should revolve around algorithm’s executing time, we can deepen the time analysis stating three cases: Worst case: defines the input data with which the algorithm runs with the most time (ie. slowest), Best case: the fastest algorithm’s run given the input data, and Average case: predicts the average time with the random input.If we are going to exclude the external influences like software and hardware and to focus only on the algorithm’s complexity (which directly influences running time) in relative terms, only our bet is to link inputs with outputs and to see how the output behaves when input changes. Since we can represent any algorithm with a mathematical expression (assuming here it is a function), our aim is to analyze how the execution time changes when we change the input data. If we search for an element in an array of 100 elements applying brute force through iterating all elements and comparing them with the search term (linear search), execution time will surely be shorter opposed to searching 100.000 elements in the same manner. If we take into account two search algorithms and compare their execution time on different input data (array sizes), we can then derive conclusions on how they perform in terms of their best and worst cases.In this sense, Big-O notation is the upper bound of the given function showing how function (our algorithm) behaves when the argument (input data, variables) tends to enlarge. It is represented as f(x)=O(g(x)), which in computer science translates to function f(x) that approximates given algorithm has an upper bound of output cases in a form of function g(x). That means we defined another function, g(x) which shows the maximum rate of growth for f(x) at larger values of x.Suppose we represent some algorithm with a function:f(x)=3x^2+30x+300At the lower values of x, certainly up to 10, the constant 300 is the dominant part of the function, but when the x rises in value it tends to dwarf other parts of the equation, so the part 3x^2 becomes one with the most weight. In fact, we can actually strip the 3 and leave only the x^2 as it contributes by far for most growth. So, we can say that function f(x) has an order of magnitude f(x)=x^2, or say O(x^2). So, in the computer science, Big-O notation shows how the given algorithm will perform in the worst possible case and it can therefore be used to approximate the performance of given algorithm, which then can be compared to other one. There is no automatized way to translate algorithm into meaningful Big-O notation. One needs to analyze the parts of the algorithm counting the steps of the computation and to add their time of execution together. There are some useful guidelines in the process: Loops. The execution time of a loop is the execution time of all statements inside a loop multiplied with all iterations. So, execution time = c * x = O(x). Nested loops. Multiplication of execution times of all loops, so in case of one outer and one inner loop we have c * x * x = O(x^2). Consecutive statements. Sum the execution time of each statement in a code block. For instance:int x=10;for (int i=0; i&lt;x, i++): // execution x times{ Console.WriteLine(i); // constant time} for (int i=0; i&lt;x, i++): // execution x times{ for (int j=0; j&lt;x, j++): // execution x times Console.WriteLine (\"i value: \"+ i+ \" and j value: \"+ j);}which translates to x+c+x^2= O(x^2). If statements. Worst execution time translates to the if test itself plus code after if or else statement, whatever is longer to execute. For example:if x==0 // constant time{ Console.WriteLine(\"Incorrect\");else for (int i=1; i&lt;x; i++) // x times{Console.WriteLine(\"Number is \" + i) // constant time}so, total time = c+x+c=O(x). Logarithmic-complexity statements. If it takes a constant time to reduce the problem size by a fraction, then algorithm is O(logn). For instance:void Log(int x){ int i=1; while (i&lt;=x) { i*=2; Console.WriteLine(i); }}Log(100)Output:248163264128The loop exits after n times so we can define 2^k=x =&gt; k=logn (base 2 rather than 10). So, total time here is O(logn).Generally, we can establish the estimates for the commonly used mathematical expressions as follows: Figure 1: Common time complexities Figure 2: Graphical representation of some time complexitiesOne can dive more into mathematics and complexity theory to further study the best and average cases as well as amortized analysis; however, for most approximations and comparations the worst-case algorithm’s behavior depicted by the Big-O notation suffices.Interpretation of the Big-O notation’s outcome is also worth discussing. One has to bear in mind constantly that the calculated O(g(x)) is the upper bond of the notation, meaning the worst-possible case. If one algorithm gives O(x) and another O(x^2), that does not mean the first one is faster; rather that the second one will tend to have worse outcome on the large enough input. So, the comparative analysis via Big-O notation will have some drawbacks if one wants to find best running time for given algorithm, and then the analysis must be broadened by introducing another notation, Omega-notation, which we will leave for another time.ConclusionIn this article we have shown that we can design self-made tests in our particular programming language to measure the executional timing of our code, however this does have some serious drawbacks as the results are not comparable but rather very dependable upon a number of circumstances. Complexity theory gives us the approximative tools to translate an algorithm into set of mathematical statements and to estimate its complexity giving estimates on each part of the code. Such results can be then compared to other algorithms’ estimates within given mathematical boundaries. This analysis using Big-O notation can give a reasonably detailed picture on the worst-possible case of algorithm’s complexity." }, { "title": "Python’s (non-existing) increment/decrement operators", "url": "/posts/Python's-incerement-decrement-operators/", "categories": "python, coding", "tags": "python", "date": "2022-07-22 00:00:00 +0200", "snippet": "Have you tried using very basic ++/– operators in Python? Incrementing/decrementing of numeric variables should be routine, right? Actually, it turns out they do not exist, and that they are not necessary. In C(ish) languages, unary operators are commonly used in flow control as iterators, providing the ability of loop’s progression within specified limits (pre- and postfix difference will be discarded here). So simple, basic for loop in C# looks something like this:(general format)for (initialization; condition; iteration)some code(actual example)int a=5;for (int i=10; i&gt;=a;i--){ Console.WriteLine(i);}Output:1098765Of course, while loop is the same as for loop, just slightly reconstructed:int a=5;int i=10;while (i&gt;=a){ Console.WriteLine(i); i--;}Output:same as previous example.So unary operator ‘–’ decrements the integer variable by one. Attempting to use same in Python gives an error because unary operators don’t exist in Python which may be surprising to some. Sure, instead of i– in C# we could use i=i-1 or its somewhat shorter version i-=1, no problems whatsoever. Same is possible in Python as well. But, let’s go a bit deeper to see what is really going behind the scenes in Python when it comes to variables changing their values, and why the nonexistence of unary operators isn’t a big deal.Statement a+=1 is in Python known as the Augmented Assignment Operator. It (re)assigns, rather than only increments. How is that? If we create an integer, increment it and then check the hex ids, we will realize that new object is created in memory and that the integer’s reference is changed:a=1print(id(a)) # 2636855378160print(hex(id(a))) # 0x265f0d400f0a+=1print(id(a)) # 2636855378192print(hex(id(a))) # 0x265f0d40110Notice how the both id (unique integer) and its hex address changed after incrementation. That is consequence of the fact that integers are immutable in Python – in other words, when we change the integer’s value, new object is created in the background and the reference now points to that object, while old one is or will be at some point garbage-collected. Both in C# and Java, int datatype is also immutable albeit the internal structure of those languages is a bit different. However, in Python the value is being reassigned to a modified, newly created object in memory.The for loop in Python somewhat differs from other languages as it does not require an iterator value because there is an option to use range() function:for (initialization) in range(condition): some codeIf we want to output printed integers from 10-5, as in our C# example, we would simply write:for a in range (10, 4, -1): # loop from 10 to 5 (not including 4) with -1 incrementation print(a)Output:1098765Equivalent while loop in Python is, given the differences in the syntax, essentially same as in C#:i=10while(i&gt;4): print(i) i-=1 # ConclusionDue to the way of Python’s internal structure, unary operators ++/– used as incrementors/decrementors in other languages are not existent. Instead, either we use Augmented Assigned Operators += and -=, while in some cases we can achieve the same using for loop with range() function." }, { "title": "Missing ‘switch’ Statement? Python 3.10 gets you covered", "url": "/posts/missing-switch-statement/", "categories": "python, coding", "tags": "python, branching", "date": "2022-07-19 00:00:00 +0200", "snippet": "Most languages support multiple flow control statements, vast majority of them is familiar with if and switch. So, someone coming from C/C++/Java/C# and alike languages might wonder about equivalent statement in Python. First of all, let’s clarify the subtle difference between those statements in order to see what exactly are we looking for.Consider a simple structure in Python:userInput=1if (userInput==1): print(\"Addition\")elif (userInput==2): print(\"Subtraction\")elif (userInput==3): print(\"Multiplication\")elif(userInput==4): print(\"Division\")Depending of the value of the variable userInput, we want to achieve different outcomes. Sure, multiple if/elif/else statements are doing the job, but if we’d had quite a bit more of conditions, the code would look somewhat cumbersome. Instead, one might feel a need to use a switch statement, especially if coming from C(ish) language(s). Implementing switch in C# would look something like:userInput=1;switch (userInput){ case 1: Console.WriteLine('Discounted Price 1=40'); break; case 2: Console.WriteLine(\"Discounted Price 2=35\"); break; default: Console.WriteLine(\"Standard price: 45\") break;}Now, in Python community, the possibility of switch statement implementation was discussed at least in the last 15 years (see this: https://peps.python.org/pep-3103/), until finally, Python 3.10 released in October 2021. implemented something called Structural Pattern Matching (see what’s new in Python 3.10 here: https://docs.python.org/3/whatsnew/3.10.html) which introduced Match Case statement, so we can write now:match userInput: case 1: print(a+b) case 2: print(a-b) case 3: print(a*b) case 4: print(a/b)Notice how the Match Case does not require a break statement – it is implicitly, internally called when the interpreter reaches the first matched case – other ones are being ignored.If our code has a default case option – that is the option that executes if previous cases weren’t matched – we’d expect to use a default keyword for that, however Python neatly achieving the same using underscore char as a wildcard pattern “_” which here detects any input and proceeds as a default case. case _: print(\"This is the default case\")We can use logical OR in the same line like this: case 5 | 6 | 7 | 8 | 9: print(\"Invalid input. Please enter the correct input value.\")Data types to match could be usual ones like integers and strings, but also floats (!), so this is possible:test=1.36match test: case 1.00: print(\"Stock conversion\") case 1.36: print(\"Conversion kw to hp enabled\")Besides primitives, objects could be matched as well. Consider this code:class Car: def __init__(self, engineType): self.engineType=engineType def selector(self): match self.engineType: case 'petrol': print(\"Petrol engine\") case 'diesel': print(\"Diesel engine\") case 'EV' | 'electro': print(\"Electro engine\")car1=Car('petrol')car1.selector()We created a simple class with instance variable (object property) engineType, then we defined a method selector which uses matching of that instance variable. Lastly, we create an object car1 with an argument and call a method which executes its logic, finds matched case and outputs the respective string. This line:match self.engineTypeactually checks instance variable of the object of type class Car, in our example it checks an instance variable engineType of the object car1.Further possibility is to match multiple patterns, ie. primitives/object (arguments). This code declares method musicPlayer, which is called upon with three arguments, both of which is matched because of the asterisk “*” sign:def musicPlayer(param): match param: case [*genres]: for genre in genres: print(f'We are playing {genre}.')musicPlayer(('rockabilly','country', 'swing'))ConclusionSwitch argument was something that some Python developers sought for. Although some disputed the very need for it, as other viable solutions do the job, Python version 3.10 introduced pattern matching, which enables switch case and much more. In this article, we have shown basic usage patterns including simple primitives’ case switching, as well as more advanced and to some developers’ unusual features like usage of wildcards, pattern recognition and object pattern matching. Therefore, case matching is only one subset of the newly introduced features - prior to 2021. it didn’t exist in Python ecosystem. It is certainly more powerful than switch statements that we commonly use in other main languages." } ]
