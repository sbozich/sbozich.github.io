[ { "title": "SEPA SCT Tool v0.1 ‚Äî Deterministic pain.001.001.09 CLI and Web Demo", "url": "/posts/SEPA-SCT-tool/", "categories": "releases, sepa, iso20022", "tags": "sepa, sct, iso20022, pain.001.001.09, cli, determinism, validation, web-demo", "date": "2026-01-11 22:00:00 +0100", "content": "SEPA SCT Tool v0.1 is an open‚Äësource, deterministic generator and validator for SEPA Credit Transfer (SCT) messages based on ISO 20022 pain.001.001.09 (Customer Credit Transfer Initiation; 2019 message version). Project repository: https://github.com/sbozich/sepa-sct-toolkit This post serves as a long‚Äëform introduction to the project. It complements the GitHub README and is intended for readers who want to understand why the tool exists, how it is designed, and what problems it deliberately does and does not solve. Why This Tool Exists SEPA payment files are infrastructure artifacts, not user‚Äëinterface objects. In many environments, pain.001 files are: generated indirectly by spreadsheets or ERPs, validated only at bank upload time, rejected with opaque error messages, and difficult to reproduce or audit after the fact. This project takes a different approach: correctness before convenience determinism before heuristics explicit validation stages instead of silent normalization offline, inspectable tooling instead of opaque services The result is a CLI‚Äëfirst tool that makes validation failures understandable and reproducible before files reach a bank or PSP. Interfaces and Authority Model The project exposes two interfaces with a strict authority boundary: CLI (Authoritative) The CLI is the sole authoritative implementation. It performs: Parsing and well‚Äëformedness checks XSD validation against a pinned pain.001.001.09 schema set Explicit business‚Äërule validation Only the CLI should be used for production validation or pre‚Äëflight checks. Web Demo (Non‚ÄëAuthoritative) The Web Demo is a static, client‚Äëside inspection and learning surface: no backend no telemetry no data leaves the browser no XSD validation It exists to make the structure of SCT messages easier to explore, but it must never be treated as authoritative. If the Web Demo and CLI ever disagree, the CLI is correct. Scope and Constraints (v0.1) The scope of v0.1 is intentionally conservative. Included Scheme: SEPA Credit Transfer (SCT, classic) Message: pain.001.001.09 Currency: EUR only Structure: exactly one PmtInf Inputs: canonical JSON (strict contract) end‚Äëuser CSV (normalized deterministically) Outputs: pain.001.001.09 XML canonical JSON validation report human‚Äëreadable report Explicitly Excluded SCT Inst (instant payments) Other pain.001 versions Multiple PmtInf blocks Multi‚Äëcurrency support Bank‚Äëspecific acceptance guarantees Backend services or hosted validation These exclusions are design decisions, not missing features. Determinism as a First‚ÄëClass Property By default, the tool is deterministic: identical canonical input ‚áí identical XML output identical IDs (Message ID, Payment Info ID, End‚Äëto‚ÄëEnd IDs) identical reports This makes the tool suitable for: regression testing audits CI pipelines controlled migrations A random ID mode exists but must be explicitly enabled and is always reported. Input Model and Validation Philosophy The tool uses a canonical JSON model as its primary input contract. Key characteristics: strict schemas (additionalProperties: false) no silent field dropping no per‚Äëtransaction currency (SCT is EUR‚Äëonly) early rejection of non‚Äëconforming input This explains why inputs that ‚Äúlook reasonable‚Äù at first glance may still be rejected: the goal is to fail early and explicitly rather than generate files that will later be rejected by a bank. CSV input is normalized deterministically into the same canonical model. Running the Web Demo Locally The Web Demo is published as static files. From the repository root: python3 -m http.server 5173 Then open: http://127.0.0.1:5173/web/demo/index.html Opening files directly via file:// URLs is not supported due to browser security restrictions. Customization and Institutional Integration The public repository targets a scheme‚Äëpure, standards‚Äëbounded SEPA SCT profile. In real‚Äëworld environments, banks and institutions often require: additional validation rules institution‚Äëspecific formatting constraints integration into existing ERP, treasury, or payroll systems alignment with internal compliance workflows The architecture allows such adaptations, but they are intentionally out of scope for the public repository. Organizations interested in customization, integration, or institution-specific variants are encouraged to contact the author to discuss requirements and constraints. Project Status and Licensing v0.1 is an early, intentionally conservative release. The project is published as open source, with the explicit intent to allow: inspection experimentation forking adaptation A specific OSI‚Äëapproved license will be finalized prior to a stable 1.0 release. Disclaimer This tool validates against published standards and a documented ruleset but does not guarantee bank or PSP acceptance. Final acceptance is determined by the receiving PSP and their local validation rules." }, { "title": "Introducing GrossNetto ‚Äì a Free, Privacy‚ÄëFirst Gross‚ÜîNet Salary Calculator", "url": "/posts/salary-calculator/", "categories": "tools", "tags": "salary, payroll, tax, calculator, privacy, web-tools", "date": "2025-12-26 13:00:00 +0100", "content": "Over the last weeks I‚Äôve been building a tool I personally wanted for a quite some time: GrossNetto ‚Äì a free, privacy‚Äëfirst gross‚Üînet salary calculator that runs entirely in your browser. Live: https://grossnetto.com It‚Äôs a static, client‚Äëside calculator: open it, run numbers, close the tab. No signup. No backend. No data sitting on someone else‚Äôs server. Core principles 100% client‚Äëside and privacy‚Äëfirst Static HTML/CSS/JS (GitHub Pages) Fronted by Cloudflare for speed and reliability No cookies, no trackers, no accounts, no database Your inputs stay in your browser tab Market‚Äëaware logic (not a ‚Äúone size fits all‚Äù widget) Payroll rules vary by country and change over time. GrossNetto is built with explicit market logic instead of generic guesses, so it can be extended and maintained without breaking the core. GrossNetto currently implements deterministic, market-specific payroll logic for the following countries: Germany (DE) Austria (AT) United Kingdom (UK) France (FR) Netherlands (NL) Poland (PL) Spain (ES) Slovenia (SI) Italy (IT) Sweden (SE) Its tax logic is updated for the 2026. year for all supported markets. Successive years will be added, retaining already passed ones. Transparent outputs Where possible, the tool favors: explicit inputs explicit assumptions clear totals and breakdowns The goal is not ‚Äúmystical correctness‚Äù but repeatable, auditable calculation paths. How to use it Open https://grossnetto.com. Select your market (and any required parameters like tax class). Enter gross wage. Review the breakdown and adjust inputs until it matches your scenario. That‚Äôs it. No registration flow. No onboarding funnel. Under the hood GrossNetto intentionally stays simple: plain HTML markup CSS tuned for fast UI iteration modular JavaScript for: form/state handling market rules calculation engine i18n (where applicable) If you prefer self‚Äëhosting, you can clone the repository and serve it as a static site. Repository (code): https://github.com/sbozich/salarycalc Where this fits in my toolbox GrossNetto joins my other small, focused utilities: ListComparator ‚Äì compare, merge and deduplicate text lists TextMetricSEO ‚Äì quick text metrics with a simple SEO angle InvoCreator ‚Äì offline, privacy‚Äëfirst invoice generator GrossNetto ‚Äì gross‚Üînet salary calculations without tracking Same philosophy across all of them: Small tools that solve a narrow problem well, without turning the user into the product. Feedback If something is unclear, missing, or numerically suspicious, I want to hear about it: Suggestions / bug reports: modblaises@proton.me If the tool saves you time (or prevents one spreadsheet rabbit hole), that‚Äôs a win." }, { "title": "Full Linux Audio Modernization on Zorin OS (PipeWire + Bluetooth A2DP)", "url": "/posts/Full-Linux-Audio-Modernization-Zorin-OS-(PipeWire+Bluetooth-A2DP)/", "categories": "Linux, Audio, PipeWire, Zorin", "tags": "pipewire, zorin, bluetooth, a2dp, hdmi, wireplumber, linux audio", "date": "2025-12-22 00:00:00 +0100", "content": "Modern Ubuntu-based distributions (including Zorin OS 17) ship both PipeWire and PulseAudio, creating a hybrid audio stack. That stack may work acceptably until you introduce: HDMI displays, Bluetooth devices, GNOME Wayland sessions, and profile switching. The result can be instability and degraded audio. This article documents a real-world modernization on Zorin OS where two major issues were resolved: HDMI audio distortion caused by sample-rate instability Bluetooth refusing A2DP connections, producing org.bluez.Error.* failures Once corrected, the system delivered: 48 kHz stable HDMI PipeWire-only routing WirePlumber as session manager Bluetooth A2DP SBC-XQ high-fidelity Zero distortions, zero reboots 1. Detect whether PulseAudio or PipeWire owns the server You want the PulseAudio server shim running on PipeWire, not the PulseAudio daemon itself. pactl info | grep \"Server Name\" Expected: Server Name: PulseAudio (on PipeWire 0.3.48) If instead: Server Name: pulseaudio then the legacy daemon is interfering. Check processes: ps aux | grep -i pulse 2. Disable and mask native PulseAudio Prevent it from respawning: systemctl --user --now disable pulseaudio.service pulseaudio.socket systemctl --user --now mask pulseaudio.service pulseaudio.socket Start PipeWire‚Äôs PulseAudio layer: systemctl --user start pipewire-pulse.service Re-check: pactl info | grep \"Server Name\" 3. Replace pipewire-media-session with WirePlumber Legacy configurations still ship pipewire-media-session, which is deprecated. Check state: systemctl --user status wireplumber If inactive, enable it: sudo apt install wireplumber systemctl --user enable --now wireplumber Disable the old session manager: systemctl --user status pipewire-media-session.service It should be masked. 4. Fix HDMI distortion by enforcing 48 kHz HDMI devices expect 48 kHz audio. Linux may default to 44.1 kHz, causing glitching, resampling artifacts, and unstable timing. Create: ~/.config/pipewire/pipewire.conf.d/10-force-48k.conf Contents: context.properties = { default.clock.rate = 48000 default.clock.allowed-rates = [ 48000 ] default.clock.min-quantum = 16 default.clock.quantum = 1024 } Restart: systemctl --user restart pipewire systemctl --user restart wireplumber Confirm: pactl list sinks | grep -E 'Sample' Expected: s32le 2ch 48000Hz This eliminated HDMI rattling and static when connecting a TV. 5. Identify Bluetooth failure modes Typical Bluetooth problems include: Device pairs but never connects org.bluez.Error.Failed br-connection-profile-unavailable No A2DP profiles in PulseAudio Device shows only Serial Port Audio sink never appears HSP/HFP fallback breaks sound quality No pairing agent under Wayland Example state: bluetoothctl connect &lt;MAC&gt; Failed to connect: org.bluez.Error.Failed br-connection-profile-unavailable This means the application layer has no A2DP backend. 6. Install PipeWire Bluetooth support Check packages: apt list --installed | grep libspa You must have: libspa-0.2-bluetooth libspa-0.2-modules If missing: sudo apt install libspa-0.2-bluetooth libspa-0.2-modules -y Restart entire audio and Bluetooth stack: systemctl --user restart wireplumber systemctl --user restart pipewire sudo systemctl restart bluetooth 7. Install Blueman for pairing on Wayland Wayland GNOME often does not spawn a proper pairing agent. Blueman fills the gap. sudo apt install blueman Autostart: mkdir -p ~/.config/autostart cp /etc/xdg/autostart/blueman.desktop ~/.config/autostart/ Start once: blueman-applet &amp; 8. Pair with bluetoothctl when needed bluetoothctl agent on default-agent power on pair XX:XX:XX:XX:XX:XX trust XX:XX:XX:XX:XX:XX connect XX:XX:XX:XX:XX:XX Now profiles should appear. 9. Validate A2DP profiles pactl list cards | grep -i bluez -A20 Expected: a2dp-sink a2dp-sink-sbc a2dp-sink-sbc_xq headset-head-unit The highest‚Äëquality SBC is: a2dp-sink-sbc_xq Set it using Blueman‚Äôs Audio Profile UI. 10. Validate the sink and codec pactl list sinks | grep -E 'bluez|Sample' Expected: Sample Specification: s16le 2ch 48000Hz api.bluez5.codec = \"sbc_xq\" This confirms: 48 kHz A2DP mode high-bandwidth SBC-XQ 11. Validate PipeWire timing with pw-top pw-top Look for: QUANT‚âà256 or 1024 RATE=48000 ERR=0 Example: bluez_output.&lt;MAC&gt; QUANT=256 RATE=48000 ERR=0 That indicates perfect stability. 12. Final Result After applying these steps: HDMI distortion vanished PulseAudio was removed from the signal path WirePlumber enforced stable sample clocks Bluetooth paired reliably High‚Äëfidelity A2DP SBC‚ÄëXQ streamed flawlessly Zero reboots required Audio survived HDMI hot‚Äëplugging This is what Linux audio should be in 2025. 13. Looking ahead Zorin OS 18 (Ubuntu 24.04 base) ships newer PipeWire stacks (0.3.6x+) Most of these issues disappear automatically SBC‚ÄëXQ becomes default on many devices Some platforms begin exposing AAC and LDAC Conclusion Linux audio modernization means: PipeWire in control WirePlumber policy active No PulseAudio daemon Forced 48 kHz for HDMI Proper Bluetooth SPA High‚Äëquality A2DP When configured this way, HDMI and Bluetooth become zero‚Äëmaintenance subsystems, fully Wayland‚Äëcompatible and stable." }, { "title": "Introducing InvoCreator ‚Äì a Free, Privacy‚ÄëFirst Invoice Generator", "url": "/posts/invocreator/", "categories": "tools", "tags": "invoice, invoicing, freelancing, privacy, web-tools", "date": "2025-11-27 10:00:00 +0100", "content": "Over the last days I quietly shipped a third tool into my little toolbox family, next to ListComparator and TextMetricSEO: InvoCreator ‚Äì a free, privacy‚Äëfirst invoice generator that runs entirely in your browser. Live: https://www.invocreator.com Most online invoice tools want an account, a subscription, and your data sitting on someone else‚Äôs servers. I wanted something much simpler: open a page fill in the fields print or save as PDF close the tab ‚Äì and nothing is stored anywhere So InvoCreator is exactly that: a static, client‚Äëside invoice editor with a layout tuned for real‚Äëworld invoices in multiple languages and markets. Why I built it I kept running into the same problems with typical invoicing tools: they want you to register and log in for even a single invoice they store client / invoice data on their servers they are opinionated about layout, currencies or markets many of them are overkill for simple, occasional invoices For many freelancers, one‚Äëperson businesses, and small shops, that‚Äôs not necessary. Sometimes you just need: a clean invoice layout correct fields and VAT handling the ability to save as PDF and be done So InvoCreator is built with a few principles: no signup, no tracking, no backend everything runs in your browser fully usable offline once loaded easy to export/import as JSON templates Key features 100% client‚Äëside and privacy‚Äëfirst Static HTML/CSS/JS, hosted on GitHub Pages and fronted by Cloudflare No database, no cookies, no analytics, no trackers All data lives only in your browser tab until you print or export Multi‚Äëlanguage &amp; market aware InvoCreator currently supports: English (International / EU) English (US) German (DE) Spanish (ES) Italian (IT) Slovenian (SI) Swedish (SE) Bosnian / Croatian / Serbian (BHS) Each language/market has its own: field labels helper notes documentation / help text VAT / reverse‚Äëcharge wording Layout tuned for real invoices The invoice layout includes: seller and buyer blocks (with optional logos) itemized rows (quantity, unit, price, tax) subtotals, tax/VAT and final total optional customer number, order reference, notes and footer It‚Äôs designed to look familiar to accountants, clients, and tax offices ‚Äì not like a random generic template. VAT and reverse charge For EU‚Äëstyle invoices there is: standard VAT percentage per market reverse charge toggle where VAT is paid by the recipient localized reverse‚Äëcharge explanations per language Payment QR, templates and themes Optional payment QR block (SEPA QR for EUR or generic QR style otherwise) JSON export/import: Download JSON template ‚Äì save current invoice data as .json Load JSON template ‚Äì restore it later Light/dark theme toggle, with print‚Äëfriendly layout in both cases How to use InvoCreator Using it is intentionally straightforward: Open https://www.invocreator.com in a modern browser. Choose your language / market from the top menu. Fill in seller, buyer, item lines, VAT, notes, and optional reverse‚Äëcharge/QR. Click Print invoice and use the browser dialog to print or save as PDF. Optionally, export your current invoice as a JSON template and reuse it later. That‚Äôs it. There is no ‚Äúaccount‚Äù. When you close the tab, it‚Äôs gone (unless you save the PDF or JSON somewhere). Under the hood Technically, InvoCreator is very simple on purpose: plain HTML markup one CSS file for layout and print styling a small JavaScript core for: form state calculations QR code generation JSON import/export an i18n file with all translations and documentation text There is no build system or framework ‚Äì you can clone the repository and open index.html directly in a browser, or serve it with any static file server. Repository: https://github.com/sbozich/invocreator How InvoCreator fits into the toolbox This tool joins my other small, focused web utilities: ListComparator ‚Äì compare, merge and deduplicate text lists (emails, SKUs, etc.) TextMetricSEO ‚Äì analyze text length and simple SEO‚Äëoriented metrics InvoCreator ‚Äì generate clean invoices offline, with no tracking The idea is the same across all of them: Small, practical tools that solve a narrow problem well, without user accounts, tracking, or heavy UI. Feedback and support If you try InvoCreator and something feels off, confusing or missing, I‚Äôd love to hear it: Suggestions and comments: modblaises@proton.me If you find the tool useful and want to support further development, you can: BuyMeACoffee (also linked in the footer of the app) Thanks for reading, and if InvoCreator saves you a bit of time or removes one SaaS subscription from your life, it‚Äôs already doing its job." }, { "title": "TextMetricSEO ‚Äî a browser-based word & phrase frequency analyzer", "url": "/posts/TextMetricSEO/", "categories": "Tools", "tags": "text, seo, frequency, analyzer, js", "date": "2025-11-10 10:00:00 +0100", "content": "I‚Äôve published a new little web tool: TextMetricSEO ‚Äî a privacy-friendly word and phrase frequency analyzer that works entirely in the browser. The idea is simple: sometimes you just want to paste a big text, see which words dominate, check whether you‚Äôre repeating a phrase too often, or quickly estimate keyword density for SEO ‚Äî without signing up, sending the text to a server, or installing anything. This tool does exactly that. üåê Live site: https://www.textmetricseo.com Why? I needed something similar when I was writing and editing, and I already had a ‚Äúlist comparator‚Äù tool (built before, put online recently). So I built another small, self-contained tool, again with these rules: static site only (no database), no login, runs in browser, multilingual. The result is a page that you can host on GitHub Pages, put behind Cloudflare, and forget about. What it does 1. Word frequency Paste or upload text and the tool will: count total words, count unique words, show the most frequent words in a table, calculate percentages for each word. This is useful when you want to see if your text is too ‚Äúheavy‚Äù on certain filler words, or if the core topic words actually appear often enough. 2. Phrase counting (bigrams / trigrams) This is the part most word counters don‚Äôt have. You can switch ‚ÄúPhrase counting‚Äù to: Bigrams (2 words) or Trigrams (3 words) and the tool will list repeated word combinations in a separate table. That‚Äôs great for: spotting clich√©s in your writing, finding repeated patterns you didn‚Äôt intend, checking whether your SEO keyphrase actually appears in the text. I added it because I wanted something more than ‚Äúword ‚Üí count‚Äù. 3. Multilingual support TextMetricSEO supports: English German Serbian / Croatian / Bosnian When you pick a language, the tool loads that language‚Äôs stopword list and can filter them out (‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúaber‚Äù, ‚Äúund‚Äù, ‚Äúi‚Äù, ‚Äúda‚Äù‚Ä¶). That makes the frequency table much cleaner. Stopwords are just JSON files in /assets/stopwords/, so it‚Äôs easy to extend. 4. Text cleaning options There‚Äôs a small panel of options: Case handling (lowercase all / keep case) Strip punctuation Ignore numbers Minimum word length Filter stopwords Exclude custom words (comma-separated) This matters because sometimes you want raw counts, and sometimes you want a clean list that‚Äôs closer to keywords. 5. Privacy-friendly Everything runs in the browser. No uploads. No backend. So you can paste sensitive text (drafts, reports, client work) and it never leaves your machine. Technical notes built with plain HTML/CSS/JS has manifest.json and a service worker ‚Üí can be used as a mini PWA perfect for GitHub Pages + Cloudflare mobile-friendly layout custom domain: textmetricseo.com Try it üëâ https://www.textmetricseo.com Created by S. Bozich ‚Äî privacy-friendly, open-source, and built with ‚ù§Ô∏è for the web." }, { "title": "Linux Optimization & Security Manual (Zorin & Ubuntu-Based Systems)", "url": "/posts/linux-optimization-security-manual/", "categories": "Linux, ZorinOS", "tags": "Zorin, Ubuntu, Linux, Optimization, Security, Systemd, Flatpak", "date": "2025-11-04 00:00:00 +0100", "content": "In late 2025, with Windows 10 reaching end-of-support and millions of otherwise capable computers being declared ‚Äúobsolete‚Äù by design, many users are looking for sustainable, secure ways to keep their machines productive. Zorin OS ‚Äî a polished, Ubuntu-based Linux distribution ‚Äî provides a bridge between familiarity and freedom. It allows users to escape vendor-locked ecosystems while continuing to use existing hardware with a supported, modern operating system. This manual was written to help anyone ‚Äî whether migrating from Windows or refining an existing Linux setup ‚Äî understand and apply safe, meaningful optimizations. It reflects real-world experience with Zorin 17.3 and 18 (built on different Ubuntu bases), and most of the tweaks also apply to Ubuntu 22.04 / 24.04 LTS, Linux Mint, and other Ubuntu-based systems. The goal is not to chase artificial benchmarks, but to balance performance, stability, and community values ‚Äî so your system stays fast, reliable, secure, and transparent for years to come. ‚Äî Sinisa Bozic ¬∑ sbozich.github.io Contents System Overview Core System Maintenance Performance &amp; Responsiveness Power &amp; Device Management GNOME / Wayland UX and Session Behavior Connectivity &amp; Peripheral Stability Security &amp; Access Automation &amp; Scheduling Firmware &amp; Boot Integrity Optional Cleanup / Reverts Summary &amp; Philosophy Updated for 2025 ‚Äî Zorin 17.3 &amp; Ubuntu 24.04 LTS This practical manual covers safe, reversible system optimizations for Zorin OS and other Ubuntu-based distros. ‚öôÔ∏è 1. System Overview ‚Äî (Scope &amp; Safety Rules) Target Environments Zorin OS 17.3 / 18 Ubuntu 22.04 / 24.04 LTS Linux Mint and derivatives GNOME and Wayland assumed (most runs on Xorg too) Guiding Principles Every tweak must be fully reversible. Never break printing, Bluetooth, or Wi-Fi. Use upstream-supported mechanisms only (systemd, UFW, APT). Keep automation transparent ‚Äî no opaque cron jobs or hidden scripts. üß∞ 2. Core System Maintenance ‚Äî (Keep Base Layer Healthy) 2.1 Preserve APT Auto-Updates ‚Äî (Kernel &amp; Security Patching) systemctl status unattended-upgrades sudo systemctl enable --now unattended-upgrades Why: ensures signed kernel and package updates remain in sync with Secure Boot. Revert: sudo systemctl disable --now unattended-upgrades Risk Benefit Very Low Ensures automatic kernel and security patching 2.2 Daily Flatpak Auto-Update ‚Äî (User-Level, GUI-Independent) mkdir -p ~/bin cat &lt;&lt;'EOF' &gt; ~/bin/zorin-flatpak-auto-update.sh #!/usr/bin/env bash LOGFILE=\"$HOME/.local/share/flatpak-auto.log\" mkdir -p \"$(dirname \"$LOGFILE\")\" { echo; echo \"=== $(date '+%Y-%m-%d %H:%M:%S') ===\"; flatpak update -y; } &gt;&gt; \"$LOGFILE\" 2&gt;&amp;1 EOF chmod +x ~/bin/zorin-flatpak-auto-update.sh Create service and timer: mkdir -p ~/.config/systemd/user cat &lt;&lt;'EOF' &gt; ~/.config/systemd/user/zorin-flatpak-auto-update.service [Unit] Description=Automatic Flatpak updates (Zorin) [Service] Type=oneshot ExecStart=%h/bin/zorin-flatpak-auto-update.sh EOF cat &lt;&lt;'EOF' &gt; ~/.config/systemd/user/zorin-flatpak-auto-update.timer [Unit] Description=Run automatic Flatpak updates daily [Timer] OnCalendar=daily Persistent=true [Install] WantedBy=default.target EOF systemctl --user daemon-reload systemctl --user enable --now zorin-flatpak-auto-update.timer Verify: systemctl --user list-timers | grep flatpak tail -n 10 ~/.local/share/flatpak-auto.log Revert: systemctl --user disable --now zorin-flatpak-auto-update.timer rm ~/.config/systemd/user/zorin-flatpak-auto-update.* rm ~/bin/zorin-flatpak-auto-update.sh Risk Benefit Minimal Keeps Flatpak apps current without GUI ‚ö° 3. Performance &amp; Responsiveness ‚Äî (Reduce I/O and Background Load) 3.1 Lower Swappiness ‚Äî (Reduce Disk Thrashing) cat /proc/sys/vm/swappiness echo 'vm.swappiness=10' | sudo tee /etc/sysctl.d/90-swappiness.conf sudo sysctl -p /etc/sysctl.d/90-swappiness.conf Why: with ‚â•8‚ÄØGB RAM, reduces swapping and improves responsiveness. Revert: sudo rm /etc/sysctl.d/90-swappiness.conf &amp;&amp; sudo sysctl -p Risk Benefit Low Faster multitasking and reduced SSD wear 3.2 Enable Weekly SSD TRIM ‚Äî (Maintain Write Performance) sudo systemctl enable --now fstrim.timer systemctl status fstrim.timer Why: trims unused blocks weekly, preserving SSD speed. Revert: sudo systemctl disable --now fstrim.timer Risk Benefit None Sustains SSD performance automatically 3.3 Limit Journal Size ‚Äî (Prevent Log Bloat) sudo mkdir -p /etc/systemd/journald.conf.d printf '%s\\n' '[Journal]' 'SystemMaxUse=200M' | sudo tee /etc/systemd/journald.conf.d/size-limit.conf sudo systemctl restart systemd-journald Why: prevents logs from consuming disk space. Revert: sudo rm /etc/systemd/journald.conf.d/size-limit.conf &amp;&amp; sudo systemctl restart systemd-journald Risk Benefit Low Prevents storage bloat without losing important logs 3.4 Disable Non-Essential Services ‚Äî (Reduce Boot Latency) sudo systemctl disable --now man-db.timer smartmontools.service sudo systemctl mask NetworkManager-wait-online.service Why: disables slow or redundant background services. Revert: sudo systemctl unmask NetworkManager-wait-online.service sudo systemctl enable --now man-db.timer smartmontools.service Risk Benefit Medium (loss of SMART alerts, slower manual man updates) Shorter boot and lower background I/O üîã 4. Power &amp; Device Management ‚Äî (Efficiency &amp; Longevity) 4.1 TLP Power Optimizer sudo apt install -y tlp tlp-rdw sudo systemctl enable --now tlp sudo tlp-stat -s Why: replaces multiple vendor daemons with one policy manager. Revert: sudo systemctl disable --now tlp Risk Benefit Low Longer battery life, cooler operation 4.2 Lid Switch Policy sudo nano /etc/systemd/logind.conf Set desired behavior: HandleLidSwitch=suspend or ignore for desktops. Restart logind: sudo systemctl restart systemd-logind Risk Benefit Low Predictable lid behavior; avoids accidental suspends üñ•Ô∏è 5. GNOME / Wayland UX ‚Äî (Minimalist Comfort) 5.1 Refine Screen Lock Timing gsettings set org.gnome.desktop.session idle-delay 900 gsettings set org.gnome.desktop.screensaver lock-delay 30 Risk Benefit None Prevents frequent lock interruptions 5.2 Hide Activities Button gsettings set org.gnome.shell.extensions.dash-to-dock show-show-apps-button false Risk Benefit None Cleaner dock layout and visual focus 5.3 Middle-Click Show Desktop sudo apt install -y xdotool xdotool key Super+d Bind via Settings ‚Üí Keyboard ‚Üí Custom Shortcuts. Risk Benefit None Faster workspace visibility and navigation 5.4 ‚Äî Set Up and Manage PIN Login on Zorin OS (GDM) This section explains how to enable a secure PIN-based login and unlock on Zorin OS (GNOME / GDM) using libpam-pwdfile. The PIN works alongside your regular password ‚Äî you can use either at any time. üß± 1. Install Required Packages sudo apt update sudo apt install -y libpam-pwdfile whois üóÇÔ∏è 2. Back Up the Original PAM Configuration Always back up before editing PAM files. sudo cp /etc/pam.d/gdm-password /etc/pam.d/gdm-password.bak sudo chown root:root /etc/pam.d/gdm-password.bak sudo chmod 600 /etc/pam.d/gdm-password.bak üßæ 3. Create the PIN Storage File sudo touch /etc/custompinfile sudo chown root:root /etc/custompinfile sudo chmod 600 /etc/custompinfile üîë 4. Add User + PIN Securely Enter your PIN interactively so it isn‚Äôt exposed in shell history. read -s -p \"Enter PIN for current user: \" PIN; echo HASH=$(mkpasswd -m sha-512 \"$PIN\") sudo bash -c \"echo \\\"$(whoami):$HASH\\\" &gt;&gt; /etc/custompinfile\" sudo chown root:root /etc/custompinfile sudo chmod 600 /etc/custompinfile View it (optional) ‚Äî only the hashed value is shown: sudo cat /etc/custompinfile ‚öôÔ∏è 5. Link the PIN to GNOME Login (PAM Rule) Add this line to the top of /etc/pam.d/gdm-password: sudo sed -i '1i auth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile' /etc/pam.d/gdm-password Confirm: sudo sed -n '1,10p' /etc/pam.d/gdm-password You should see: auth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile #%PAM-1.0 auth requisite pam_nologin.so ... üß™ 6. Test the PIN Lock the screen (Super + L) or log out. Enter your PIN and press Enter. If it fails once, log in with your normal password, then lock again and retry (PAM caches update after first login). ü©∫ 7. Verify Security ls -l /etc/custompinfile Expected: -rw------- 1 root root ... If needed: sudo chown root:root /etc/custompinfile sudo chmod 600 /etc/custompinfile üßπ Disable or Remove PIN Login You can temporarily disable or fully remove the feature. A. Temporarily Disable PIN Login Comment out the line in /etc/pam.d/gdm-password: sudo sed -i 's|^auth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile|#auth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile|' /etc/pam.d/gdm-password To re-enable: sudo sed -i 's|^#auth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile|auth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile|' /etc/pam.d/gdm-password B. Completely Remove PIN Login Restore the original configuration and delete the PIN file: sudo cp /etc/pam.d/gdm-password.bak /etc/pam.d/gdm-password sudo chmod 644 /etc/pam.d/gdm-password sudo rm -f /etc/custompinfile üßØ Recovery if Locked Out Boot into Advanced ‚Üí Recovery Mode ‚Üí root shell. Remount root as writable: mount -o remount,rw / Restore backup: cp /etc/pam.d/gdm-password.bak /etc/pam.d/gdm-password Reboot: reboot üîÑ Update PIN read -s -p \"Enter NEW PIN: \" PIN; echo HASH=$(mkpasswd -m sha-512 \"$PIN\") sudo sed -i \"/^$(whoami):/d\" /etc/custompinfile sudo bash -c \"echo \\\"$(whoami):$HASH\\\" &gt;&gt; /etc/custompinfile\" ‚úÖ Summary Step Purpose Example command / action 1 Install dependencies sudo apt install libpam-pwdfile whois 2 Backup PAM file sudo cp /etc/pam.d/gdm-password /etc/pam.d/gdm-password.bak 3 Create PIN file sudo touch /etc/custompinfile &amp;&amp; sudo chmod 600 /etc/custompinfile 4 Add PIN interactively sudo sh -c 'mkpasswd -m sha-512 &gt;&gt; /etc/custompinfile' (you‚Äôll be asked for the PIN) 5 Insert PAM rule at top of file sudo sed -i '1iauth sufficient pam_pwdfile.so pwdfile=/etc/custompinfile' /etc/pam.d/gdm-password 6 Test Lock the screen ‚Üí enter PIN ‚Äî Disable (temporarily) sudo sed -i 's/^auth sufficient pam_pwdfile.so pwdfile=\\\\/etc\\\\/custompinfile/# &amp;/' /etc/pam.d/gdm-password ‚Äî Remove (restore original state) sudo mv /etc/pam.d/gdm-password.bak /etc/pam.d/gdm-password &amp;&amp; sudo rm /etc/custompinfile Note: This method uses standard PAM modules and doesn‚Äôt modify GDM‚Äôs graphical interface. It‚Äôs secure when /etc/custompinfile is root-owned and chmod 600. Always keep your password login active as fallback. whois provides the mkpasswd command used to hash the PIN. 5.5 Listing All Installed Programs (APT, Flatpak, AppImage &amp; GNOME Extensions) This utility script provides a comprehensive overview of your installed applications and GNOME extensions on Zorin OS or any Ubuntu-based Linux distribution. It detects: APT packages you manually installed (excluding system defaults) Flatpak apps, both user and system scope AppImages in common directories (e.g. ~/Applications, ~/Downloads) AppImageLauncher-integrated desktop entries GNOME extensions, including name, description, and source URL At the end of execution, the script prints results directly to terminal and asks if you want to save them to your home folder. If confirmed, it writes a timestamped .txt file and opens it automatically in your default text editor. üß© How to Use Create the script file: nano ~/bin/list-installed.sh Paste the full code below into the file. Save with Ctrl + O, exit with Ctrl + X, and make it executable: chmod +x ~/bin/list-installed.sh Run anytime with: list-installed.sh üìú The Script #!/bin/bash # =============================================================== # List installed software on Zorin / Ubuntu-like systems # - APT (manual/user-installed) # - Flatpak (user + system) # - AppImages (common locations) # - AppImageLauncher desktop entries # - GNOME extensions (user + system) with metadata # Shows in terminal, then asks to save to $HOME # =============================================================== TMPFILE=$(mktemp) teeout() { tee -a \"$TMPFILE\" } echo \"===== USER-INSTALLED APT PACKAGES =====\" | teeout comm -23 \\ &lt;(apt-mark showmanual | sort) \\ &lt;(gzip -dc /var/log/installer/initial-status.gz 2&gt;/dev/null | awk '/Package: / {print $2}' | sort) | teeout echo | teeout echo \"===== FLATPAK (USER SCOPE) =====\" | teeout flatpak list --user --app --columns=application,version,origin 2&gt;/dev/null | teeout || echo \"No user-scope flatpaks or flatpak not installed.\" | teeout echo | teeout echo \"===== FLATPAK (SYSTEM SCOPE) =====\" | teeout flatpak list --system --app --columns=application,version,origin 2&gt;/dev/null | teeout || echo \"No system-scope flatpaks.\" | teeout echo | teeout echo \"===== APPIMAGE PROGRAMS (detected in common locations) =====\" | teeout SEARCH_DIRS=( \"$HOME/Applications\" \"$HOME/.local/bin\" \"$HOME/Downloads\" \"$HOME/.local/share/applications\" ) found_any=false for dir in \"${SEARCH_DIRS[@]}\"; do if [ -d \"$dir\" ]; then matches=$(find \"$dir\" -maxdepth 2 -type f -iname \"*.AppImage\" 2&gt;/dev/null) if [ -n \"$matches\" ]; then echo \"In: $dir\" | teeout echo \"$matches\" | teeout echo | teeout found_any=true fi fi done if [ \"$found_any\" = false ]; then echo \"No AppImages found in common locations.\" | teeout fi echo | teeout echo \"===== APPIMAGELAUNCHER-INTEGRATED ENTRIES (.desktop) =====\" | teeout launcher_entries=$(ls \"$HOME/.local/share/applications/\"*.desktop 2&gt;/dev/null | grep -i appimage || true) if [ -n \"$launcher_entries\" ]; then echo \"$launcher_entries\" | teeout else echo \"No AppImageLauncher desktop entries found.\" | teeout fi print_extensions_from_dir() { local DIR=\"$1\" local LABEL=\"$2\" echo | teeout echo \"----- $LABEL -----\" | teeout if [ ! -d \"$DIR\" ]; then echo \"Directory not found: $DIR\" | teeout return fi for extdir in \"$DIR\"/*; do [ -d \"$extdir\" ] || continue metadata=\"$extdir/metadata.json\" if [ -f \"$metadata\" ]; then python3 - &lt;&lt;'PY' | teeout import json, pathlib, os md_path = pathlib.Path(os.environ.get('METADATA')) try: data = json.loads(md_path.read_text(encoding=\"utf-8\")) except Exception: print(f\"* {md_path.parent.name} (could not parse metadata.json)\") else: uuid = data.get(\"uuid\", md_path.parent.name) name = data.get(\"name\", \"\") desc = data.get(\"description\", \"\") url = data.get(\"url\", \"\") print(f\"* {uuid}\") if name: print(f\" name: {name}\") if desc: print(f\" desc: {desc}\") if url: print(f\" url: {url}\") PY else echo \"* $(basename \"$extdir\") (no metadata.json)\" | teeout fi done } echo | teeout echo \"===== GNOME EXTENSIONS (USER, with metadata) =====\" | teeout print_extensions_from_dir \"$HOME/.local/share/gnome-shell/extensions\" \"User extensions\" echo | teeout echo \"===== GNOME EXTENSIONS (SYSTEM, with metadata) =====\" | teeout print_extensions_from_dir \"/usr/share/gnome-shell/extensions\" \"System extensions\" echo | teeout echo \"===== ENABLED GNOME EXTENSIONS =====\" | teeout if command -v gnome-extensions &amp;&gt;/dev/null; then gnome-extensions list --enabled 2&gt;/dev/null | teeout || echo \"No enabled extensions detected.\" | teeout else echo \"gnome-extensions command not found.\" | teeout fi echo | teeout echo \"===== SUMMARY =====\" | teeout echo \"Run date: $(date)\" | teeout echo read -rp \"Save this report to your home folder? (y/n): \" confirm if [[ \"$confirm\" =~ ^[Yy]$ ]]; then OUTFILE=\"$HOME/installed_$(date +%Y-%m-%d_%H-%M).txt\" mv \"$TMPFILE\" \"$OUTFILE\" echo \"Saved to: $OUTFILE\" if command -v xdg-open &gt;/dev/null 2&gt;&amp;1; then xdg-open \"$OUTFILE\" &gt;/dev/null 2&gt;&amp;1 &amp; fi else rm -f \"$TMPFILE\" echo \"Report not saved.\" fi üí° Tips Run this after setting up your Linux system ‚Äî it gives a complete inventory useful for backup or reinstallation. You can version-control your installed_*.txt reports in GitHub for reference. Add an alias for quick access: alias listapps='~/bin/list-installed.sh' üßæ Output Example ===== USER-INSTALLED APT PACKAGES ===== gimp curl htop ===== FLATPAK (USER SCOPE) ===== com.github.tchx84.Flatseal 1.10.0 flathub org.videolan.VLC 3.0.21 flathub ... üèÅ Result Running this script gives you: A live overview of all installed software A clean, optional export file in your home directory A reproducible record of your environment for migrations or audits üåê 6. Connectivity &amp; Peripherals ‚Äî (Stable Wireless and Bluetooth) 6.1 Bluetooth Resume sudo systemctl enable --now bluetooth.service Risk Benefit None Ensures Bluetooth auto-starts after suspend 6.2 Wi-Fi Powersave Fix echo -e '[connection]\\nwifi.powersave=2' | sudo tee /etc/NetworkManager/conf.d/wifi-powersave.conf sudo systemctl restart NetworkManager Risk Benefit None Prevents Wi-Fi dropouts and improves stability üîí 7. Security &amp; Access ‚Äî (Harden Without Bloat) 7.1 Firewall sudo ufw enable sudo ufw allow ssh sudo ufw allow 445/tcp sudo ufw status Risk Benefit Low Protects network services with minimal overhead 7.2 ClamAV sudo apt install -y clamav clamtk sudo freshclam Risk Benefit Low (CPU use during scans) Detects common malware and infected USBs 7.3 Rootkit Hunter sudo apt install -y rkhunter sudo rkhunter --update sudo rkhunter --check Risk Benefit Low Detects potential rootkits and hidden binaries 7.4 Password Managers Bitwarden (cloud) or KeePassXC (offline). flatpak install flathub org.keepassxc.KeePassXC Risk Benefit None Secure credential storage and autofill ‚è±Ô∏è 8. Automation &amp; Scheduling systemctl list-timers --all sudo apt autoremove --purge &amp;&amp; sudo apt clean tail -20 ~/.local/share/flatpak-auto.log Risk Benefit Very Low Keeps system clean and maintenance automated üß¨ 9. Firmware &amp; Boot Integrity [ -d /sys/firmware/efi ] &amp;&amp; echo \"UEFI mode\" || echo \"Legacy BIOS mode\" mokutil --sb-state sudo fwupdmgr get-updates Risk Benefit None Confirms Secure Boot and firmware health üßπ 10. Optional Reverts ‚Äî (Return to Stock State) dconf reset -f / sudo systemctl unmask NetworkManager-wait-online.service sudo systemctl enable --now man-db.timer smartmontools.service Risk Benefit Medium (resets all GNOME tweaks) Full restoration of Zorin default behavior üß≠ 11. Summary &amp; Philosophy Tune your system because you understand it ‚Äî not because ‚Äúa list said so.‚Äù The best optimization is one you can confidently undo. Principle Goal Keep APT updates and UFW active Maintain secure base Automate Flatpak updates Ensure app freshness Lower swappiness Boost responsiveness Enable SSD TRIM Preserve SSD longevity Verify Secure Boot Maintain integrity Author: Sinisa Bozic ¬∑ 2025 Linux Optimization &amp; Security Manual ‚Äî Zorin &amp; Ubuntu-based Systems" }, { "title": "The List Comparator is Online: Free Tool for Effortless Data Cleaning", "url": "/posts/the-list-comparator/", "categories": "Web Resources", "tags": "Online Tools, Data Processing, Productivity", "date": "2025-11-03 21:00:00 +0100", "content": "üåê Try it now: listcomparator.com Ever been faced with two (or more) massive lists and the daunting task of finding what‚Äôs changed? Manually comparing spreadsheets or text files is tedious, error-prone, and a huge waste of time. That‚Äôs why I built listcomparator.com ‚Äî a free, browser-based tool that makes list comparison and data cleanup effortless. üí° Privacy First: Your data never leaves your computer. Everything runs locally in your browser. ‚öôÔ∏è Powerful Features at Your Fingertips Our comparator is packed with capabilities that give you total control over your data: Multi-List Comparison Go beyond just two lists ‚Äî compare up to four lists simultaneously to find common entries or create a unified master list. Intuitive Comparison Modes For two lists, choose from five clear options such as ‚ÄúIn Both (A ‚à© B)‚Äù or ‚ÄúOnly in B (B ‚àí A)‚Äù. For three or more lists, use Intersection (In ALL lists) or Union (All distinct items) for quick, broad insights. üßπ Advanced Data Cleaning Before comparing, clean your data with a click: Trim whitespace Remove empty or duplicate lines Perform Find &amp; Replace (supports Regex and ‚ÄúApply to All Lists‚Äù) These options make it easy to standardize or transform entries across multiple lists at once. üìÅ Flexible Data Input Simply paste your lists or upload files (.txt, .csv). If you upload a CSV, the tool intelligently lets you choose the column to compare ‚Äî perfect for contact lists or data exports. üìä Rich Results Display For two-list comparisons, you‚Äôll see: A Venn diagram giving an instant visual summary A color-coded Diff View highlighting added and removed lines üë• Who Is It For? This tool is built for anyone who handles text or data regularly: Marketers: Clean email lists, find new vs. unsubscribed users, or merge contacts from different campaigns. Data Analysts: Reconcile datasets, spot discrepancies, or identify common records across multiple sources. Developers: Compare configuration files, logs, or feature lists between versions. Everyone: Manage product inventories, guest lists, contacts ‚Äî any text-based dataset. üöÄ Try it now: List Comparator Created by S. Bozich ‚Äî privacy-friendly, open-source, and built with ‚ù§Ô∏è for the web." }, { "title": "Understanding the CSS Cascade&#58; How Styles are Applied in Web Development", "url": "/posts/understanding-the-css-cascade/", "categories": "web development", "tags": "CSS, cascade, specificity", "date": "2023-10-20 00:00:00 +0200", "content": "Cascading Style Sheets (CSS) is a fundamental technology used to style the layout and appearance of web pages. One of the core concepts that developers need to understand when working with CSS is the CSS cascade. The cascade refers to the mechanism by which the browser determines which CSS rules to apply when multiple conflicting rules are defined. This process is central to ensuring that your web pages are styled correctly and consistently. In this article, we‚Äôll explore what the CSS cascade is, how it works, and how developers can use it effectively in their web development projects. What is the CSS Cascade? The CSS cascade is essentially the set of rules that determines the final styles applied to an element on a web page when multiple CSS rules target the same element. When more than one rule affects the same element, CSS has to decide which rule should take precedence. This decision is based on the order of the rules, their specificity, and whether the rule is marked as important. There are three primary factors that influence the cascade: Source Order (the order in which styles are defined in the CSS). Specificity (the weight of a selector that targets an element). Importance (rules marked with !important override other styles). 1. Source Order CSS rules are applied in the order in which they appear in the stylesheet, or in the order they are linked in the HTML document. If two rules have the same specificity, the rule that appears last in the code will take precedence. For example, if we have the following two CSS rules: p { color: blue; } p { color: red; } The text color of all &lt;p&gt; elements will be red because the second rule appears after the first one. The later rule overrides the previous one due to its position in the source code. 2. Specificity Specificity determines the ‚Äúweight‚Äù of a CSS selector. It is a measure of how specific a rule is in selecting an element. The more specific a selector is, the higher its specificity value, and thus, the more likely it is to be applied when there are conflicting styles. CSS specificity is calculated based on a few different factors, primarily focusing on how the selector targets the element. The specificity hierarchy is as follows: Inline styles: style=‚Äù‚Ä¶‚Äù ‚Äî highest specificity. IDs: #id ‚Äî second highest. Classes, pseudo-classes: .class, :hover ‚Äî lower specificity. Elements and pseudo-elements: div, h1, ::after ‚Äî lowest specificity. Here‚Äôs an example: /* Element selector (lowest specificity) */ p { color: blue; } /* Class selector */ .text-blue { color: red; } /* ID selector (highest specificity) */ #main-text { color: green; } Given the following HTML: &lt;p class=\"text-blue\" id=\"main-text\"&gt;Hello, world!&lt;/p&gt; The ID selector #main-text will be applied first, as it has the highest specificity, so the text will be green, overriding the class and element styles. 3. Importance (!important) The !important declaration is a special flag that forces a CSS rule to take precedence over other conflicting rules, regardless of specificity or source order. While it can be useful for applying critical styles, it should be used sparingly, as it can make your code harder to maintain and debug. For example: p { color: blue !important; } p { color: red; } In this case, even though the second rule specifies red, the !important declaration ensures that the blue color is applied, since it has the highest priority. However, using !important too frequently can lead to problems, such as difficulty overriding styles later in the code. It is often better to adjust the specificity or source order of your selectors rather than relying on !important. The Cascade in Action Let‚Äôs consider a more complex example to see how the cascade works in real-world scenarios. Suppose you have the following CSS: /* Global rule */ h1 { color: black; } /* Class rule */ h1.header { color: blue; } /* ID rule */ #special-header { color: red; } /* Inline style HTML &lt;h1 id=\"special-header\" class=\"header\" style=\"color: yellow;\"&gt;Hello, world!&lt;/h1&gt;*/ In this case: The global rule applies to all h1 elements, setting their color to black. The class rule is more specific and overrides the global rule, setting the color of .header to blue. The ID rule has a higher specificity than the class, so it overrides the color to red. Finally, the inline style is the most specific and will override all previous styles, setting the color to yellow. So, the text will appear yellow because the inline style has the highest specificity and is applied last. Best Practices for Working with the CSS Cascade Minimize the Use of !important: While !important can solve conflicts, overusing it can lead to maintenance headaches. Try to rely on source order and specificity to control the cascade instead. Use Specific Selectors Thoughtfully: Be mindful of how you select elements. Avoid overly broad selectors that could be unintentionally overridden. Instead, use specific class and ID selectors when possible. Organize Your CSS: Keep your CSS well-structured. Group related styles together, and order your CSS rules logically. For example, you can place more general styles at the top and override them with more specific styles further down. Use the Developer Tools: Modern web browsers have excellent developer tools that allow you to inspect the applied styles on elements. This can be helpful for understanding how the cascade is affecting the rendering of your page. Conclusion The CSS cascade is a powerful concept that governs how styles are applied to HTML elements when multiple conflicting rules are present. By understanding the factors that influence the cascade‚Äîsource order, specificity, and importance‚Äîweb developers can create more efficient, maintainable, and predictable stylesheets. Being mindful of the cascade and using it effectively ensures that your styles are applied in the way you intend and that your code remains clean and easy to manage." }, { "title": "Navigating tomorrow - How AI Will Reshape the Job Landscape in the Next 5 Years", "url": "/posts/How-AI-Will-Reshape-the-Job-Landscape-in-the-Next-5-Years/", "categories": "AI, job market", "tags": "AI, jobs", "date": "2023-10-10 00:00:00 +0200", "content": "The accelerated advancement of artificial intelligence (AI) has become an influential force shaping our world, particularly in the realm of employment. Over the next five years, the employment market is poised for a transformation that will be significantly driven by AI. From revolutionizing industries to redefining job roles, these changes are set to bring both opportunities and challenges. Automation and Job Transformation AI‚Äôs ability to automate routine tasks is already redefining industries. Repetitive jobs in manufacturing, data entry, and customer service are being automated, leading to concerns about job displacement. However, as some roles become obsolete, new opportunities emerge. For instance, AI necessitates the creation of positions focused on AI development, maintenance, and oversight. Human-AI collaboration will become the new norm, with individuals working alongside AI systems to enhance productivity and decision-making. Skill Evolution and Lifelong Learning The demand for technical skills in data analysis, machine learning, and programming will soar. However, it‚Äôs not just technical skills that will be sought after. Soft skills like creativity, emotional intelligence, adaptability, and critical thinking will be invaluable, as they are areas where AI currently lacks prowess. Lifelong learning will become a necessity, as individuals will need to continually upskill and adapt to stay relevant in the evolving job landscape. New Professions and Industries The rise of AI will spawn new professions and industries. AI ethicists, responsible for ensuring the moral and fair use of AI, will be in high demand. Cybersecurity experts will be essential to protect AI systems from potential threats. Healthcare, finance, agriculture, and education are just a few sectors that will experience significant AI-driven changes, creating opportunities for those with expertise in both AI and these fields. Remote Work and Flexibility The pandemic accelerated the adoption of remote work, and AI will further facilitate this trend. AI-powered tools and platforms will enable effective collaboration and productivity, making remote work more seamless. As a result, geographical boundaries will become less of a barrier, allowing for more flexible work arrangements and the potential for a more global job market. Challenges and Ethical Concerns With the rapid integration of AI, concerns about job displacement, bias in algorithms, and the ethical use of AI will continue to persist. Ensuring a balance between technological advancement and the well-being of the workforce will be a significant challenge. Policymakers, organizations, and educational institutions will need to collaborate to address these challenges and create frameworks that safeguard against negative consequences. Conclusion The next five years will witness a pivotal transformation in the job market as AI continues to permeate various sectors. While some job roles will diminish, many new opportunities will arise, necessitating a shift in skills and approaches to work. Embracing these changes with a focus on continuous learning and adaptation will be crucial in navigating this evolving landscape. In this era of AI, the ability to blend technological expertise with uniquely human skills will define success in the job market. It‚Äôs not just about adapting to AI; it‚Äôs about shaping it responsibly and leveraging its potential to create a more innovative, efficient, and inclusive job market." }, { "title": "Review of the book - \"Learning Python\"", "url": "/posts/review-book-learning-python/", "categories": "books, python", "tags": "python, books", "date": "2023-04-04 00:00:00 +0200", "content": "During my initial familiarization with the Python and computing in general, I tried obtaining knowledge from multiple sources. Yet I always preferred books as the main knowledge source so hereby I present a short review of one of the most comprehensive books written on Python which I bought five years ago. ‚ÄúLearning Python‚Äù is a thick, heavy reference book which is mostly oriented towards advanced users. With a length of over a 1500 pages, it is a bit overwhelming to beginners as it dives deeply into the matter, explaining fine grains not only of the Python itself, but often deals with broader concepts in the computing science as well (as opposed to the book I previously reviewed ). It can be considered as a textbook too as it could be read from the start to beginning, but as this is a book on programming with a myriad of concepts that are best understood, memorized and utilized through actual usage (i.e. experimenting with the code), it is better suited as a reference book. It is definitively not a workbook. Skimming through contents reveals the structure of the syllabus: after a light introductory text on the Python itself, including history, installation and IDE, the focus goes to the Python types and supported operations. This chapter deals with the most important data structures and introduces the dynamic typing to the user. I read multiple programming books (besides on Python mostly on Java and C#) and this book provides one of the best explanations of many general programming concepts like conditional statements, functions and function scopes and even some parts of object-oriented programming. This could partially be explained due to the fact that the Python is significantly lighter in syntax and sometimes features than aforementioned programming languages, but on a lower level, reader really needs to understand more complex topics and often the author gives the concepts comparison with other languages along with the description of its function in Python, for instance take a look at those excerpts: ‚ÄúReaders with a background in C may find Python references similar to C pointers (memory addresses). In fact, references are implemented as pointers, and they often serve the same roles, especially with objects that can be changed in place (more on this later). However, because references are always automatically dereferenced when used, you can never actually do anything useful with a reference itself; this is a feature that eliminates a vast category of C bugs. But you can think of Python references as C ‚Äúvoid*‚Äù pointers, which are automatically followed whenever used.‚Äù ‚ÄúIf you‚Äôve ever used C++ or Java, you‚Äôll recognize that Python‚Äôs self is the same as the this pointer, but self is always explicit in both headers and bodies of Python methods to make attribute accesses more obvious: a name has fewer possible meanings.‚Äù ‚ÄúLike the unified try, chained exceptions are similar to utility in other languages (including Java and C#) though it‚Äôs not clear which languages were borrowers. In Python, it‚Äôs a still somewhat obscure extension, so we‚Äôll defer to Python‚Äôs manuals for more details.‚Äù What I like is that the material is presented in logical order, starting from the basics and then going up to more complex matters. The book deals with the OOP only in the second half, which isn‚Äôt always the case in some other readings where the user is immediately bombarded with it. Note that I don‚Äôt have a thing against the OOP but as an advanced concept its comprehension should probably need be somewhat postponed and not rushed nor imposed onto the innocent reader üòä The OOP chapter consists of some 300 pages which comprises of one fifth of the book and the content is presented in a detailed and resourceful manner. Still, I would like that the chapter on exceptions is presented before OOP, but if one sees this book as a reference rather than a textbook, the ordering of the chapters isn‚Äôt important and isn‚Äôt linear so my remark in that sense could be discarded. Chapters on functions, modules and exceptions are great too ‚Äì they provide practical and theoretical explanations on modular software development through a Python‚Äôs prism. Yet they are very detailed with detailed overviews of those topics and advanced programmers will benefit a lot from those materials. The book concludes with advanced topics like byte and Unicode strings, decorators, metaclasses and some other interesting and useful things. Important notice is that the book is now a decade old, written in 2013. That is a long time in computing. Although one could say that the Python now is more popular and widely used than back then, there were multiple changes in Python itself and there are some complex issues stemming from the version change issues which author covers here. Yet, I believe this is still by far the most comprehensive book on Python and as such still very relevant. It is probably best suited to sit on the shelf next to the programmer‚Äôs desk along with some good workbook. This book definitively has stood the test of time. As Python today is one of the world‚Äôs most used programming languages (many sources put it into the top 5), I am convinced that ‚ÄúLearning Python‚Äù will (continue to) find its place among the users and that the author will eventually delight the numerous readers with the updated version of the text." }, { "title": "Review of the book - \"A Smarter Way To Learn Python\"", "url": "/posts/review-of-book-learn-python/", "categories": "books, python", "tags": "python, books", "date": "2023-02-23 00:00:00 +0100", "content": "If you are keen to learn programming, there are multiple learning paths available, as well as various tools that can be used in that regard. Regardless of your previous education and knowledge that you may possess, there is a myriad of dedicated learning websites, courses, mentorships, tuitions and books. In this article, we‚Äôll assume that you are newbie with no programming experience interested in Python and wanting to learn from the book. ‚ÄúA Smarter Way To Learn Python‚Äù is an introductory Python book, comprehensible to the total beginners with no previous experience in the field whatsoever. It can be used by more seasoned programmers too as a quick reference to some simpler topics. The concept author used in the book is a presentation of various topics in the form of lessons followed by numerous online exercises that should solidify the obtained knowledge. Contents In total, there are 77 lessons or chapters as the author calls them, which could be grouped into following segments: introductory topics (14 chapters), including variables, basic expressions and if statements, data structures and miscellaneous related stuff, which can be further subdivided to lists (5 chapters); tuples (1 chapter); dictionaries (16 chapters); for loops (2 chapters); while loops (2 chapters), functions (10 chapters), classes and OOP (9 chapters), working with external data (15 chapters), including CSV and JSON files, debugging and general information topics (3 chapters). In addition, through 5 appendixes, some preparatory topics like Python installation, running and saving programs are explained too. The book comes under 240 pages and it can be quickly consumed, depending on one‚Äôs particular workload, in probably 2 weeks time together with the included exercises. What will you learn from the book are the basic concepts; you will get information about how things as done, such as taking input from user, concantenating variables, grouping code via functions and classes etc., especially in a syntactical regard. Presented information is a bare minimum to get started with the programming in Python. The explanations are sufficient to get a grasp on how things may be done. Yet, as one progresses, the questions will inevitably arise, as the book follows linear learning pattern and it does not discuss much out of the lesson‚Äôs scope. For instance, the reader may ask himself why use CSV when there is a dictionary and vice versa; could the OOP be replaced and/or supplemented with functions; and finally; what could one do with the obtained knowledge presented in the book. While it is understandable that it is impossible to dive into any deeper discussion in such a brief book, there is complete absence of any advices and recommendations when further learning path is concerned. Yet, many other programming books also fail in this domain. Examples of the exercises There are no projects in this book, which is also to expect. Instead, through the exercises after the each lesson, the user is pointed to the book‚Äôs accompanying website. There are numerous quiz-like exercises where user can type simpler commands and the page checks its validity. There are no review exercises so every exercise is mapped to the particular chapter in the book. Some examples of the questions are, taken in a random order: What is the keyword for adding a new element to the end of a list? Eliminate the element with an index of 2 from a list whose name is products. Rewrite this so you can make changes to the elements. sizes = (\"sm\", \"med\", \"lg\", \"xl\") Targeting the correct element in the following list, assign ‚Äúfi‚Äù to the variable giant_syllable. giant_speak = [\"fee\", \"fi\", \"fo\", \"fum\"] Delete a dictionary item whose key is an integer. Make everything up. In the following code, what is the value of total if the function call doesn‚Äôt pass any arguments? def add_numbers(first_number = 2, second_number = 4): total = first_number + second_number This is the first line of a function definition. When optional arguments are passed, they‚Äôre assigned to x, which is a ____. def fill_dictionary(new_customer, first_name, last_name, **x): Code the first two lines of a function. Line 2 begins the definition of a class. Make everything up. Create an instance of this class. The name of the instance is geneva. The value is 144. class Lake(): def __init__(self, depth): self.depth = depth Complete the fourth line. 1 with open(\"cats.csv\") as f: 2 cat_content = csv.reader(f) 3 cats_list = [] 4 for each_line in ___________ In the end, after the book‚Äôs completion the reader will have a fair comprehension of basic Python‚Äôs features. From that point there are two possible learning paths: The user may continue to consume similar material, which puts emphasize on the syntax rather on underlying logic. That can lead to After this book, more advanced material may be consulted, which will provide answers not only to how, but to why as well. This route is longer and harder but it enables understanding of the fundamentals of programming which are not related to just one programming language. Conclusion The ‚ÄúSmarter Way To Learn Python‚Äù is a decent read, which will fairly quickly put a complete beginner into a position of understanding basic concepts of coding in Python, and as such is worth of studying." }, { "title": "HP Notebook 15 review ‚Äì computing on a budget", "url": "/posts/hp-notebook-15-review/", "categories": "hardware, laptop", "tags": "hardware, hp notebook", "date": "2023-02-15 00:00:00 +0100", "content": "Some 18 months ago I got as a present a 15,6-inch HP laptop. Didn‚Äôt need a laptop, but an acquaintance of mine provided me with a new machine for some service I provided to him. Since I do vast majority of my computing on a desktop PCs (one exemption to that being that stubborn little Thinkpad), I was curious how far a brand-new laptop can go in terms of performance, portability, connectivity and overall user experience. HP Notebook 15 gen. 2021 Specifications Specs-wise, I was surprised when realized that the machine sports an AMD-processor. To be precise, it is an AMD Ryzen 5 3500U quad-core with HT (8 threads) with discrete GPU Radeon Vega 8, with 15W TDP and 12nm lithography. That puts it somewhere in the lower-middle place on the market reserved for ultrabooks, mini PCs and budget laptops. There is 8 GB DDR4-2400 SDRAM (one piece, hence one-channel) and main storage 256 GB PCIe NVMe M.2 SSD. Sounds decent, although a bit tight. Given the tendency of soldering everything to the motherboard, I expected little-to-none upgrades possible; out of curiosity I immediately voided warranty (could afford being reckless as I have plenty of alternative PCs lying around) and unscrewed the back cover of machine to see the internals: The RAM memory is NOT soldered and there are two slots, according to specs sheet max. supported amount is 16 GB (but the machine could probably take 32 GB as well given the chipset supports it); furthermore, there is space for 2.5‚Äù device (HDD/SSD) but that comes with some complications, later about that. I was hoping to find at least one internal USB connector (to put an USB mouse/keyboard controller into), but that was unjustified ‚Äì there is no such thing inside. Design, build quality, ports The machine is not an ultrabook, but it is reasonably thin, so some scarcity of ports is expected. I supposed there will be an USB-C, but it is somehow missing (meaning no charging via phone charger and no video output through it), but there are 3 USB ordinary ports (type A), of which two are 3.1, third is 2.0 (all support only data transfer, no charging). Which means, laptop can‚Äôt charge other devices through USB and it can‚Äôt be charged through it. There is standard LAN port which is a fine addition (usually missing on newer laptops), a HDMI a/v out (standard size, not mini), usual headphone/mic combo connector and SD card reader. Kensington lock is present as well, should you need it. Basics are covered, nothing more, nothing less. Battery One Li-ion 3-cell 41 Wh battery powers the laptop, I thought it would be bad, but given the efficiency of components, the battery life is surprisingly good ‚Äì depending on usage, one could get the duration of typical working day without topping-up. The charging takes about 2 hours. There is no space for second battery as on some models. Screen is 15.6‚Äù Full-HD (1920*1080) 16:9 anti-glare IPS panel which looks pretty good; the images are reasonably sharp and viewing-angles too. It sports a decent pixel density of 142 ppi. I haven‚Äôt tested the gamut and color profiling as I am not into that stuff. It can be a complete desktop-replacement solution even during the longer working times and is pleasant to look at. Input devices Keyboard is decent, feedback is satisfying given the 2 cm height of the typing board (whole laptop is 2.25 cm but few millimeters go to the top cover which contains the screen). To my satisfaction, the dedicated numerical keyboard is present so you get almost a full-keyboard-alike experience, albeit there are some drawbacks, namely the usual placement of some non-essential keys is displaced all around (ins/del, home/end, pgup and pgdown keys as well as f-keys) ‚Äì missing IBM/Lenovo classic keyboard. Yet, keyboard is not only satisfying, but actually good ‚Äì could work long-term on it. Now the biggest drawback: the touchpad itself and especially buttons are REALLY bad. The touchpad feels somewhat clumsy and not precise enough but is usable; however the touchpad keys are so hard to press that the user must mentally prepare to apply additional force every time the mouse key need to be pressed which ruins whole user-experience BIG TIME. Fast solution would be to simply hook up the external mouse and to avoid touchpad buttons whenever possible. Storage The system supports the NVMe SSD and 256 GB unit is installed. The disk benchmark immediately shows very slow transfer rates which indicate an issue of some kind. Apparently, although the disk is PCIE 3.0 x4 capable, the data bus is somehow capped to the only PCIE 1.0 x2, limiting the data transfer to 500 MB/s (which is basically only one-quarter to the max theoretical read and one-third of the write speed). The BIOS isn‚Äôt configurable so the user can‚Äôt change anything of importance in it. This is a serious drawback and puts HP on the throne of shame, as the disk transfer rates of such kind are now 15 years old. Going further, 8 GB of RAM is on the lower edge, especially given the fact that the GPU RAM takes 2 GB of it, so user has only the 6 GB at disposal. For the light, office use, browsing and similar tasks it‚Äôll do the job, however, forget anything more demanding. Expansion to the 16 GB will prove a bit difficult, since one should buy the same memory module which is impossible to find on the market (two same modules would avoid eventual memory issues), so the most probable solution would be acquiring a 2*8 GB modules of DDR4 laptop RAM, which amount to 10% of the laptop‚Äôs worth. There is a possibility of replacing SSD as the faster ones are readily available, and, technically, it is possible to add another 2.5‚Äù HDD/SSD, however HP complicated things here and didn‚Äôt provide the connector which must be manually bought and installed. To be precise, three parts are actually needed: Hard drive bracket, part number L20455-001 (here you could also use some strong double-sided foamy tape instead) Hard drive connector board, part number L20454-001 Hard drive/solid-state drive cable, part number L20456-001 NVMe can also be upgraded to a larger one, which is probably best option when it comes to storage expansion. Performance As usual, main CPU is soldered and not replaceable so you‚Äôre stuck with what you initially obtained in that regard. Even if the RAM is maxed-out, the CPU will probably remain a constraint if heavier software load runs on this machine. I compared the performance of rather old desktop i7 22-nm 3770 CPU with this 3500u Ryzen, both being 4-core, 8-threads, and the i7 just crushes it in terms of everything ‚Äì it may be not a fair comparison given the different market segments but between those chips is 7 years-difference in favor of that Ryzen ‚Äì Moore‚Äôs Law is stuck in rut that‚Äôs for sure. Still no replacement for the displacement. Thermals and miscellaneous devices The machine internal heat management is good ‚Äì there is no significant heating and the thing stays reasonably cool for a prolonged period of time. The fan runs reasonably quiet and is tolerable even when on full speed. I have no complaints in terms of peripherals and miscellaneous equipment ‚Äì speakers, webcam and connectivity work as supposed. Nice thing is that the wireless is 802.11 ac meaning 5 GHz networks are supported. Weighing 1.8 kg (charger excluded), it is somewhat heavier than ultrabooks ‚Äì but if you want 0,3-0,4 kg weight reduce with the same specs prepare to pay at least twice as much as this laptop costs! The weight is manageable though, it is significantly lower than 2 kg and it is well balanced over the laptop‚Äôs area. I measured 1,75 kg without and 2,05 kg with the charger, respectively. The build quality is decent, there are occasional, rare squeaks which is expected given the thin plastic‚Äôs body which seems durable. Portability is average: ideally, you‚Äôd want a 14‚Äù laptop or less when carrying around, but the additional screen-estate you get with this display is worth if you want portable desktop replacement. It will fit in most backpacks though. Conclusion Although it is a decent machine in most departments, even if it would have had a full-speed NVMe drive I wouldn‚Äôt have bought it for the single reason that it does not have USB-C (meaning, you can ditch the original charger and just use your phone‚Äôs one to charge it ‚Äì one thing less in the backpack!). Another thing is crappy touchpad which means that an average user will probably have to carry around external mouse. Third thing, if you are constantly on the move, you‚Äôd be better-off with a lower-screened laptop which you can hook-up to larger external screen if needed. If you can live with those things, or if you don‚Äôt require much portability, this machine is a solid choice especially if it sits on the desk and doesn‚Äôt move often. Update 16th of February 2023: A BIOS update has solved the issue with the SSD speed connectivity:" }, { "title": "C# Mini Project - Library Management System program detailed overview", "url": "/posts/library-management-system/", "categories": "c#, sql server", "tags": "c#, databases, windows forms, visual studio", "date": "2022-11-22 00:00:00 +0100", "content": "In this mini project, we will be creating a program in C# which serves as a Library Management System, enabling the basic functions of one such system. In particular, there are separate records of a books, books‚Äô categories, authors, publishers, library members, as well as the records on book issues and returns. It is in fact a Windows Forms GUI program connected to the underlying database in MS SQL Server which uses similar logic as the Car Rental program. Database overview The underlying database consists of 7 tables, as follows: author contains data on authors of the books, book contains books‚Äô records, category holds the books categories, issuebook records the issuance of the books, member contains the library‚Äôs members data, publisher saves the information on publishers, returnbook records the return of the previously issued books. Structure of the program The program holds a number of .cs files, each for the previously created database tables, respectively. As usual, startup Program.cs runs Main.cs which is the main selection menu of the program. Category.cs, Author.cs, Publisher.cs, Book.cs, Member.cs Those classes are similar to each other, in fact, in terms of both GUI elements and underlying code they consist of the same elements. In particular, they have in common DataGridView, ComboBox, TextBox elements and corresponding buttons for adding/updating the data. On the coding side, they all connect to the database, displaying its contents in aforementioned Windows Forms elements. The database can be updated as well. The database connectivity is ensured through the following code: SqlConnection con = new SqlConnection(\"Data source=DESKTOP-N5IF2SJ\\\\SQLEXPRESS; Initial Catalog=slibrary; Integrated Security=true;\"); As in the Car Rental example, the classes itself are consisting of multiple methods which eliminate the need for multiple-code usage. In particular, those methods are load(), getid(string id). Additionally, there are two click events which define the action on user‚Äôs button clicks. Once is one class created with such a structure and tested, creating other classes is a matter of a work in Visual Studio and repeating the same code within the other classes, paying respect to their particular contents, GUI and database elements. Connection string: SqlConnection con = new SqlConnection(\"Data source=DESKTOP-N5IF2SJ\\\\SQLEXPRESS; Initial Catalog=slibrary; Integrated Security=true;\"); load() method: public void load() { sql = \"select * from category\"; cmd = new SqlCommand(sql, con); con.Open(); dr = cmd.ExecuteReader(); dataGridView1.Rows.Clear(); while(dr.Read()) { dataGridView1.Rows.Add(dr[0], dr[1], dr[2]); } con.Close(); } getid(string id) method: public void getid(string id) { sql = \"select * from category where id= '\" + id + \"'\"; cmd = new SqlCommand(sql, con); con.Open(); dr = cmd.ExecuteReader(); while(dr.Read()) { txtname.Text = dr[1].ToString(); txtstatus.Text = dr[2].ToString(); } con.Close(); } button1 click event: private void button1_Click(object sender, EventArgs e) { string catname = txtname.Text; string status = txtstatus.SelectedItem.ToString(); id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); if (Mode==true) { sql = \"insert into category(catname,status) values(@catname, @status)\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@catname\", catname); cmd.Parameters.AddWithValue(\"@status\", status); cmd.ExecuteNonQuery(); MessageBox.Show(\"Category created\"); txtname.Clear(); //txtstatus.Items.Clear(); txtstatus.SelectedIndex = -1; txtname.Focus(); } else { sql = \"update category set catname=@catname, status=@status where id=@id\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@catname\", catname); cmd.Parameters.AddWithValue(\"@status\", status); cmd.Parameters.AddWithValue(\"@id\", id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Category updated\"); txtname.Clear(); //txtstatus.Items.Clear(); txtstatus.SelectedIndex = -1; txtname.Focus(); } } datagridview click event: private void dataGridView1_CellContentClick(object sender, DataGridViewCellEventArgs e) { if(e.ColumnIndex==dataGridView1.Columns[\"edit\"].Index &amp;&amp; e.RowIndex&gt;=0) { Mode = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); getid(id); } else if (e.ColumnIndex == dataGridView1.Columns[\"delete\"].Index &amp;&amp; e.RowIndex &gt;= 0) { Mode = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); sql = \"delete from category where id=@id\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@id\", id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Category deleted\"); txtname.Clear(); //txtstatus.Items.Clear(); txtstatus.SelectedIndex = -1; txtname.Focus(); con.Close(); } } LendBook.cs, ReturnBook.cs The class lendbook.cs is similar to previous, the new elements are issuing and return date elements. Nothing is computed here but passed to the database. The calculation of the days is done in the returnbook.cs class: similar to the Car Rental project, the calculation is done in SQL and passed back to the program: cmd = new SqlCommand(\"select book,issuedate,returndate,DATEDIFF(dd,returndate,GETDATE())as elap from issuebook where memberid='\"+ txtmid.Text+ \"'\",con); Conclusion The presented program takes into consideration records of authors, books, books categories, members and publishers, and records the books‚Äô issuing and return date. Upon return, it calculates the number of passed days, and if it greater than allowed, calculates the fine." }, { "title": "Adding functionality to Windows Sandbox", "url": "/posts/adding-functionality-windows-sandbox/", "categories": "windows, sandbox", "tags": "operating systems, virtual machines", "date": "2022-11-18 00:00:00 +0100", "content": "Adding functionality to Windows Sandbox Windows Sandbox is a fast, functional and effective virtual environment introduced in 2019. It is mostly delivered as-is, albeit Microsoft added some configurability in 2020, with Windows 10 build 18342 and newer, or Windows 11. Configuration files Configuration files of Windows Sandbox have .wsb file extension, they are formatted as XML and enable control of the following sections: Audio input: Shares the host‚Äôs microphone input into the sandbox. Clipboard redirection: Shares the host clipboard with the sandbox so that text and files can be pasted back and forth. Logon command: A command that‚Äôs executed when Windows Sandbox starts. Mapped folders: Share folders from the host with read or write permissions. Exposing host directories may allow malicious software to affect the system or steal data. Memory in MB: The amount of memory, in megabytes, to assign to the sandbox. Networking: Enable or disable network access within the sandbox. Printer redirection: Shares printers from the host into the sandbox. Protected client: Places increased security settings on the RDP session to the sandbox. vGPU (virtualized GPU): Enable or disable the virtualized GPU. If vGPU is disabled, the sandbox will use Windows Advanced Rasterization Platform (WARP). Video input: Shares the host‚Äôs webcam input into the sandbox. Creating Shared Folders Among the customizable options, most useful one is probably the possibility of direct access to the folders on the host within the Windows Sandbox itself. Basically it is a matter of creating a textual file with extension named as .wsb with following content: &lt;Configuration&gt; \t&lt;MappedFolders&gt; \t &lt;MappedFolder&gt; \t\t&lt;HostFolder&gt;absolute path to the host folder&lt;/HostFolder&gt; \t\t&lt;SandboxFolder&gt;absolute path to the sandbox folder&lt;/SandboxFolder&gt; \t\t&lt;ReadOnly&gt;value&lt;/ReadOnly&gt; \t &lt;/MappedFolder&gt; \t&lt;/MappedFolders&gt; &lt;/Configuration&gt; After that, just run the created file and in desktop on virtual machine you‚Äôll find the mounted shared folder. There are some additional options which are presented in detail here. There is possibility to configure some settings in Windows Sandbox by using other means like adding the files manually and editing the registry, as described here. Conclusion Although it is possible to add some functionality to Windows Sandbox by using aforementioned means, its configurability is still very limited especially when compared to some other third party solutions, like Sandboxie. Yet, since Windows Sandbox is included in the Windows itself, it is worth modding it a bit to better suit user‚Äôs needs." }, { "title": "Windows Sandbox overview - Enabling, Features, Usability, Security", "url": "/posts/windows-sandbox-overview/", "categories": "windows, sandbox", "tags": "operating systems, virtual machines", "date": "2022-11-13 00:00:00 +0100", "content": "Over the time, Windows 10 got some useful features that are not always obvious or even enabled. Many users are not even aware of their existence. When it comes to productivity, it turns out that the Windows for some time sports a fully-fledged virtualization engine (called Hyper-V) and an instant-sandbox called simply Windows Sandbox. In this article we will take a closer look at the Windows Sandbox, to see what it is all about. What it is Windows Sandbox is an instance of a fresh Windows installation that starts within Windows itself. It is a clean Windows virtual machine that doesn‚Äôt need to be set up or preinstalled like traditional virtual machine engines (like beforementioned Hyper-V). It just runs from scratch, enabling the user fully-fledged Windows environment which is totally separated from host. Being a sandboxed, it does not remember its state. How to enable it Windows Sandbox is not supported on Home versions of either Windows 10 or 11. Windows 10 supported versions (Pro, Enterprise, Education) need to be build 18305 or newer ‚Äì Windows Sandbox was introduced in May 2019. so, every supported and updated Windows is good to go. Windows Sandbox is not enabled by default; therefore, it doesn‚Äôt show in the Start Menu. Prior to enabling it, the system needs to support a Virtualization option in BIOS: its state can be checked in the Task Manager: If the host is virtualized itself (i.e., run in the virtual machine itself), the Nested Virtualization is required to run the Windows Sandbox. Enabling it is a matter of starting PowerShell and entering following command: Set-VMProcessor -VMName &lt;VMName&gt; -ExposeVirtualizationExtensions $true Once the virtualization is enabled, the Windows Sandbox should be enabled by going to Control Panel &gt; Programs &gt; Turn Windows Features On or Off: After the system restarts, Windows Sandbox appears in the Start Menu and it can be started: Features The Windows Sandbox is a live, clean-slate image derived from the host operating system. That means it sports the same Windows version as host and it is already updated to the same state (in stark contrast to the virtualized Windows system which doesn‚Äôt share any traits to the underlying OS). Some important aspects of Windows Sandbox are: It is a part of the Windows ‚Äì no separate download is necessary (in respect to the Windows build requirement) It is a one-time session, meaning that it does not remember the state after the restart (for instance installed programs and copied files will disappear after the restart or closing down the program), It is isolated from the host ‚Äì the changes done inside will not impact the host operating system (with some remarks later explained), It runs rather quick even on some older hardware. Transferring content There are two possibilities to copy files from the host to the Windows Sandbox. The first option is usual copying through Copy (Ctrl+C) and Paste (Ctrl+V). Note that the files‚Äô drag-and-drop is not supported. Other way is configuring a shared folder between host PC and Windows Sandbox which is covered here. Usability Software testing Windows Sandbox is perfect companion when it comes to software testing. Fire it up, install the program, run it and play with it, change parameters, evaluate it. Once the Sandbox is closed, nothing escapes it ‚Äì no harm done. It is perfect for power users, developers, testers. Windows options exploration Inside Windows Sandbox, user can freely explore the myriad of Windows options (those options that do not require restart though), figure-out what they do, how they behave - all without fear of system failure. Browsing the Web Since Microsoft Edge comes preinstalled, user can freely surf the Internet without having to worry about cookies, ads and related stuff that may impact the system. Inside Windows Sandbox, there is no need to clear cookies ever. Note that the Internet connectivity within Windows Sandbox does not work if VPN on host system enabled. Security Using Windows Sandbox in most cases is isolated from the host, yet there are two potential security issues with it: Windows Sandbox by default utilizes Windows Defender antivirus, which offers decent protection against malware. Due to the temporary nature of Windows Sandbox itself, the proper antivirus suite can‚Äôt be installed so the user is limited to a Windows Defender, which lacks some security features like behavioral analysis needed to detect zero-day (unpatched) threats. In that way, user can wrongly assume that the file is safe and use it on the host system, thus endangering the host and the network. Advice: use online virus scan, just to be sure. When it comes to networking, Windows Sandbox is protected by NAT (network address translation), preventing network devices to initiate access to it. However, in some cases there is possible to detect the subnet and therefore the network devices on the host system ‚Äì enabling the scan for the opened ports on those devices, which poses a security risk. Hint: for true isolated experience, disable the network adapter on Windows Sandbox when testing for malware. Conclusion Windows Sandbox is a neat useful Windows feature which gives the look and feel of the fresh, unbloated system. Being very handy to use, it can give additional value to the user experience and a boost in productivity and security." }, { "title": "C# Mini Project - Car Rental program detailed overview", "url": "/posts/mini-project-car-rental/", "categories": "C#, sql server", "tags": "C#, databases, windows forms, visual studio", "date": "2022-11-08 00:00:00 +0100", "content": "In this article, we will be reviewing a simple Windows program which can be used to manage a car rental company. The program is synced to a MS SQL Server database and it features basic features like registration of cars and customers, tracking of rentals and returns. Front-end uses Windows Forms UI framework and the development is done in Visual Studio. The source code is available at this link. Database overview The database part is done in SQL Server and it consists of four simple tables: dbo.carreg is the table where rental cars are registered, dbo.customer is the table for customers records, dbo.rental tracks rentals with date records, dbo.returncar manages returns of cars, i.e., termination of rentals. Note: dbo prefix is automatically added on the creation of tables in SQL Server Management Console, they can be referenced with and without this prefix. Structure of the program A program consists of the login screen, main window and sub-windows for each option (car registration, customer data, rental and return records). The main Program.cs section calls the Form1.cs and so the program is being run. The program consists of multiple .cs files which are separate classes itself. They are designed using Windows Forms UI elements from the Visual Studio corresponding toolbar. Login screen (Form1.cs) This simple form consists of two textboxes and two buttons. There is a predefined username (testadmin) and password (123) which is checked upon the click on the Login button. If the username and password are correctly entered, the program then creates a new instance of the class Main and displays it. private void button1_Click(object sender, EventArgs e) { string uname = txtuname.Text; string pass = txtpass.Text; if (uname.Equals(\"testadmin\") &amp;&amp; pass.Equals(\"123\")) { Main m = new Main(); this.Hide(); m.Show(); } else { MessageBox.Show(\"Username or password do not match\"); txtuname.Clear(); txtpass.Clear(); txtuname.Focus(); } } Main window (Main.cs) This section simply creates objects (class instances of the corresponding classes) and displays them via the buttonX_click method: private void button1_Click(object sender, EventArgs e) { carreg c = new carreg(); c.Show(); } private void button2_Click(object sender, EventArgs e) { customer c = new customer(); c.Show(); } private void button3_Click(object sender, EventArgs e) { rental r = new rental(); r.Show(); } private void button4_Click(object sender, EventArgs e) { returncar r=new returncar(); r.Show(); } private void button5_Click(object sender, EventArgs e) { this.Close(); } Car registration (carreg.cs) From the designer‚Äôs viewpoint, this form consists of multiple elements like textboxes, buttons, grids, etc. Obviously, user can enter a registration number, make and model, and to mark whether the auto is available or not. Those data is then entered into the database and displayed within the DataGridView element on the right side. Let‚Äôs examine the most important parts of this structures and their link to the underlying code. The connection to the SQL Server is established by following code: using System.Data.SqlClient; SqlConnection con = new SqlConnection(\"Data Source=DESKTOP-N5IF2SJ\\\\SQLEXPRESS; Initial Catalog=carrental; User ID=testadmin; Password=123;\"); SqlCommand cmd; SqlDataReader dr; Notice that this code will be the same in the other parts of the program. The objects cmd and dr are instances of the SqlCommand and SqlDataReader classes; the first one provides the possibility of entering the SQL commands, another one executes SqlDataReader objects. The class consists of multiple methods which are repeated throughout the project. The method load() loads the data from the carreg table into the DataGridView element: public void load() { sql = \"select * from carreg\"; cmd = new SqlCommand(sql,con); con.Open(); dr = cmd.ExecuteReader(); dataGridView1.Rows.Clear(); while(dr.Read()) { dataGridView1.Rows.Add(dr[0], dr[1], dr[2], dr[3]); } con.Close(); } The method getid() displays the data from carreg table into the outlined textbox fields. This method is called later when editing the car data records. public void getid(string id) { sql = \"select * from carreg where regno='\" + id + \"'\"; cmd = new SqlCommand(sql,con); con.Open(); dr = cmd.ExecuteReader(); while(dr.Read()) { txtregno.Text = dr[0].ToString(); txtmake.Text = dr[1].ToString(); txtmodel.Text = dr[2].ToString(); txtavl.Text = dr[3].ToString(); } con.Close(); } Notice how this line enables the calling of the records with different ID numbers by declaring string variable id and escaping the characters while doing the SQL call: sql = \"select * from carreg where regno='\" + id + \"'\"; The data from the database is then parsed to the corresponding text boxes. button1 click event The button ‚ÄúAdd‚Äù has internal name button1. When the user click add, the entered data from the textboxes are entered into the database. This button has also a second function, which is update of selected records within database. Via the variable Mode, the selection is being made whether the new data is inserted into the database, or the existing data is being edited. The variable is also being used by another method which can change the state. private void button1_Click(object sender, EventArgs e) { string regno = txtregno.Text; string make = txtmake.Text; string model = txtmodel.Text; string aval = txtavl.SelectedItem.ToString(); if(Mode==true) { sql = \"insert into carreg(regno, make, model, available) values(@regno, @make, @model, @available)\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@regno\", regno); cmd.Parameters.AddWithValue(\"@make\", make); cmd.Parameters.AddWithValue(\"@model\", model); cmd.Parameters.AddWithValue(\"@available\", aval); cmd.ExecuteNonQuery(); MessageBox.Show(\"Record added\"); txtmake.Clear(); txtmodel.Clear(); txtmake.Focus(); } else { sql = \"update carreg set make=@make,model=@model,available=@available where regno=@regno\"; con.Open(); cmd = new SqlCommand(sql, con); cmd.Parameters.AddWithValue(\"@make\", make); cmd.Parameters.AddWithValue(\"@model\", model); cmd.Parameters.AddWithValue(\"@available\", aval); cmd.Parameters.AddWithValue(\"@regno\", id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Record updated\"); txtregno.Enabled = true; Mode = true; txtmake.Clear(); txtmodel.Clear(); txtavl.Items.Clear(); txtmake.Focus(); } con.Close(); } Method Autono() This method sets the car registration number field to the 00000 (if there is no data read from the database), else it increments it by 1, so that the cars entered have the unique reg. no. public void Autono() { sql = \"select regno from carreg order by regno desc\"; cmd = new SqlCommand(sql,con); ; con.Open(); dr = cmd.ExecuteReader(); if(dr.Read()) { int id = int.Parse(dr[0].ToString())+1; proid = id.ToString(\"00000\"); } else if (Convert.IsDBNull(dr)) { proid = (\"00000\"); } else { proid = (\"00000\"); } txtregno.Text = proid.ToString(); con.Close(); } Method dataGridView1_CellContentClick() This method enables editing or deletes the data displayed in dataGridView1. In fact, it is an event that reacts on an user clicking on the particular parts of the gridview. If the user clicks on Edit button, the variable Mode is set to false, which enables updating of the records via the button1_Click() method previously described. Moreover, it sets the textbutton textregno as disabled (since we are editing the particular row in the table); it then reads the id value which is the first field in the datagrid current row and it passes that value to the aforementioned getid() method which enters the other text fields with the corresponding data from the database. private void dataGridView1_CellContentClick(object sender, DataGridViewCellEventArgs e) { if(e.ColumnIndex==dataGridView1.Columns[\"edit\"].Index &amp;&amp;e.RowIndex&gt;=0) { Mode = false; txtregno.Enabled = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); getid(id); } else if (e.ColumnIndex == dataGridView1.Columns[\"delete\"].Index &amp;&amp; e.RowIndex &gt;= 0) { Mode = false; id = dataGridView1.CurrentRow.Cells[0].Value.ToString(); sql = \"delete from carreg where regno=@id\"; con.Open(); cmd = new SqlCommand(sql,con); cmd.Parameters.AddWithValue(\"@id\",id); cmd.ExecuteNonQuery(); MessageBox.Show(\"Record deleted\"); con.Close(); } } Button2, which is a refresh button, loads two methods and clears the fields. In that way, the records are being displayed into the gridview. The class runs the same methods on startup as well: public carreg() { InitializeComponent(); Autono(); load(); } The difference being here that the refresh button enables reloading new records on change ‚Äì the new records wouldn‚Äôt be displayed without refresh button until the program‚Äôs restart. private void button2_Click(object sender, EventArgs e) { load(); Autono(); txtmake.Clear(); txtmodel.Clear(); txtmake.Focus(); } Customer Registration This class follows the same logic as previous one, just the names are different. Rental Registration Here we have two new methods; other code is similar to previous classes. Two feats are new to explain. Every car needs to have an availability mark; such column exists in table dbo.carreg. Idea is that cars that are taken are marked as unavailable so that the same car can‚Äôt be booked twice in the same time period. Method txtcarid_SelectedIndexChanged() queries database with the following: select * from carreg where regno='\"+ txtcarid.Text+ \"' \" txtcarid is the designation for the ComboBox, so basically this query tries to get the data filtered by the given carid and to display it in the gridview. If the read from the database succeeds (meaning the data with the particular id exists), then this line: aval=dr[\"available\"].ToString(); gets the value of column available in the carreg table. If that value is no, meaning car is not available, then the other fields below the ComboBox are disabled because the data on the taken car can‚Äôt be changed. Else, those fields are editable. If the read from the database does not succeed, this means car with the particular carid isn‚Äôt entered into the database and therefore unavailable. private void txtcarid_SelectedIndexChanged(object sender, EventArgs e) { cmd = new SqlCommand(\"select * from carreg where regno='\"+ txtcarid.Text+ \"' \", con); con.Open(); dr = cmd.ExecuteReader(); if (dr.Read()) { string aval; aval=dr[\"available\"].ToString(); label9.Text = aval; if(aval==\"No\") { txtcustid.Enabled = false; txtcustname.Enabled = false; txtfee.Enabled = false; txtdate.Enabled = false; txtdue.Enabled = false; } else { txtcustid.Enabled = true; txtcustname.Enabled = true; txtfee.Enabled = true; txtdate.Enabled = true; txtdue.Enabled = true; } } else { label9.Text = \"Car not available\"; } con.Close(); } txtcustid_KeyPress method is used when the user enters a value in txtcustid field(Customer ID) and presses key within it. If that key is a Return (KeyChar==13), then via SQL query the program fetches the data from the database. Else, a customer with entered ID does not exist. private void txtcustid_KeyPress(object sender, KeyPressEventArgs e) { if(e.KeyChar==13) { cmd = new SqlCommand(\"select * from customer where custid='\" + txtcustid.Text + \"' \", con); con.Open(); dr = cmd.ExecuteReader(); if (dr.Read()) { txtcustname.Text = dr[\"custname\"].ToString(); } else { MessageBox.Show(\"Customer ID not found\"); } con.Close(); } } Returncar Class returncar takes into account CarID, CustomerID, Date of return, it counts the elapsed days and it calculates the fine if the number of rental days is bigger than defined in DueDate field in Rental Registration class. It has similar options to edit and delete data in the DataGridView. Let‚Äôs see how the fine is calculated. When the user enters the value into the CarID field and press an Enter, this code is being executed: cmd = new SqlCommand(\"select car_id,cust_id,date,due,DATEDIFF(dd,due,GETDATE()) as elap from rental where car_id='\"+txtcarid.Text+\"'\",con); This SQL command will fetch the wanted data and via a SQL function DATEDIFF will calculate the difference between actual date of car return and supposed return date. If that difference is larger than 0 (if it exists), the program will calculate a fee that is times 100 of the number of passed days, else the fee doesn‚Äôt exist. Conclusion In order to develop a simple program like this, one should possess following knowledge: designing a database in SQL Server, connecting a SQL Server to a C#, writing basic SQL queries, using Windows Forms UI framework, manipulate the elements of the UI and write the corresponding code, finally, have a basic overview of Visual Studio in which the program is developed. Possible improvements: improve user login via using trusted connection (without passing an user id and password) - current solution is prone to SQL injection attack, use other way to connect to the database, without having the need to manually close the connection every time the data is fetched (open connections to the SQL server are security issues as well as performance hogs)." }, { "title": "Why (not) study IT ‚Äì my thoughts", "url": "/posts/why-study-it/", "categories": "studies, education", "tags": "university, academia", "date": "2022-11-01 00:00:00 +0100", "content": "Having just earned my bachelor‚Äôs degree in computer science, I wanted to outline and summarize the pros and contras of pursuing IT studies (or any studies for that matter) in the 2022. This is my second bachelor so obviously I was inclined to study in the past, but for the somewhat odd reasons. Moreover, if I was today thinking of enrolling, I would probably not do it. Here is the quick recap of my reasoning. Pros Consistency This was the main reason for me to pursue the studies. I was aware from the beginning that the studies have streamlined path, with steps to conquer towards the end goal. This may seem obvious, but comparing it with a self-study (and especially having tried it as I did), it is a major advantage: you are forced to do your assignments, seminal papers over multiple exams. It is usually a linear process; you need to pass an exam and then move to other ones. When doing that over a longer period, one faces with an internal struggle in order to achieve the most important thing in professional life: consistency. Being consistent means doing small but continuous improvements on any matter, which are added-up over the time, enabling a mastery over given topic. The source of consistency in my case, apart from the curiosity and passion about the underlying tech matters, were the tuition costs: the fact that the money you invested in education could be spent elsewhere. Therefore, opportunity costs are pressing you to justify your current sacrifice (expressed in your decision to allocate your money to finance your studies). Of course, you are sacrificing your current income in order to achieve return on your investment in the future, which is the main reason for studying anyways (apart for some rare non-monetary incentives). Networking with the peers In my view, the second most important advantage of studying is meeting other people in similar position (studying the same thing) and connecting with them. Being constantly on the same place at the same time during number of years enables people to acquaint with each other, to connect at various levels respective to their primary (topics of study) and secondary (hobbies, similar views on other aspects of life) interests. This creates lasting personal connections that could be significant in later professional and sometimes even personal life. Unfortunately, because of the extraordinary events starting in 2020 my networking experience was very limited. Intellectual challenge In this world, any achievement is done by continuous effort through series of challenges. This is true for any type of achievement, being intellectual, physical or artistic. Studying will mentally challenge you, forcing you to obtain new knowledge and to adapt during the process of solving problems. You will fall and rise; do trial and error; until you either quit temporarily or permanently, or succeed. It can be frustrating but addictive too. Cons Dealing with academia I guess no one is fond of messing on a daily level with hierarchical bureaucracy that considers you as a number on the list: waiting for the enrollment while queuing in the students‚Äô lines, admission to various tests, seminal papers, managing installments, deadlines of various things and other trivial and boring issues. It is as if you are employed just that instead of being paid, you are obliged to pay and to obey at all times. Then there is dealing with lecturers‚Äô egos: taken on average, academia doesn‚Äôt encourage free thinking and exchange of ideas supported by arguments. On numerous occasions, student will be wrong about something and will have to accept the argument of the academics (which is the obvious thing to do and one of the reasons of studying); but when vice-versa situation occasionally happens, academics often just don‚Äôt have the habit of reevaluating their views. And of course, studying is not about rightness or wrongness per se, but it should include honesty and integrity, virtues which seems to be in a rapid decline in our times. If you are a maverick, you will have very hard time on academia. Wrong perception of curriculum It may happen that the enrolling student is passionate about one set of topics and that the given uni‚Äôs curriculum (and/or the execution of the curriculum) is just not focusing on those interests and passions enough. The end effect is losing motivation and your passion, because if you are passionate about Python or C# for example, and if your uni‚Äôs lectures on those topics are not broad enough as expected but instead focuses on VBA, you will feel like losing time and achieving wrong goals. To be fair, there is some flexibility on curriculum because on the later years of study students usually can choose their courses, but even then, I‚Äôve found out that self-study and custom-tailored courses offer more insight and depthness on the selected topics than those served by academia. Unjustified costs Depending on one‚Äôs country of origin, enrollment possibilities and the choice of the uni, the studying costs will range greatly. Still, it is safe to say that taken in relative regard, they will be significant. The incurring costs need to be justified and if usual way of financing the studies is taking student loans that span over the multiple years, it will mean a significant reduction of a student‚Äôs living standard afterwards ‚Äì until the breaking point is reached when the gains will overcome the costs. Therefore, the dilemma of justifying the costs of studies is whether the gains can be achieved without studying or not. I believe that, especially in IT field, studying is becoming more and more redundant. What future brings? The basic reasons of pursuing any studies just a decade ago would be to be able to learn from the recognized academics and industry‚Äôs experts: to attend the lectures in person, to actually see and hear the lecturers and interact with them. Since then, rapid changes took place as everything became digitalized and even freely available. Today, one can find loads of quality material online: myriad of courses, lectures and even complete studies are available freely or paid, in the video, text, audio and other forms (with additional material such as manuals, course books and exercises). Then there are specialized websites, apps, forums, social media groups and channels which are practically on the one‚Äôs fingertip. For those inclined to more traditional ways of learning, there are ratings and reviews of studies and courses; advices from the fellow students; and group collaboration tools that can be really useful when it comes to evaluating the decision about picking the right type of education. So, my guess is that the possibility of obtaining access to the freely shared knowledge will be preferred in the near future. While I am mostly thinking about IT, that will probably apply to other, non-regulated fields of human endeavors too, because in the end, knowledge is about passion, not necessarily about academic degrees. Conclusion Although I think that obtaining a diploma in IT still has some merits, it has some serious drawbacks. It all depends on the one‚Äôs personal situation and perception: if you want all-rounded, broad educational path with a conservative mindset of starting your career, then pursuing studies would be appropriate. For those eager to focus and specialize, other, modular learning paths are probably better option: they will lead to quicker employment or possibility of creating something on your own (maybe in entrepreneurial sense)." }, { "title": "List comprehensions in Python", "url": "/posts/list-comprehensions-python/", "categories": "python, list comprehension", "tags": "python, functional programming, iteration", "date": "2022-10-21 00:00:00 +0200", "content": "Iteration protocol in Python consists of multiple statements, of which a for loop is the usual way to iterate over corresponding data structure. For loop is not the only though. In this regard, Python borrowed from the other languages, particularly from the Haskel, providing something called comprehension iteration. Suppose we have a following code: L=[2,4,6,8] for x in range(len(L)): L[x]=L[x]**2 print(L) # Outputs: [4, 16, 36, 64] It certainly works as planned, but there is more concise and even faster way to do the same: L=[x**2 for x in L] # Outputs the same This one liner replaces the for loop completely; it partially resembles the syntax of the for loop, only written backwards, while bringing some novelties in the process. Let‚Äôs explore its details. List comprehension basics If we closely look at the prior statement: L=[x**2 for x in L] it is clear that the statement inside constructs a new list, because it is written in square brackets. It begins with an arbitrary expression (x**2), followed by the ‚Äòfor‚Äô; ending with the iterable object (L). When this statement is run, an interpreter iterates the elements in the right side, applying the code on the left side of for; end result being creation of a new list with all its elements multiplied by 2, for every x in L. List comprehensions are optional, they are not required by any feature of Python, as they can be fully represented by for loops and lists. However, they are less verbose and very common in code, therefore their understanding is an important part of comprehending Python. Moreover, when it comes to code performance, they may have a significant boost compared to executing for loop statements, because comprehension iterations are performed inside Python interpreter at C language speed. List comprehensions apply an arbitrary expression to iterated items, rather than applying set of lined statements or a function. Therefore, similar to lambda, they can be put to places where multiple-lines statements wouldn‚Äôt be allowed. Comprehensions are a general term as they can be applied not only to lists, but to sets and dictionaries as well (albeit they are most widely used with lists). Their formal syntax is: Examples It almost seems as they form a mini-language itself, especially when coupled with an if statement: res=[x for x in [2,3,4,5,6] if x%2==0] print(res) #Outputs : [2, 4, 6] which is the same as: res=[] for x in [2,3,4,5,6]: if x%2==0: res.append(x) print(res) # Outputs : [2, 4, 6] Quasi-nesting is also supported: res=[x+y for x in [2,3,4,5,6] for y in [10,20,30]] print(res) # Outputs: [12, 22, 32, 13, 23, 33, 14, 24, 34, 15, 25, 35, 16, 26, 36] which is the same as: res=[] for x in [2,3,4,5,6]: for y in [10,20,30]: res.append(x+y) The iterable part in comprehension can be any iterable type. For instance, it can iterate over strings as well: print([x for x in 'collection']) # Outputs: ['c', 'o', 'l', 'l', 'e', 'c', 't', 'i', 'o', 'n'] Emulating concatenation: print([x+y for x in 'ab' for y in 'cd']) # Outputs: ['ac', 'ad', 'bc', 'bd'] This code produces identical output (iterating over a list instead of string in the second part): print([x+y for x in 'ab' for y in ['c','d'] ] ) To give more complex example, let‚Äôs start backwards and first define the usual, for loop statement: res=[] for i in range(10): if i%3==0: for j in range(4): if j%2==1: res.append((i,j)) print(res) Output: [(0, 1), (0, 3), (3, 1), (3, 3), (6, 1), (6, 3), (9, 1), (9, 3)] Same code expressed as list comprehension with the same output: print([(i,j) for i in range(10) if i%3==0 for j in range(4) if j%2==1 ]) Performance, readability and usability remarks As we have seen, list comprehensions per se are not required syntactically, but their usage usually yields a performance advantage over the usual for loops. Due to the fact that they execute at the full C language speed (which is also true for the map iterations), they are in most cases significantly faster than for loop Python bytecode. Yet, there is certainly a tradeoff between list comprehensions‚Äô conciseness, performance gain and lowered readability, because they bring less verbose but as well less understandable statements. So, when used, list comprehensions should be kept simple and neat; for more complex tasks, full statements are preferred. There is a reason why the for loop is the usual way of doing most tasks; one should be aware of other ways too which is the purpose of this article. Conclusion List comprehension enables creation of effective and fast one-liners in Python which replace for loops. They are expressions, meaning they can be inserted in places where multiple lined and nested code couldn‚Äôt. Iteration through comprehension is widely used in practice, so if kept simple, it is a neat and elegant coding solution." }, { "title": "Iterating over combinatorics", "url": "/posts/iteration-over-combinatorics/", "categories": "python, algorithms", "tags": "python, itertools, combinations", "date": "2022-10-19 00:00:00 +0200", "content": "Very often, there is a need for implementing combinatorics (permutations, combinations and variations) and iterating over the generated values in a list for example. While it is possible to create user-tailored solution to it, there is module in Python that provides solutions to those problems. The itertools module provides four methods for implementing combinatorics: product() itertools.permutations() itertools.combinations() itertools.combinations_with_replacement() Examples: product('ABCD', repeat=2) Output: AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DD permutations('ABCD', 2) Output: AB AC AD BA BC BD CA CB CD DA DB DC combinations('ABCD', 2) Output: AB AC AD BC BD CD combinations_with_replacement('ABCD', 2) Output: AA AB AC AD BB BC BD CC CD DD The user can choose one of those four methods depending on the type of combinatorics required. Basically, the product() method can be configured to output variations with repetition ‚Äì basically all possible ways to combine a given set of data. The purpose of other three methods is self-explanatory. The first method itertools.permutations() takes a list or another collection and produces a sequence of tuples which generate all permutations‚Äô possibilities. For example: collection=['a','b','c'] from itertools import permutations for x in permutations(collection): print(x) Output: ('a', 'b', 'c') ('a', 'c', 'b') ('b', 'a', 'c') ('b', 'c', 'a') ('c', 'a', 'b') ('c', 'b', 'a') As demonstrated in the first example, it is possible to set a length of an output data: for x in permutations(collection,2): print(x) Output: ('a', 'b') ('a', 'c') ('b', 'a') ('b', 'c') ('c', 'a') ('c', 'b') Going further, itertools.combinations() produce a sequence of combinations from the input items. For example: # (using the code from the previous example) for x in combinations(collection,3): print(x) Output: ('a', 'b', 'c') Those are combinations without repetition (i.e. repetitive characters are not produced as they are considered the same). Combinations with repetition can be generated as follows: from itertools import combinations_with_replacement for x in combinations_with_replacement(collection,3): print(x) Output: ('a', 'a', 'a') ('a', 'a', 'b') ('a', 'a', 'c') ('a', 'b', 'b') ('a', 'b', 'c') ('a', 'c', 'c') ('b', 'b', 'b') ('b', 'b', 'c') ('b', 'c', 'c') ('c', 'c', 'c') The itertools module contains other interesting tools, like infinite iterators and terminating iterators. Certainly, when it comes to combinatorics and aforementioned other topics, it is the quickest way of implementing solutions to those problems." }, { "title": "10 essential free, open-source programs for Windows and Linux", "url": "/posts/ten-essential-free-programs/", "categories": "software, open-source", "tags": "software, freeware", "date": "2022-10-16 00:00:00 +0200", "content": "This list comprises of 10 great freeware, open-source programs which may not be an obvious find for everyone. It includes the useful utilities for every-day general use and productivity and most are available for both Windows and Linux platforms. 7-Zip (file archiver) 7-Zip When it comes to file archiving and compression, 7-Zip is very capable archiver supporting a wide-range of formats (ZIP, RAR and many more) as well as its native 7z format which has open architecture as well. The program supports any Windows version since 2000 (supporting both GUI and command-line versions), while it functions as a command-line-only tool in Linux. Archives can be encrypted with the 256-bit AES. The Windows version can be used as a file manager as well and it can be associated into the shell context menu. The program can calculate the checksums of files in the CRC or SHA formats. The only con to the program from my viewpoint is an absence of the automated update check-ups. Audacity (audio editor) Audacity If you need to record or digitize, edit, mix or post-process audio files, chances are that you‚Äôll be able to do it with Audacity. The program was introduced over 20 years ago with a premise to offer basic audio editing, but since then became widely known digital audio files editor. Audacity features include live audio recording through a microphone or mixer, or digitizing recordings from other media; post-processing through adding of various effects; digital multitrack editing; third-party plug-ins support; support for variety of audio formats and more. The program support both Windows and Linux. Bitwarden (password manager) Bitwarden On average, a user with an active digital life probably has over 100 passwords to remember. If they are manually managed, it could lead to a password overload which means using the same or slightly altered passwords for multiple logins, which poses a significant security risk. Better way to manage passwords is using a password manager, which protects the encrypted password vault with one-login-to-remember, relieving the user of mismanaging the dozens of web page logins, PINs, and what not. Bitwarden is one of the best password managers out there. Being open-source, it is constantly publicly reviewed and frequently updated. It can be used not only as a personal password tool; it supports a plethora of features in business environment like multi-user support, API access, user groups, SCIM support, SSO integration, encrypted sharing, encrypted file attachments, and more. Bitwarden can sync passwords between multiple devices and store them in a protected vault. The vault is by default stored at a cloud but it can be saved locally as well ‚Äì in both cases it is encrypted by strong 256-bit AES. It supports practically every platform available, both desktop and mobile. Other security features include two-factor (2FA) authentication, password generator, password sharing, password auditing and breach monitoring. Worth mentioning is that only the basic version of Bitwarden is free, other ones come with a cost. Notable mention: Keepass is worthy alternative to the Bitwarden if you want only basic, locally stored vault and password management. Its basic configuration can be upgraded with numerous plugins though, although this customization does require a bit of a user effort. BleachBit (disk-space cleaner) BleachBit Temporary files created by operating system and other software can clog-up the system, affecting performance. They can pose a security risk as well. Although modern operating systems have embedded some basic cleaners, third-party apps still do a much better job in this regard. BleachBit is a free program that can clean digital trash from multiple sources. It detects the major browsers‚Äô left-overs like web cache, cookies, session data and more; it can find left-over and safe-to-delete files in the operating system directories as well. Furthermore, it offers a file shredder. It has a couple of draw-backs though: it does not clean all the trash (like operating system update left-overs) and it does not have a scheduler. Bulk Crap Uninstaller (program uninstaller) Bulk Crap Uninstaller Software admins, testers and computer enthusiasts install a number of programs which at some point need to be removed. It can be done manually either through operating system or by particular program‚Äôs uninstaller. Sometimes there is no uninstaller and the program needs to be manually deleted, folder by folder, and even if uninstaller is present, it is mostly likely that the left-overs will be scattered throughout the drive that it will be very hard and time consuming to fully remove all of them. There is variety of program uninstallers on the market, both paid and free. Bulk Crap Uninstaller (BCU) is viable option due to the fact that is pretty powerful and configurable, allowing multiple uninstallations and automatic folder cleanup afterwards, all that with a minimal user input during the uninstall process. Multiple uninstallation or batch uninstall means that the user will simply mark the number of programs which the BCU then quietly uninstalls. Furthermore, it can deinstall programs which don‚Äôt have the uninstaller, searching the directories and doing the dirty work for the user. It has a system cleaner which can remove system updates and preinstalled bloatware and as such, it can be a perfect companion to the aforementioned BleachBit. Other useful features are startup manager, application rating, creation of uninstall presets and much more. GIMP (image editor) GIMP GIMP is a bitmap image editor that has a reputation as a closest free thing to the Adobe Photoshop. It mimics in many ways the tools of the Photoshop and it has the rich feature set; however, the user interface and the workflow require adjusting. GIMP supports multiple image filters and brushes, as well as transformation, selection, layer and masking tools. There are vast number of available plugins and it supports scripting and macros which allows automating tasks. GIMP supports all available platforms including Windows, Linux and more. MEGAsync (cloud drive) MEGAsync MEGAsync is an end-to-end encrypted cloud drive with generous 20 GB of storage for a free account (paid plans available too). It uses a 128-bit AES (paid versions utilize 256-bit AES). It supports all common software platforms, as well as mobile ones, enabling seamless data syncing among multiple devices. One possible limitation is somewhat lacking online collaboration and integration into office suites, due to its security features. Notable mention: Sync.com offers end-to-end encryption as well, and its free plan includes 5GB of cloud space. However, it does not offer a Linux client. Notepad++ (text editor) Notepad++ This one probably needs no introduction. Whether you‚Äôre just looking for a more advanced text editor than Notepad or you need almost a fully-fledged IDE, Notepad++ gets you covered. Standard option comes with a ton of useful options and its capabilities can be expanded by numerous plugins. It supports displaying and editing of nearly any programming language; additionally, it can run most of them directly (with some forking). Its only drawback is that is Windows based only. Notable mention: Visual Studio Code is either al pari to Notepad++ or better (especially when it comes to debugging and testing), and it supports Linux and other platforms too. ShareX (screen-capture) ShareX ShareX is probably the best screen capture utility for Windows. It can seamlessly capture static screen images or animated ones; it offers many utilities ranging from color-picker, QR scanning, OCR processing, to task automation including taking captures, applying some basic edits (like adding a watermark) and uploading to online service, thumbnail creation and more. It is Windows-only application. Shotcut (video editor) Shotcut Shotcut is a capable video editing software available on all major platforms. It uses internal codecs so it isn‚Äôt dependable upon system ones. Supports wide range of video formats as well as still images; audio and video capture from various sources, multi-format timeline editing, video effects, capable audio editing options and filters etc. It supports a lot of output formats, while it lacks in number of transition effects offered. Its user interface is pretty good too." }, { "title": "Text clean-up and string sanitization with Python", "url": "/posts/text-cleanup-sanitization-python/", "categories": "python, clean-up, string sanitization", "tags": "python, text parsing", "date": "2022-10-11 00:00:00 +0200", "content": "Text sanitizing means cleaning-up the entered text according to the previously established parameters or rules. Those rules usually involve text parsing and handling. A whole set of tools could be used to clear-up such an input. For example, basic tools like string replacement method - str.replace() or regex ‚Äì re.sub() can achieve removing or changing a specific character sequences. Additionally, there is Unicode normalization. Consider following strings: s1='Menue\\u00f1o' # Menue√±o s2='Menuen\\u0303o' # Menue√±o They represent the same text, and are represented by multiple characters therefore the strings itself are different: print(s1==s2) Output: False Obviously comparing those strings will be a problem so this needs to be fixed somehow. When considering Unicode, Python sports the unicodedata module with method normalize() which can do the trick: fix_s1=unicodedata.normalize('NFC', s1) fix_s2=unicodedata.normalize('NFC', s2) print (fix_s1==fix_s2) # true NFC argument means fully composition of the characters, so if we apply it to the both strings, they will be comparable. There is another argument which depicts decomposition NFD, which fully decomposes the use of the underlying characters: fix_s1=unicodedata.normalize('NFD', s1) fix_s2=unicodedata.normalize('NFD', s2) print (fix_s1==fix_s2) # true To see the difference between NFC and NFD, let‚Äôs ascii print the first string after applying NFC and NFD parameters, respectively: print(ascii(fix_s1)) # NFC applied outputs 'Menue\\xf1o' print(ascii(fix_s1)) # NFD applied outputs 'Menuen\\u0303o' Furthermore, when trying to clean-up the text, the various strip() methods could be used for processing. They can be used to get rid of whitespace and characters on the either left or right side of the string, but not in the middle: s=' python is open source ' s=s.strip() print(s) Output: python is open source Here the s.strip() method trims leading and trailing whitespace, but it can‚Äôt remove the inside ones. To do that, we could use the replace() method, as follows: s=s.replace(' ','') Which outputs: pythonisopensource Going further, there are other, more advanced options to the text clean-up. Suppose a chaotic string like this: \"·πó≈ï√∂gr·∫∑m·πÉ√¨ng\\fis\\t great\\r\". There is a way to process whole ranges of characters or strip diacritical marks by using str.translate() method. Let‚Äôs first fix the whitespace: s='·πó≈ï√∂gr·∫∑m·πÉ√¨ng\\fis\\t great\\r' process={ ord('\\t'): None, # deleted ord('\\f'): ' ', # replaced with an empty space ord('\\r'): None # deleted } s=s.translate(process) print(s) Output: ·πó≈ï√∂gr·∫∑m·πÉ√¨ng is great Basically, it uses a translation table which is then applied to the given string. One can build a larger table and include all the fixes inside. Adding this code to the previous block: import unicodedata import sys combine=dict.fromkeys(c for c in range(sys.maxunicode) if unicodedata.combining(chr(c))) b=unicodedata.normalize('NFD', s) b=b.translate(combine) print(b) Output: programming is great In this example, a dictionary mapping every Unicode character (through comprehension iteration) is created from the dict.fromkeys(). The original input is then normalized into decomposed form using normalize() with a NFD parameter. Afterwards, translate() method is used to delete all the accented characters. Ending note is that the larger data set, the more possible performance issues the complex code may have. Therefore, performance-wise, it is sometimes better to repeat simpler code than to forge out an one-solution-fits-all. In any case, a testing will reveal what is the most acceptable solution in that regard." }, { "title": "Retro computing - review of the Lenovo Thinkpad X201", "url": "/posts/thinkpad-x201-review/", "categories": "hardware, laptop", "tags": "retro, hardware, thinkpad", "date": "2022-10-09 00:00:00 +0200", "content": "Lenovo Thinkpad X201 A blast from the past, a machine that has been in active service for staggering 12 years and still rocking is certainly worth reviewing of. It is a 2010 fine specimen of the ultra-portable X-series line, which traces its roots back to the year 2000. when IBM introduced the X20. Thinkpads need no introduction, as they are iconic machines that set standard some 30 years ago, as a complete desktop-replacement solution which was able to mimic the look and feel, as well as performance, of a classic desktop PC - which was unheard for the era. Overview This particular model was introduced in 2010 (see initial review here). It originally sported a mobile version of the first i5 generation, coupled with a 4 GB RAM and usual suspects like radio module (Bluetooth 2.1 and a WiFi 4 a/g/n adapter). It has a 3G module as well, which was an optional feature. Surprisingly enough, it has a modem (trully blast from the past!) The initial asking price in Europe was 1500-1800 EUR (depending on the configuration) which was a lot of the money back then, especially considering its form-factor. The 15.6‚Äù top-of-the-line Dells and Toshibas were sold for equal or less amount. I bought it refurbished in 2014 At the time I just discarded my old Thinkpad T60 (which was legend itself back in the time) and needed something newer - dual-core hyperthreading CPU was fine. The X201 came with a preinstalled Windows 7, 160 GB HDD, and a wiggly radio adapter which soon stopped working. As I recall, the BIOS came locked-out, meaning I could enter it and see the configuration, and at most, change boot-order, but everything else was inaccessible. Which meant no WAN card swaps. I tried to flash BIOS that accepted unlisted WAN cards (Lenovo locked-out unapproved adapters), but that never worked properly. Then I installed small mPCIe to USB adapter and hooked an USB WiFi adapter with a detachable antenna, which worked fine for a couple of years but that died as well. From that point on, I used external USB WiFi adapters and wired connection. As Windows 7 was relatively snappy with 4 GB RAM, the significant limiting factor was a spinning hard drive. Somewhere around 2016 I upgraded it with a SSD, which brought new life into this small machine. Although X201 supports only SATA II interface (max. 3 GB/s), the performance gain is vastly improved due to much shorter random seek-time in comparison to mechanical disk. Windows 7 functioned really fine, especially when I maxed-out the RAM to 8 GB, adding one additional 4 GB module. As Microsoft ended long-term support for Windows 7 in January 2020, I could either switch to Linux, or try to install and use Windows 10. At that moment I was still Windows-oriented, and have had not fully explored alternatives to it, so I was keen enough and installed Windows 10 on the machine, which was 10 years old at the time. After 6 months, this showed to be a mistake, as Windows 10 hogged up laptop‚Äôs resources especially with its frequent updates and fixes. During that time, I found alternatives to mostly all the software I use and switched to Linux. After trying a couple distros, I finally settled on a Linux Mint. Less demanding MATE desktop environment runs really fine; in comparison to Windows 10 it flies. It is continuously updated hence the machine can be used online (but the update process is much more smooth, simpler and faster in comparison to the Windows 10); it is snappy and feels happy with the 8 GB RAM and it heats up the case way less than Windows 10. So, even after 12 years, the machine can still be used in a productive manner which is awesome and it justifies the high initial investment. Pros Build quality and design Although being small, this is the classic Thinkpad: a bit uninspiringly-looking to some but very sturdy. It does not bend, does not squeal, if water is spilled on the keyboard, it is drained without side-effects. The case is made of magnesium alloys and the inner side of screen is of plastics that can bend but not much. Thinkpads were never flashy machines, they signify functionality, usability and practicality over attentive showiness. Keyboard and touchpoint The keyboard is the famous 7-row classic Thinkpad one: it is considered the best in the business. Sadly, Lenovo switched it later to chicklet-style starting from X230 onwards (which does have some advantages) and the position of the keys from that model onwards no longer resembles the classic PC keyboard‚Äôs look. Here, you can be in full control just by touching the group of keys and sensing the particular key or group (without looking down) as for instance Insert, Delete, Home, End, PgUp and PgDown are separated (rather than in-line integrated). The big Enter button (not halved-one)is really an enjoyment to use as well. The touchpoint enables spotless movement of the mouse cursor and the user has the two groups of mouse keys to its disposal, which taken together with the touchpad brings outstanding control options. The feedback from the keyboard‚Äôs keys is great: one just needs to try typing on any newer laptop and then compare the experience. Thinklight keyboard backlight lights-up the keyboard, enabling typing in the dark conditions. Battery life This particular model has a heavy 9-cell non-original, replacement battery which was purchased 5 years ago. The battery life ranged from 8-ish hours when it was new, it degraded pretty fast since then though, so it currently gives around 2 hours of life. Additional point with the 9-cell battery which protrudes on the back side is the feat that the user can hold the laptop with one hand safely (even inverted). Smaller and lighter variants of the battery (4 and 6-cell) were also available. Upgradable Just like an ordinary desktop PC, this line of laptops was made upgradable. I think that the CPU itself is not soldered on this particular model; in addition, the memory slots are easily accessible; user can replace a hard-disk as well and add other devices through the PCMCIA/aforementioned mPCIe/memory card slot, as well as through ordinary USB ports. A docking station called Ultrabase adds plethora of ports including Display Port, 4 USBs and CD/DVD unit which essentially transforms this tiny laptop into the fully-fledged PC. Cons Screen The only significant downside to this machine is the very faint and incapable display. The resolution is passable for the time; however the brightness and the viewing angles are really bad and the color representation is not inspiring to say at least. This was not the laptop for the image/video editing. The screen could be upgraded to IPS which solved much of those problems, but it didn‚Äôt provide higher resolution. No HDMI or Display Port Thick and heavy for today‚Äôs standards Multimedia features are weak Speakers, microphone and web-cam were mediocre even back in the 2010. They are usable, but the engineers‚Äô focus was obviously elsewhere. Conclusion This ancient machine has won many computing battles and earned my stripes through reliable, hassle-free, consistent functioning over the 8 years in my possesion. Thanks to Linux, it is still a viable machine for most of my use-cases and I intend to use it for a foreseeable future. Hats off to classic Thinkpads!" }, { "title": "Connecting Python to MySQL and tips to avoid SQL injection attacks", "url": "/posts/connect-python-to-mysql-avoid-injection/", "categories": "python, mysql, database", "tags": "python, databases, mysql, injection attack, security", "date": "2022-10-08 00:00:00 +0200", "content": "To quick-connect Python program with a MySQL, one needs to consider following: MySQL driver needs to be installed, connection object needs to be created, cursor object needs to be created, the query should be executed. Of course, Python can connect to most databases, yet it seems that the most-preffered database is MySQL, for the reason of being powerful, free and open source, just like Python is. Steps to follow MySQL driver connects MySQL database/server and Python program. It is installed either during MySQL install process, or separately by using PIP: python -m pip install mysql-connector-python In any case, to check whether the MySQL is installed or not, following is typed into Python prompt (no error after executing means it is working properly, otherwise re-run the installation): import mysql.connector In order to link the database and the Python program, the mysql.connectir provides the connect() method: Connection_object= mysql.connector.connect(host = &lt;hostname&gt;, user = &lt;username&gt;, passwd = &lt;password&gt;) Hostname: localhost or IP address Username: name of the user that can access the database Password: password that has been associated with the particular user Database: name of the database to connect to Purpose of the connection object is to provide multiple working environments using same connection to the database: import mysql.connector db = mysql.connector.connect(host=\"localhost\", user=\"root\", passwd=\"admin123\", database=\"testdatabase\") The cursor() function creates the cursor object which then further uses methods like execute and rollback. Inside those methods the SQL queries are run: cursor=db.cursor() cursor.execute(\"CREATE TABLE Food (name VARCHAR(50), price int UNSIGNED, foodID PRIMARY KEY AUTO_INCREMENT)\") cursor.execute(\"DESCRIBE Food) Display the data by looping through the cursor object: for x in cursor: print(x) This is pretty straightforward and the syntax is similar for other databases. However when writing SQL code one should be aware of SQL injection attacks which are very common as they use SQL code to manipulate databases in an unwanted way. Usually, attacker has the access to the program‚Äôs code or part of it, but it can‚Äôt access the database directly, so it is trying to gain unwaranted access through the program‚Äôs (in this case Python‚Äôs code). Preventing SQL injection attacks Although this is a broad topic, the aim of this article is to show how to connect to a database from Python and give a few important tips that can prevent SQL attacks. Sanitization is a common termn meaning typing such a code that prevents itself from being exploited. For instance, using execute() method with %s in place of each variable while passing the value by the list or tuple as the second parameter of execute(): cursor=db.cursor() max_price=[10] cursor.execute(\"\"\" SELECT flour FROM Food WHERE price &lt;%s \"\"\", (max_price)) This would be insecure: cursor=db.cursor() cursor.execute(\"SELECT flour FROM Food WHERE price &lt;10 \") Because the variable is encoded separately in a list or tuple, it can‚Äôt be changed by manipulating SQL code, and the characters are excaped properly, using triple quotes (\"\"\") as well as the comma character instead a %. Of course, there are a lot of other ways to prevent SQL injections but maybe the best ones are done when writing the code." }, { "title": "Determining the most frequently occuring item in a sequence", "url": "/posts/determining-most-occuring-item/", "categories": "python, algorithms", "tags": "python, functions, arguments, parameters, counter", "date": "2022-09-28 00:00:00 +0200", "content": "A sequence is a positionally ordered collection of items. It consists of elements which can be refered to using the index number. In the most programming languages, Python included, the elements‚Äô numbers are in the range 0-(n-1), meaning if the sequence has 10 elements (or its length is 10), starting index is 0 and the last is 9. In Python, most common sequence types are: lists (mutable), strings (immutable), typles (immutable). Now, very common problem is, given the sequence of items, to determine the most frequently occuring items in it. song=[\"Been\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Been\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Keep\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Keep\", \"spending\", \"most\", \"our\", \"lives\", \"Living\", \"in\", \"a\", \"gangsta's\", \"paradise\", \"Tell\", \"me\", \"why\", \"are\", \"we\", \"so\", \"blind\", \"to\", \"see\", \"That\", \"the\", \"ones\", \"we\", \"hurt\", \"are\", \"you\", \"and\", \"me\", \"Tell\", \"me\", \"why\", \"are\", \"we\", \"so\", \"blind\", \"to\", \"see\", \"That\", \"the\", \"ones\", \"we\", \"hurt\", \"are\", \"you\", \"and\", \"me\" ] from collections import Counter word_counts=Counter(song) top5=word_counts.most_common(5) print(top5) Output: [('spending', 4), ('most', 4), ('our', 4), ('lives', 4), ('Living', 4)] Python has a solution in a form of embedded collections.Counter class which does exactly what we are looking for. The method most_common() returns the most frequent occurences. It can count any sequence of hashable input items, since a Counter class is a dictionary itself that maps the items to the number of occurences. print(word_counts['spending']) # outputs 4 For additional set of data: additional=[\"I\", \"guess\", \"they\", \"can't\", \"I\", \"guess\", \"they\", \"won't\", \"I\", \"guess\", \"they\", \"front\", \"guess\", \"guess\"] for word in additional: word_counts[word]+=1 top5=word_counts.most_common(5) print(top5) Output: [('guess', 5), ('spending', 4), ('most', 4), ('our', 4), ('lives', 4)] There is an update() method which does the same thing: word_counts.update(additional) We can use addition and subtraction operations: a=Counter(song) b=Counter(additional) print(a) Output: Counter({'spending': 4, 'most': 4, 'our': 4, 'lives': 4, 'Living': 4, 'in': 4, 'a': 4, \"gangsta's\": 4, 'paradise': 4, 'me': 4, 'are': 4, 'we': 4, 'Been': 2, 'Keep': 2, 'Tell': 2, 'why': 2, 'so': 2, 'blind': 2, 'to': 2, 'see': 2, 'That': 2, 'the': 2, 'ones': 2, 'hurt': 2, 'you': 2, 'and': 2}) print(b) Output: Counter({'guess': 5, 'I': 3, 'they': 3, \"can't\": 1, \"won't\": 1, 'front': 1}) c=a+b print(c) Output: Counter({'guess': 5, 'spending': 4, 'most': 4, 'our': 4, 'lives': 4, 'Living': 4, 'in': 4, 'a': 4, \"gangsta's\": 4, 'paradise': 4, 'me': 4, 'are': 4, 'we': 4, 'I': 3, 'they': 3, 'Been': 2, 'Keep': 2, 'Tell': 2, 'why': 2, 'so': 2, 'blind': 2, 'to': 2, 'see': 2, 'That': 2, 'the': 2, 'ones': 2, 'hurt': 2, 'you': 2, 'and': 2, \"can't\": 1, \"won't\": 1, 'front': 1}) d=a-b print(d) Output: Counter({'spending': 4, 'most': 4, 'our': 4, 'lives': 4, 'Living': 4, 'in': 4, 'a': 4, \"gangsta's\": 4, 'paradise': 4, 'me': 4, 'are': 4, 'we': 4, 'Been': 2, 'Keep': 2, 'Tell': 2, 'why': 2, 'so': 2, 'blind': 2, 'to': 2, 'see': 2, 'That': 2, 'the': 2, 'ones': 2, 'hurt': 2, 'you': 2, 'and': 2}) Conclusion Counter class is very useful tool when it comes to counting the data in sequences. It is an implemented solution which should be preferred to user-written, manual solutions involving dictionaries." }, { "title": "Functions' arguments in Python", "url": "/posts/function-arguments-python/", "categories": "python, functional programming", "tags": "python, functions, arguments, parameters", "date": "2022-09-27 00:00:00 +0200", "content": "Arguments, as pieces of information passed to a function provide a way to insert values into function‚Äôs mechanism. In Python, function arguments directly depend on object reference model, as there is a stark difference in behavior of immutable and mutable arguments. At this point, a linguistical remark is needed to differentiate between argument and parameter: although those two terms are usually used interchangeably, strictly speaking, when declaring function we use parameters, while upon a function‚Äôs call we pass arguments. Python uses number of matching rules when differentiating passed arguments. Those rules are, as follows: positional arguments are matched from left to right. This is the most widely used case, for example: def func(a,b,c): print((a+c)/b) func(3,9,12) # 1.6666666666666667 func(3,12,9) # 1.0 keyword arguments are matched by given name, instead of position. For instance: def func(b=20,a=5,c=22): print(a,b,c) func() # 5 20 22 func(3,12,9) # 12 3 9 arguments with defaults specify default values if the function‚Äôs call doesn‚Äôt contain all specified arguments (parameters) def func(a, b=10, c=20): print(a,b,c) func(12) # 12 10 20 func(12, 15, 22) # 12 15 22 star expressions (varargs): arguments preceeded with * or ** characters to collect an arbitrary number of arguments def func(*args): print(args) func(10) # 10 func(1,2,3,4) # 1,2,3,4 ** feature works for keywords and collect them into a new dictionary: def func(**args): print(args) func(x=5, y=10) # {'x': 5, 'y': 10} varargs unpacking. Passing arbitrarily number of positional or keyword arguments def func(a,b,c,d): print(a,b,c,d) args =(0,1) args+=(3,5) func(*args) # 0 1 3 5 Using the same function but providing a dictionary instead and **: def func(a,b,c,d): print(a,b,c,d) args={'a':0, 'b':5, 'c':10, 'd':15} func(**args) # 0 1 3 5 keyword only: arguments that must be passed by name def func(a, *b, c): print(a, b, c) func(1,2,3) \"\"\" TypeError: func() missing 1 required keyword-only argument: 'c'\"\"\" func(1,2,c=3) #1 (2,) 3 Now let‚Äôs explain difference between immutable and mutable arguments. Consider the following code: def func(a): a=10 b=20 func(b) print(b) # 20 Here the variable a is assigned the object with the value od 20 on the function‚Äôs call. Although inside a function‚Äôs definition the object a changes to 10, changes don‚Äôt have effect on function‚Äôs call as they only reset the local variable to another object. In contrast with C/C++, Python lacks aliasing; meaning that assignment to a parameter name defined in function (a=10) does not change variable b upon the function‚Äôs call. Since arguments are, in fact, pointers to the objects, they can‚Äôt share same objects permanently ‚Äì when it comes to immutables (see this article). However, when mutable objects (lists, dictionaries, class objects) are passed as arguments, in-place changes will probably occur, impacting the callers as well. For instance: def func(x,y): x=5 y[0]=\"test\" X=1 C=[2,4] func(X,C) print(X,C) # 1 ['test', 4] In the upper code, the func function assigns values to a immutable and mutable variables. Since x is a local variable in the function‚Äôs code scope, x=5 has no effect upon function‚Äôs call as previously explained ‚Äì all it does is changes the local variable a to another‚Äôs object reference. Y is a local variable but since the list is the mutable object (see explanation), statement y[0]=‚Äùtest‚Äù is the in-place change which impacts the value of the corresponding list‚Äôs element. This is the reason why this change outlives the function‚Äôs call and impacts the list C. If those in-place changes are not wanted, we could copy the list so the original one isn‚Äôt affected: def func(x,y): x=5 y=y[:] y[0]=\"test\" Summarizing Python‚Äôs argument matching rules, here is the complete list of possible cases: def func(name): matches arguments by position or name def func(name=value): defines default argument value (if not passed in function‚Äôs call) def func(*name): matches and collects remaining, possibly multiple, positional arguments in a tuple def func(**name): matches and collects remaining, possibly multiple, keyword arguments in a dictionary def func(*other, name): defines arguments that must be passed in calls by keywords func(value): matching by position in function‚Äôs call func(name=value): matching by name in function‚Äôs call func(*iterable): pass all objects in iterable as individual positional arguments func(**dict): pass all key/value pairs in dict as individual keyword arguments" }, { "title": "Functions and scopes in Python, Part 2", "url": "/posts/functions-scopes-python-pt2/", "categories": "python, functional programming", "tags": "python, functions, scopes, def, return, nonlocal, global", "date": "2022-09-01 00:00:00 +0200", "content": "The usual way to get an output from function is a return statement. As we have seen in the part 1 of this article, once the interpreter reaches the return statement, it returns the value and ends the processing of the function‚Äôs body. At that point, all scope internal to the function is being destroyed and nothing after that, with an exception of return value, is being retained or remembered. Yet, there is another way to code functions that would return values in a consecutive way: those are generator functions and are available in many programming languages, Python included. Such generator function‚Äôs distinction is an output type: instead of single value, they return a generator object which can be iterated. Let‚Äôs examine more closely how they function. Yield statement While usual functions return value and are done with any computation or memorization afterwards, generator functions can remember their state (scope and variables‚Äô values); thus, they can be stopped and later resumed. Therefore, when being suspended, they yield the value, temporarily stop the execution which can be resumed because the state is saved. Yield can be seen as a consecutive return because it returns the value in a similar manner, but it can be resumed. This allows the function to output a series of values throughout the function‚Äôs run-time, in a contrast to returning all the work immediately and exiting like return functions do. There are particular usage-cases for generator functions which we‚Äôll later more closely examine. We mentioned that generator functions output generators which can be iterated. It turns out that they share common traits with the iteration protocol. In Python, iterator objects implement a next method which iterates one step forward until the end of iteration range is reached, raising an StopIteration exception to mark the end of iteration. All iteration methods use this iteration protocol which is also used by generator functions, making them in a sense a variation of iterators. The generator object which is the output of generator function supports the iteration protocol and aforementioned next method, which is being run until either the next yield value or StopIteration exception. To illustrate this, consider following example: def gen_func(a): print(\"First iteration\") yield a a+=1 print(\"Second iteration\") yield a a+=1 print(\"Last iteration\") yield a b=gen_func(3) next(b) next(b) next(b) Output: First iteration Second iteration Last iteration As we can see, by using yield returns, the local scope is remembered, so when the next method is used on a generator function object, it actually iterates one step further, as if it would in any iterative process (in order to output yield value to the console print method should be used). When next reaches end of iterative range, it finishes and it can‚Äôt be called again using the same function‚Äôs call. If we try to print out the object: print(b) The output we‚Äôd get is: &lt;generator object gen_func at 0x000002595D926420&gt; To get and display the yielded values from the object we need to iterate them and use print: for i in gen_func(3): print(i) Output: First iteration 3 Second iteration 4 Last iteration 5 We can put them in a list as well: print(list(gen_func(3))) Output: First iteration Second iteration Last iteration [3, 4, 5] We could get the same results as in the previous example by using a standard function with returned list of generated values. However, when it comes to processing larger data sets, they may have an advantage in terms of memory use and performance since they don‚Äôt execute in the moment and completely but rather in consecutive steps. By doing so they actually distribute the resources (computational power which equals time as well as the memory space) over a number of iterations. Consider this example: import timeit def sq(n): lista=[] for i in range(n): lista.append(i**2) return lista def gen_sq(n): for i in range(n): yield i**2 normft=timeit.timeit('sq(1000000)', globals=globals(), number=5) genf=timeit.timeit('gen_sq(1000000)', globals=globals(), number=5) print('Exec time of a normal function: ', normft) print('Exec time of a generator function: ', genf) print(\"Exec time of a generator function is faster on average \", normft//genf, \" times\") Output: Exec time of a normal function: 1.7032738999696448 Exec time of a generator function: 4.499917849898338e-06 Exec time of a generator function is faster on average 378512.0 times Functions sq(n) and gen_sq(n) do the same thing, which is computing a range of numbers‚Äô squares. The first function makes all the computation and saves the results in the list while another creates generator which does the same computing but without using memory, hence the enormous execution time difference. Therefore, generators really shine when processing large data input. Another possible usage is infinite streams. As generator functions don‚Äôt create any data structure it is possible to generate infinite iteration stream within a function. In the following example we demonstrate the way to generate infinite stream outside and inside the function, using generators. When executed, both codes will loop infinitively, but calling the function gives more flexibility, for instance using it with if branching, or inside another function or class, etc. while True: a=0 a+=1 print(a) def inc_nums(): n = 0 while True: yield n n += 1 Another example of using generator functions would be reading a very large textual file (.txt, .csv, .json and similar formats where data is separated via a newline) to extract a number of rows. We could use a normal function to do so: def text_reader(file_name): file = open(file_name) result = file.read().split(\"\\n\") return result row_count = 0 text_gen = text_reader(\"some_file.txt\") for row in text_gen: row_count += 1 print(\"Row count is \", row_count) This will function fine as long as the input data‚Äôs size is modest at most because the program processes whole input file at once. If we convert it to generator function, it will be able to process input files regardless of the file size, as the generator would process the data in a sequence manner sending back only specific value: def text_reader(file_name): file = open(file_name) for row in file: yield row row_count = 0 text_gen = text_reader(\"some_file.txt\") for row in text_gen: row_count += 1 print(\"Row count is\", row_count) Lambda Many popular programming languages, Python included, provide another way to create functions. That is lambda expression. It is used to create small, inline functions which are unnamed (although a name could be assigned but it‚Äôs not mandatory) and as such, executed without a call. Lambda is a single-liner, opposed to a block of statements when using def. Lambda is not a statement but an expression and as such is possible to appear inside places where def couldn‚Äôt (as an argument in function call, for instance). This code returns the same values: a=lambda x,y: x*y print(a(5, 10)) def mt(x,y): return x*y print(mt(5,10)) Output: 50 General form of lambda is: lambda [arg1, arg2,‚Ä¶ argN]: expression Names assigned inside lambda retain their scope, as seen from the following example: def names(): first_name='John' full_name=lambda x: first_name+' ' + x return full_name first_client=names() name_surname=first_client('Smith') print(first_client) print(name_surname) Output: &lt;function names.&lt;locals&gt;.&lt;lambda&gt; at 0x0000023F61CCA4D0&gt; John Smith As a shortened version of def, lambda expression is mostly used where statement can‚Äôt be inserted but expression can, as in jump tables: # this code uses lambda as a list literal, where def can't go lista=[lambda x: x**2, lambda x: x**3, lambda x: x**4] for a in lista: print(a(3)) # code below outputs the same yet uses def # def square(x): return x**2 # def cube(x): return x**3 # def quad(x): return x**4 # lista2=[square,cube,quad] # for a in lista2: # print(a(3)) Output: 9 27 81 In Python, lambda is often used in connection to higher-order functions; more precisely lambda is used as an argument along with functions like filter() and map(). Using filter(): lista=[-10, -9, -8, -7, -6, 6,7,8,9,10] lista2=list(filter(lambda x: (x&lt;0), lista)) print(lista2) Output: [-10, -9, -8, -7, -6] Using map(): lista=[-10, -9, -8, -7, -6, 6,7,8,9,10] lista2=list(map(lambda x: (x**2), lista)) print(lista2) Output: [100, 81, 64, 49, 36, 36, 49, 64, 81, 100] Conclusion In this article series we‚Äôve touched upon the basic premises of functional programming in general, as well as some of the basic functional elements in Python. Generator and anonymous functions are more advanced concepts with which a programmer should be at least familiar, because they can be used either to shorten, or in some cases to optimize the code and as such, it is useful to get acknowledged about their existence, structure and common usage cases." }, { "title": "Sum of intervals Codewars challenge", "url": "/posts/Sum-intervals-codewars-challenge/", "categories": "python, overlapping intervals", "tags": "python", "date": "2022-08-29 00:00:00 +0200", "content": "Sum of intervals is the Codewars challenge that is about counting overlapping intervals. Let‚Äôs examine the problem‚Äôs description and see if we can devise a valid solution to it. Figure 1: Screenshot of the Codewars challenge Description Write a function called sumIntervals/sum_intervals() that accepts an array of intervals, and returns the sum of all the interval lengths. Overlapping intervals should only be counted once. Intervals Intervals are represented by a pair of integers in the form of an array (in Python it will be a list of tuples). The first value of the interval will always be less than the second value. Interval example: [1, 5] is an interval from 1 to 5. The length of this interval is 4. Overlapping Intervals List containing overlapping intervals: [[1,4], [7, 10], [3, 5]] In Python: [(1,4), (7, 10), (3, 5)] The sum of the lengths of these intervals is 7. Since [1, 4] and [3, 5] overlap, we can treat the interval as [1, 5], which has a length of 4. Examples: sumIntervals( [[1,2],[6, 10],[11, 15]] ) =&gt; 9 sumIntervals( [[1,4],[7, 10],[3, 5]] ) =&gt; 7 sumIntervals( [[1,5],[10, 20],[1, 6],[16, 19],[5, 11]] ) =&gt; 19 sumIntervals( [[0, 20],[-100000000, 10],[30, 40]] ) =&gt; 100000030 Tests with large intervals Your algorithm should be able to handle large intervals. All tested intervals are subsets of the range [-1000000000, 1000000000]. Thinking about the problem It seems that there are two ways to approach this problem. First one is iterating through the intervals, putting them into a suitable data structure, filtering out the duplicates (or processing it with some sort of unique-outputting function) and counting the occurrences. This is easier to write but will surely be slow to execute. Another way would be to consider given intervals‚Äô limits, using them as a comparing points between intervals. This requires only iteration and comparison of the start and end points (instead of iterations of whole range of values), which will take a bit more code but will be much faster to run. Brute-force solution (slow performance) def sum_of_intervals(intervals): lista=[] for x in intervals: for y in range(x[0],x[1]): lista.append(y) return len(set(lista)) This iterates the input list two-folds; firstly it iterates the tuples in the list, after that it iterates the range within the tuples and puts them into a list. The code outputs the length of a set ‚Äî list is converted to set to filter out the duplicate values. It is a fast solution to write but it will be slow with larger input values as the time to execute is O(n¬≤). See here for explanation about a Big-Omega notation: Performance-efficient solution In this case, enhancing performance means avoiding iteration through the whole range of intervals. Instead, using the intervals‚Äô limit values and comparing them should solve the problem much more efficiently. def sum_of_intervals(intervals: list) -&gt; int: sum=0 intervals=sorted(intervals) a=intervals[0][0] b=intervals[0][1] sum += b - a if len(set(intervals)) == 1: return sum else: for j in intervals[1:]: if j[0] &gt; b: sum += j[1] - j[0] a = j[0] b = j[1] elif j[0] &gt;= a and j[1] &lt;= b: continue else: sum += j[1] - b b=j[1] return sum The key to devising this solution is detecting whether the input contains overlapping intervals, and if so, how to manage them. The list is sorted so that algorithm iterates through the intervals‚Äô limits properly. After that, if the length of an input data with unique values (taking into account possible tests with repeating the same values) is one, a computation is easy and it returns the difference of the two values at the beginning and the end of interval. If that is not the case, we have three cases: if the second and other tuples‚Äô first values are larger than second value of the first tuple (we split the interval using slicing): then simply subtract them and add the result to the sum variable, if the first value of second tuple onwards is larger than first value of first tuple and the second value of second tuple onwards is smaller than the first tuple‚Äôs second value, don‚Äôt do anything with this case as it needn‚Äôt be counted. For example, if the first tuple is (1,4) and another one is (3,2), it is enough to take into account only first one, as the second is irrelevant from the description of the problem, everything else represents overlapping intervals so the second value represents the boundary of the interval that needs to be compared and subtracted. This solution is much faster as it doesn‚Äôt care whether the input data is in the range of tens, hundreds or millions as it only takes into account end values of intervals: the subtracting operation has the same speed if subtracting 1 or 1 million. On the other hand, since we iterate here only the numbers in the tuples, not their range, only possible limitation would be if we‚Äôd have lists of millions of tuples to check out, which is a non-realistic scenario anyways." }, { "title": "Functions and scopes in Python, Part 1", "url": "/posts/Functions-scopes-Python-pt1/", "categories": "python, functional programming", "tags": "python, functions, scopes, def, return, nonlocal, global", "date": "2022-08-26 00:00:00 +0200", "content": "Functions are integral part of most programming languages. Together with classes in OOP-supported languages, they make the code more readable, concise, usable, maintainable thus greatly enhancing the program design. Programming without functions today would be impossible: developers would have to copy and paste code, to repeat it everywhere, creating so-called spaghetti code which would be unreadable and impossible to maintain. In fact, in Python, understanding how functions work is probably more important than classes comprehension, or at least it is a prerequisite to understanding OOP. Basic theory behind functions A function is way to group statements into one coherent notation defined by its name, which can then be used and run multiple times later. It can actually do more than being run; it could return a value based on a processing of given inputs in some particular way. Furthermore, functions can be nested and called within themselves, which we‚Äôll explore a bit later. Benefits of grouping the code into one statement are: Increase of readability and conciseness, as the code is written once and may be easily implemented (called upon) multiple times, Enhanced usability and maintainability, as changing the function in the one place produces changes everywhere else where is called, Minimizing redundancy thus shortening the code, Splitting complex code into manageable, well-defined parts leads to a better program design, achieving procedural decomposition. Diving into code In Python, functions are created using def statement: def function(par1, par2,‚Ä¶ parN): \tsome code \treturn value # optional The def statement creates a function with dedicated, user-chosen name with one or more parameters that are optional. Function can have a return value, if it computes/produces something when being run. When the interpreter reaches a return statement inside a function, it ends the function‚Äôs call. If there is no return value, the interpreter supposes None and returns it. Other than return, the function can have yield statement, which will be discussed in pt. 2 of this article. When function is being created, it is not being run (called): function assignment does not imply its execution, which is a separate statement. On the function‚Äôs call, arguments are being passed to it (note that the parameter and argument are the same thing, with a commonly accepted linguistic distinction - parameter is what is defined on function‚Äôs declaration, while argument is being passed to the function when it is run): function(a,b): \treturn a+b In Python, the def can be seen as assignment (=) as it assigns a name to the code block within. So, it can be used everywhere where it is possible to assign a variable (more precisely a name, because Python uses names referenced to underlying objects). Therefore, functions can be assigned within the particular module, or inside an if statement (but not inside an iteration statement: for that purposes a lambda can be utilized which will be explained in the part 2 of the article as well). A function can be assigned to a separate variable, for instance: def printing(x): \tprint(x) prnt=printing(‚ÄúHello World‚Äù) In this case function is being called through the assigned name. Consider this function and its calls: function(a,b): \treturn a*b print(function(3, 4)) # outputs 12 (integer) print(function(3, 4.3)) # outputs 12.89999 (float) print(function(3,‚Äôa‚Äô)) # outputs ‚Äòaaa‚Äô (string) Since Python is dynamically typed, it does not care about the types but about interfaces. In the example above, multiplication means either mathematical multiplication, or repeating. The obvious question arises here: how Python knows which operation to apply? The answer is wide-spread polymorphism, which is everywhere in Python. The underlying object‚Äôs interfaces decide how to react on a given operation: if the object has the expected interface (or protocol), it will run. That is the reason why the multiplication runs without problems with various data types (integers, floats, strings), as they internally can manage it. On the other hand, if they don‚Äôt support it, the exception will be raised: def function(x,y): return x+y print(function(3,'a')) Output: TypeError: unsupported operand type(s) for +: 'int' and 'str' Scopes Let‚Äôs consider a simple example: a=22 def number(): a=23 print(a) number(a) Output: 23 Another one: a=22 def number(): a=23 print(a) number() print(a) Output: 23 22 In order to understand the output results of the previous examples, the concept of scope needs to be discussed. It is obvious that the declared (or in Python assigned) variables (names) have their ‚Äòspace‚Äô of validity, that is, there exists an internal mechanism which tells the interpreter when to consider which variable. A certain hierarchy can be seen from the provided examples: it seems that the variables have their defined places of validity. That is scope: it is a system which prevents variables with same names interfering with one another. Scope is directly linked with the functions, as we may have the same-named variable in the root of our program (module) and in the function. Furthermore, as previously mentioned, we can nest multiple functions. Python approaches this problem by defining a namespace ‚Äì literally a place in memory where names reside. The determining factor of how Python treat a name is the place of its assignation in program‚Äôs code. According to the places of assignation, variables in Python adhere a hierarchical order according to which they can be local, nonlocal (or enclosing), global and built-in. Figure 1: Scope hierarchy in Python This scheme gives the answer to the question on how Python knows what variable to use: If the variable (or rather name) is used (but not declared/assigned) within the function, Python will first seek the local scope, then local scope(s) of any enclosing function (nested functions), then global scope, and finally, built-in scope. The interpreter stops at the first place the name is found (if the name isn‚Äôt found at all, it returns an error back to the user). If the name is assigned within a function, Python creates the name in the local scope of that function, with exception if the name is declared global or nonlocal. Additionally, every class creates local scope so the names living there are considered local to that class. If the name is assigned outside of function, it is considered as a global (valid within the file/module), essentially local scope becomes global one. If the name does not exist in the first three categories, Python finally searches built-in names module (builtins). In the current version, there are 156 names inside it and they refer to some errors/exceptions as well as variable/data structures assignment names. Local and global scope From the previous discussion it is clear that the local scope is already defined as name space inside either a function or a class. It is separated from other scopes and as such it is a narrow, limited one. Global scope is however native to the module, that is, to the root of the file itself: a=22 def number(): a=23 return a print(number()) print(a) Output: 23 22 Here we assigned two names, one is global to the file (a=22), other is local to the function (a=23). When the function is called, it considers the first name which is found in the hierarchy, which here is a local one, hence output 23. Name a within the module itself remains unchanged so it outputs 22 as a second value. Now let‚Äôs introduce the global statement and slightly alter the previous code: a=22 def number(): global a a=23 return a print(number()) print(a) Output: 23 23 The global statement overrides the name defined inside the module: here we implicitly tell Python to consider only name which is declared global, regardless of the fact that it has been already declared within the function. Names declared global are global within the module/file, which means their value is the same everywhere ‚Äì bringing advantages but some disadvantages as well. Consider another variant of this code: def number(): global a a=23 return a print(number()) print(a) Now we assigned name only once ‚Äì but it is clear that we can access the name inside or outside the function, that is, anywhere in the module itself. Clear advantage is that we can use global statement when we want local scope (function) to be preserved and used throughout the module. Disadvantages may include unpredicted behavior and hard-to-trace errors, as global statement could be used multiple times: def number(): global a a=23 return a print(number()) global a a=22 print(a) Output: 23 22 Therefore, there is a reason names inside functions have local scope, and names within module have global scope. Additionally, there is a good reason why global statement is implicit, as it produces the global consequences users should be aware of. One last remark about the global statement is that global name always is global inside one module: they can‚Äôt be global in more modules at the same time ‚Äì ‚Äòtruly‚Äô global variables in the C/C++ sense do not exist in Python. Enclosing (nested) scope and nonlocal It is possible to define functions within functions, in that case multiple local scopes are being created: a=22 def n1(): a=23 def n2(): a=24 print(a) n2() n1() Output: 24 In the example above, there are three variables with the same name, all within separate, different respective scopes. More precisely, we have one global scope (where a=22) and two enclosing ones (a=23 and 24, respectively). The output is 24 because second local scope is taken into account only, as the interpreter finds the name in that space first. The nonlocal statement allows scope to become global, but only one step in the upper hierarchy: a=22 def n1(): a=23 def n2(): nonlocal a a=24 n2() return a print(n1()) Output: 24 Without nonlocal statement, this code would output 23, as return considers n1 function space. With nonlocal, it raises the scope one level up. So, it rather acts as a limited global statement, when we want to access/manipulate the value of the name declared within one function, calling it from a function that is one level higher. Summing it all up Taking aforementioned discussion into consideration, we will portray an example of functional design taking into account multiple scopes. Consider a task of finding multiple prime numbers between the given numbers‚Äô interval. The task consists of choosing the start number, an end number and the advancement step, printing out the Boolean values and counting the number of primes found. For example, in the number range of 9 and 20 every number needs to be checked against a prime test. The algorithms here are not important (other than they function properly) nor the output formatting; it is the program design using functions that we want to depict here: import math def is_prime(num): if num&gt;1: for i in range(2,int(math.sqrt(num))+1): if(num%i)==0: return False else: return True else: return False def choose_nos(start,end,step): global lista lista=[] for i in range(start, end, step): lista.append(i) def print_primes(lista): global lista2 lista2=[] for i in lista: print(is_prime(i)) lista2.append(is_prime(i)) def count(lista): global countTrue countTrue=0 for i in lista2: if i==True: countTrue+=1 return countTrue def run(start, end, step): choose_nos(start, end, step) print_primes(lista) print(count(lista)) run(9,20,1) Output: False False True False True False False False True False True 4 Without using functions, the code would be very messy, redundant, probably at least twice longer and pretty much hard to read. Instead, the code was grouped into 5 functions; every function relates to the segment of program that is dealing with particular sub-problem. The last function‚Äôs purpose calls previously assigned functions so the user must only write one line of code to execute it, passing the parameters and getting the result. Notice further how we used global statement on two occasions, enabling the name inside function to be visible to the rest of the code. If we were to change objectives of the code (what the code should output), it would be enough to change it on one place within the particular function and to use it as much times as needed, as the function calls needn‚Äôt be changed. Once set-up, we could import this module into another one and reuse the defined functions outside of the module where they‚Äôve been assigned, which would bring us to the field of modular programming. Conclusion This rather lengthy article outlines basics of functional programming in Python; discusses the elementary syntax and touches upon the rudimentary issues of program design. As successful implementation of functions implies understanding of scopes, the discourse had to be broadened with the explanations of name spaces, scopes and their hierarchy in Python. Rounding up the theory with appropriate examples of program code summarizes this article: its second part will review the yield and lambda statements." }, { "title": "Codewars review - lessons learned after solving 100 challenges", "url": "/posts/codewars-review/", "categories": "competitive coding, codewars", "tags": "competitive coding, codewars", "date": "2022-08-20 00:00:00 +0200", "content": "Coding, like any other skill, needs constant improvement, time investment, commitment and honing. Regardless of the knowledge level you‚Äôre at, if you don‚Äôt practice, your performance in the field will degrade because we humans tend to forget things we don‚Äôt repeat, and even if that‚Äôs not the case, your competition is certainly doing it, so you are forced to do it in order to keep the pace. While there are many ways to practice coding, one of the best ways is to do something that is: fun, instantly verifiable and comparable, motivating to endure and to keep coming back. Since the coding produces an immaterial output, it is perfect for collaboration, contemplating, sharing and discussing on the Internet. During the last 10-15 years, great online learning and collaboration communities emerged which simplified many of the aspects of dealing with the code, starting from learning, exchanging ideas, collaborating on projects and many more useful things. Figure 1: Codewars.com screenshot Codewars platform From the official documentation, Codewars is a ‚Äúplatform that helps you learn, train, and improve your coding skills by solving programming tasks of many types and difficulty levels‚Äù. It has elements of a social network because every user gets its user account with viewable code rank, can contact other users and collaborate with them, even can directly compete, solving some particular problem against the opponent. The core material of the platform is some 8000 coding challenges ranked by various criteria, of which probably the most important is the level of complexity. The user solves the challenges and gets rewarding points for every solved problem. Users are then ranked to each other by total number of points and is clearly visible how one stacks up against others. Solutions of the particular problem are not visible until user solves problem (although user can ‚Äòunlock‚Äô the solution and see the solutions forgoing at the same time the reward points). The harder the problem solved, more points are gained and the rank being higher and faster achieved. Users can also submit their own problems, which includes writing test cases for them, discussing solutions, contributing to code sharing platforms like GitHub, and much more. Figure 2: Screenshot of my Codewars profile Pros It is all about algorithms and data structures. Down to the core, coding is resourceful using of various algorithms and appropriate data structures. Reading the theory or watching live or video presentations of others explaining it is certainly good but not good enough to master the skill. Only when you do it itself through repeating, trial and error and correcting your mistakes you will progress. So, when user solves a particular challenge on Codewars he/she can‚Äôt see the solutions: in order to gain the points, user must successfully pass the tests which is considered the solving of the problem. Problems may range in complexity, but they all include using one or more algorithms, loops, conditional statements, maybe recursion, regex and so on. In fact, you are free to use whatever you want from the toolbox of the particular programming language in order to pass the tests. When it comes to data structures, you‚Äôll obviously use everything that can solve the problem, including arrays, lists, dictionaries, maps, sets and whatever not. You will perform conversions of various data types, transform data structures, manipulate them in resourceful ways and you will learn A LOT about structures‚Äô mutability, data types, possible uses and limitations, best practices, fastest algorithms and much more. That is true for the more advanced developers as well: regardless of the knowledge you may already possess, you WILL be challenged (Bjarne Stroustrup excluded). Very important feature of Codewars is seeing other people‚Äôs solutions. This is BIG, as through reading and analyzing code others have written you will IMENSELY learn. I often found myself in the situation of solving particular problem in a certain way which was at the moment maybe less sound to me, but unable to solve it using the preferred logic. After submitting, I skimmed through the others‚Äô solutions and found the way to solve the challenge in a way that I wanted to. Reading solutions helps VERY MUCH because in you realize that a particular challenge can be solved in a wide variety of ways which opens doors to a new knowledge and points you in the directions you thought prior were impossible. \tTesting comes as a part of the package As every challenge includes a number of tests to check the solution for, you will soon realize that your code needs to pass every included test. If you want to write/submit your own challenges, you will have to include tests as well. Therefore, your understanding about the designing and usage of tests will skyrocket, as you will see various examples of test cases, how they‚Äôre structured, for what particular case they are testing and so on. A software testing is very sought skill on the market so by doing these challenges you will advance in that field as well. \tImportance of trial&amp;error I would argue the attempts and failures in the process of solving something are the most important way of learning anything. Because of abundance of rules, you will fail often. It will surely be frustrating, but through every single fail you will learn, explore the issue, eventually figure out your mistakes and use that knowledge in the next iteration in you favor. You can‚Äôt learn to code only by passive engagement (for instance reading a book); that surely helps but does not provide the possibility for an user to actively solve a problem. Active engagement is the way to go, because only by example that YOU have to approach to, your mind starts actually to work, to think about the underlying knowledge and possible solutions. Codewars and similar platforms shine in this regard. \tEasily-verifiable challenges will activate your logic (thinking and reasoning) brain region(s) This obviously counts for every kind of a challenge, but even more if the challenges can be verifiable. For instance, if you have tried to challenge yourself by reading some algebra book and solving some problem found there, you could end up breaking your brain without any incentive on how actually to check where/why your incorrect solution is wrong. I am NOT saying coding challenges are comparable in some particular way to the mathematical ones, but they are certainly MUCH MORE EASILY verifiable: enter the code, click test, tests are run and you instantly get the results. If they are incorrect, you may rearrange your code to display that key variable, or even using an IDE to visualize the execution of your code. Of course, you‚Äôd do the same for mathematical problems using some computational technique (MATLAB or similar), which boils down again to verification through executing computer code. If you know you can easily evaluate your solution, you know your hardship will not be in vain as you WILL get the feedback which can gradually point you to the right direction in thinking and ultimately finding a correct solution. On a numerous occasion I caught my brain doing the problem-cracking in unexpected times, for instance during the night: subconscious being activated and pointed in the right direction (although I may stand corrected on that assertion but certainly it seems so to me). \tPersistence through healthy addiction Many say that the key to the success in every field is persistence in doing hard things. But hard does need to be boring at the same time. In fact, if we trick the brain to do the hard things in a fun way, it could be very appealing to us. Codewars achieves that, at least for me: it creates a healthy addiction of solving challenges often and on persistent basis. Firstly, everything runs in the browser, you don‚Äôt need to install anything, so you can use it everywhere. Secondly, you are being constantly challenged; since we are naturally curious, you ask yourself what awaits you next. Thirdly, you are starting to advance the concepts and to gain new knowledge, so in connection to the second point, you can always expect to learn something new. So, it creates a new habit, which is for coding enthusiast more valuable and fun than, say, watching TV (which may be occasionally fun but at least for me totally unproductive). Cons \tLacking big picture Algorithms and data structures are only an important but rather small subset of programming, or even taken broader, computer science. There are A LOT of equally important or very important things to fit the big picture in, like databases, software version control systems, GUI design, simulations, networking and system administration, cyber-security and encryption, design of systems, OOP and other programming paradigms, etc. Being good at algorithms and data structures is required, but far from being the ONLY condition for a professional developer. Even programmers with a broad range of technical know-how need to have some decent amount of interhuman and personal skills. Being stuck in a basement doing only coding challenges will NOT get you there. \tMiscellaneous I have one technical issue with Codewars: only basic tests are freely accessible. User can‚Äôt see additional tests which can be sometimes important, particularly when solving more advanced challenges. It is frustrating not to be able to see the test you‚Äôve failed. My experience In a matter of a couple of months, I‚Äôve solved 100 challenges, ranging in complexity from easy to middle ones. I used C# and Python, which are the languages I‚Äôm at the moment particularly interested for. Overall, I am satisfied with the results and plan to use the site in the future as well. My biggest gain was the persistence in the coding practice, which I couldn‚Äôt get from other sources. Conclusion In this article I elaborated on the reasons pro et contra to usage of competitive coding platform Codewars. I liked the experience and its features, at the same time being aware that coding is much more than solving prefabricated problems. Like with any other field, it is the balanced approach that is the most fruit-bearing, so be sure to complement coding challenges to research of other topics computer science consists of." }, { "title": "Understanding recursion - Towers of Hanoi", "url": "/posts/understanding-recursion-towers-hanoi/", "categories": "python, recursion", "tags": "python, recursion, towers of hanoi", "date": "2022-08-09 00:00:00 +0200", "content": "A recursion is problem-solving method which calls itself in a range of calls until the base one. Only the base call is defined with clear solution; other ones are derived from it. A base call finally terminates the recursive function. Recursive techniques can be short and simple, but sometimes hard to figure out the proper setup. There is number of problems that are traditionally solved by recursion. Note that for every solvable problem it is possible to use either recursion or iteration. Here we present famous puzzle called ‚ÄúTowers of Hanoi‚Äù, explain the input premises, define logarithm in pseudo code, and finally craft an implementation of the algorithm (we will use Python). We conclude by evaluating the code, dissecting it step by step. Premises of Towers of Hanoi puzzle Suppose we have three rods, or towers, with a number of disks on one rod. Disks are sorted according to their diameter, from largest one being on the bottom, to smallest one on the top. The goal is to move those disks in the same sorted manner onto another tower, respecting the following rules: The disks are moved in turns. It is allowed to move only one disk per turn. The move consists of moving the one disk from one tower to another one. Towers are marked as a source, destination and auxiliary ones. Their order is not fixed, i.e., all of three can bear any of those three names. It is allowed moving either disk on an empty tower, or, if the tower contains disk(s), only smaller disk may be placed onto bigger one. Figure 1: Generalized depiction of the problem and solution Discussing the possible solutions Recursive problems can be divided to subproblems of similar type. In our problem setup, we label the number of disks as n. The simplest subproblem would be when we have only one disk, i.e., when n=1. Figure 2: Solution where n=1 It is obvious that, in the case of n=1, problem is solvable in one step, by simply moving one disk from the source to the destination tower of choice. Other rules are not taken into account as there is only one disk. Going up, if we have two disks (n=2), we have added some complexity to the problem. Figure 3: Solution where n=2 The number of steps needed for solving is now three, as we now have to consider how disks are stacked onto each other. When n=3, complexity goes further up. Now the manipulation of disks really is starting to get complicated. Figure 4: Solution where n=3 Number of steps now rises to 7. Observing those cases, we can draw some conclusions: the number of cases needed for solving the puzzle is 2^n-1 (which suggests the problem has the exponentially rising complexity), when number of disks is greater than 1, the problem corresponds to the moving of the n-th disk to the destination tower, while n-1 disks are firstly moved to the auxiliary tower in order to be decomposed into single disks which are further moved until they all end up sorted from the smaller to bigger onto a destination tower, the recursive nature of the problem is revealed in the same moving pattern of n-1 disks (regardless of the size of n), which is subdivided into smaller, recursive (sub)patterns of the same nature. Aforementioned conclusions can help us devise the algorithm in pseudo-code: if the n=1, it‚Äôs a base case, move n-1 disks from source to the auxiliary tower first, move nth disk from source to the destination tower, move n-1 disks from auxiliary to destination tower. Whenever the disk moves, algorithm should notify the user, ultimately giving the print-out of the steps taken in order to solve the puzzle. The total number of steps should be at minimum 2^n-1, which can be utilized in order to control the validity of the algorithm. The recursive calls should dismount the calls to itself all up to the base case, which is where n=1. Implementation of algorithm in Python We start by defining the function and its parameters. The parameters we will pass to the function will be, as previously noted number of disks, and the three towers. Using if check we can check for the base case. So, we literally translate the presented pseude-code to Python code: def hanoi(n, source, dest, aux): if n==1: print(source, \" to \", dest) Now we need to define the else cases. It will be the part where the recursive calls are found. else: Hanoi(n-1, source, aux, dest) print(source, \" to \", dest) Hanoi(n-1, aux, dest, source) Combining it all together: def hanoi(n, source, dest, aux): if n==1: print(source, \" to \", dest) else: hanoi(n-1, source, aux, dest) print(source, \" to \", dest) hanoi(n-1, aux, dest, source) Evaluation of the code We test the written function by calling it for cases where n=1 to n=4. hanoi(1,1,3,2) Output: 1 to 3 This is the base case, we get the expected result, as the disk is transferred from the source to destination tower in one step. hanoi(2,1,3,2) Output: 1 to 2 1 to 3 2 to 3 Here we see how the function is calling itself: Figure 5: Visualization of the code where n=2, using https://pythontutor.com/ The total number of steps is 3, which is within the established limit of 2^n-1. hanoi(3,1,3,2) Output: 1 to 3 1 to 2 3 to 2 1 to 3 2 to 1 2 to 3 1 to 3 Figure 6: Visualization of the code where n=3, using https://pythontutor.com/ Again, the total number of steps is within the parameter‚Äôs range. hanoi(4,1,3,2) Output: 1 to 2 1 to 3 2 to 3 1 to 2 3 to 1 3 to 2 1 to 2 1 to 3 2 to 3 2 to 1 3 to 1 2 to 3 1 to 2 1 to 3 2 to 3 The total number of steps is confirmed for this case as well. Conclusion Towers of Hanoi is the famous puzzle which is introduced in the West at the end of the 19th century, yet it was known for centuries before in the Far East. It is one of the prime examples of recursion. If one truly dives deep enough, the beauty of the code calling itself is astonishing: in only a couple of code‚Äôs lines we have defined a universal solution to the problem regardless of the size of n. The iterative solution is more complex and not so intuitive like the recursion one." }, { "title": "Garbage collection in Python", "url": "/posts/garbage-collection-python/", "categories": "python, garbage collection", "tags": "python, garbage collection, reference counting", "date": "2022-08-03 00:00:00 +0200", "content": "When it comes to memory management, the vital task of any developer is taking care of a memory requirements of given program. This assumes allocating and dislocating the memory occupied by the variables and their data. In the early days of programming, it was a manual task so developer solely was responsible for the memory management. It was laborious job, because every single variable needed to be traced from the declaration/assignation to the point where it may be no longer needed in the code. This meant tracing variables‚Äô attributes like scope, life and their interaction within the code, which all needed to be taken into consideration when removing it from the memory. The bugs occurred often and were sometimes hard to trace. This still stands for major languages like C/C++, although there are some add-ins that offer user-configurable memory management techniques. Garbage collection stands for an automatized memory reclaiming process and it is a standard feature (rather than optional add-in) in the high-level languages developed in the last 30 years. This feature relieves the developer from the manual work and applies one or more algorithms, which run automatically, independently from the user. Garbage collector identifies previously allocated memory spaces (therefore called garbage) which are no more referenced by the program‚Äôs variables, and removes them, thus freeing memory. Python‚Äôs garbage collector specifications Python documentation states following: Garbage collection: The process of freeing memory when it is not used anymore. Python performs garbage collection via reference counting and a cyclic garbage collector that is able to detect and break reference cycles. The garbage collector can be controlled using the gc module. From the upper definition we can see that the garbage collection in Python is twofold: Reference counting algorithm takes care of number of references that each object through its life gets. It is an integral part of the Python, it is automatic, constant process running in the background and it can‚Äôt be managed from the user side. Cyclic garbage collector was added later up, when it was realized that the reference counting algorithm can‚Äôt detect some referencing cases such as reference cycles, where either object references itself or more objects are referencing each other. This garbage collector can be directly manipulated, it runs automatically periodically (but not constantly) in the background, and it can be manually switched off. Reference counting Python is structured in a such way that it does not ‚Äúhold‚Äù any data in variables but rather use them just as names which reference to the actual objects that reserve or allocate actual memory space. Every object has a header field that records actual number of references to it. For the details about this model, see following texts: The joys of Dynamic Typing, Part One - Immutables The joys of Dynamic Typing, Part Two - Mutables What reference counting algorithm does is deallocates objects which loose references to the variables (more precisely, when reference count equals zero). Consider following code: Example 1. x = {'price': 'GBP', 'quantity': 'pounds'} y = {'price': 'GBP', 'quantity': 'pounds'} print(x == y) # True print(x is y) # False print(id(x)) # 2295298897536 print(id(y)) # 2295298897792 We created two dictionaries with the same values. Equality check expectedly returns True, because the values are the same. Identity check however prints out False, which tells us that the objects behind those two dictionaries are not the same, which is confirmed if we print out their memory addresses via id() function. Now if we display reference count of those dictionaries, we get following: import sys print(sys.getrefcount(x)) # 2 print(sys.getrefcount(y)) # 2 Number of references is 2 for each object, respectively. One reference is created on assignation, another reference is created by getrefcount() function itself. Example 2. import sys list=[] def func(x): print(sys.getrefcount(x)) func(list) # returns 4 print(sys.getrefcount(list)) # returns 2 We created an empty list and defined a function which returns number of references for the passed argument. When we call the function and pass our empty list, the returned count of references is, maybe surprisingly, four: one reference for assignation of list, one reference for passing an argument, one reference for using function itself, one reference for getrefcount() function On the other side, the reference count for the list itself is two: one reference for assignment, one reference for getrefcount() function. Example 3. import sys x = [] list = [] print(sys.getrefcount(x)) # 2 list = [x] print(sys.getrefcount(x)) # 3 By assigning an object to another object the reference count increases. To sum it up, there are four ways to increase the reference count: assignment operator, argument passing, calling a function or class, inserting an object to another object. The reference count decreases when applying opposite concepts: reassigning the variable to another object, deleting the variable, exiting the function or class instance (Python destroys references with the variable scope local to that code block), deleting an object within another object. There are some things to be considered though. The variable‚Äôs scope is important. If the variable has the local scope (for example, lives only in the function or class), then on exiting from the code block, Python destroys variables with local scope and thus lowers the reference count of their objects in background. Objects are destroyed only when their reference count reaches zero, not on the decrementation of the reference count! The reference count decrementation of one object that is linked to other objects is mutually linked, so when complex object like list or dictionary (containing another objects) is deleted, the reference count for all containing objects is decremented as well. If, however, another object links one or more previously deleted objects, the interpreter will take that into account and update the current reference count of such objects. Variables outside of function/class scope that reside in the program‚Äôs ‚Äúroot‚Äù are global variables. Their reference count is shielded from the garbage collector as they are stored inside a dictionary and live until the termination of Python‚Äôs interpreter. The user can display global variables‚Äô dictionary by using function globals(). Cyclic garbage collector A reference counting algorithm has one major drawback: it can‚Äôt handle reference cycles. If one variable references itself, or two or more objects are referencing each other, such situation is called a cyclic or circular reference. The reference counts stemming from cyclic references can‚Äôt be detected using reference counting algorithm, hence, additional garbage collector‚Äôs algorithm is needed to detect those and to remove them from the memory space. If we assign a variable to itself and delete it afterwards, we can‚Äôt access the reference count as the variable is deleted: Example 4. import sys x = [] x.append(x) print(sys.getrefcount(x)) # reference count=3 del x Since variable x is deleted, we can‚Äôt access it using the getrefcount() function to determine what actually happened after deleting and what is the count of its references. Therefore, we must dig deeper and use code that can access memory address of the deleted object directly. In order to justify usage of another garbage collection algorithm, we need to prove that the reference count algorithm does not detect cyclic references. Consider following situation depicted below. Example 5. import sys import gc import ctypes class PyObject(ctypes.Structure): _fields_=[(\"refcount\", ctypes.c_long)] gc.disable() \"\"\" disable garbage collector to demonstrate that reference remains although we deleted variable\"\"\" x = [] x.append(x) # x references itself print(sys.getrefcount(x)) # reference count=3 x_address=id(x) del x print(PyObject.from_address(x_address).refcount) \"\"\" reference count=1\"\"\" gc.collect() print(PyObject.from_address(x_address).refcount) \"\"\" reference count=0 \"\"\" In this example we used ctypes, a foreign function library which enables using C data types which we will need since we‚Äôll going to access memory addresses directly. The default cyclic garbage collector was disabled, to prevent it from eventual cleaning of the reference counts. Then we assign a memory address of variable x to another variable. Afterwards, we delete the x and print out the reference count of the object that we accessed through its memory address. Since we get one reference returned following is obvious: it is a proof that object still exists, regardless of the prior deletion of the variable x, it is a proof that reference count garbage collector hasn‚Äôt detected the cyclic reference. When we manually start the cyclic garbage collector, we demonstrate that it works as it should because now the reference count that is returned equals zero. As previously noted, the cyclic garbage collector runs automatically but not constantly opposed to the reference count collector, it activates itself occasionally. Another difference is that it is completely manageable and configurable, as it can be accessed within the code by using the gc module. Basically, what cyclic garbage collector does is detection of cyclic references and creation of three generations of variables which have different conditions (thresholds). We won‚Äôt go deeper in this matter here, rather we will provide some insights of how useful the understanding of the garbage collection could be. Usage tips Broadly speaking, it is best leaving all as it is, because the garbage collectors certainly justified their existence through evolutionary, incremental advancement steps during the couple of decades. They apply advanced algorithms which, on average, do satisfying job within the specified parameters. Yet, on some specific cases it is possible to tune the Python‚Äôs cyclic garbage collector, or to disable it altogether. Sometimes is favorable to avoid cyclic references completely, by using weak references instead. Another tip worth mentioning is to take a look on the global variables in the program‚Äôs code as they are untouched by the reference counting. As always, understanding the logic behind the garbage collector reveals a lot about Python‚Äôs internals and can help you to better understand the involved processes, ultimately envisioning better code for your needs. Conclusion This article touches upon the memory management in Python; its automation and garbage collection process. We explained the reasons behind the both implementations of garbage collector in Python, together with valid examples. Additionally, we proved why there is a need for second garbage collector, and finally gave some tips worth considering when designing your code." }, { "title": "The joys of Dynamic Typing, Part Two - Mutables", "url": "/posts/dynamic-typing-pt2/", "categories": "python, dynamic typing", "tags": "python, interpreter, compiler, bytecode", "date": "2022-08-02 00:00:00 +0200", "content": "In the first part of this text, we have seen that the defining characteristic of the Python is the dynamic typing; the possibility of creating variables without their declaration but rather with immediate assignation to the value; the easy-going reassignation to another object type and value. We have made the distinction between immutable and mutable variables and explained the variations when managing immutable variables. Now it‚Äôs the time to refer to mutables and analyze their behavior to get a complete picture of assignment and reference model which is central in Python. Mutable types Mutable are considered following Python‚Äôs objects: Lists. List is a mutable object type in Python which allow storing multiple objects in ordered manner. They are indexed - meaning the order of objects inside list matters and is being tracked internally. Lists are not fixed in size (like static arrays in C languages which Python doesn‚Äôt support); they are changeable; allow duplicate values and are mutable. This ensures an excellent flexibility of their use as the internal manipulations of its contents is automated (for instance, list automatically inserts or deletes particular containing object, rearranges the remaining indexes and the length attribute). Mutability means they can change content in place without creating another list, as we will see in examples. Dictionaries. Dictionary is such data structure which allows storing values in key: value pairs. As in lists, order matters, they are changeable but they don‚Äôt allow duplicates ‚Äì its keys are unique. Dictionaries are mutable as well and they are indexed as well, but through indexing one can access key or values, not both at once. Sets. Sets can store multiple items in a single variable which is different from lists and dictionaries which store multiple objects; they are not ordered and not indexed; and are mutable. Consider following statements: List1=[1,2,3,4] List2=List1 List1=‚ÄùHello World!‚Äù This is an example of shared reference without changes inside a list. List1 and List2 link to the same object in the first two lines, then the List1 references another object (with another type) while List2‚Äôs reference remains unchanged. Note that we can access the lists‚Äô elements through indexing so List1[0]=1, List[1]=2 and so on. Now we introduce the changes inside a list with shared reference: List1=[1,2,3,4] List2=List1 L1[0]=5 This code returns following: List1=[5,2,3,4] # List1 changed in place, there was no new object, the old one is being modified List2=[5,2,3,4] # List2 also changed since it references the same object Figure 1: List‚Äôs mutability, i.e. changes in place result with no additional object being created In the upper example we changed only object within the List1 which resulted in a change of the object‚Äôs value; therefore, no new object needs to be created. This is important to note because if there are multiple references to the list, they all will be changed as well, which may or may not be immediately understandable. So, when structuring the code, pay attention to the shared references as they will change upon the mutable object‚Äôs change. If we wanted different behavior, that is, only change directly affected variables, other with shared references to stay unchanged, we could use some workarounds. For instance, we may copy the value of the list when declaring another one: List1=[1,2,3,4] List2=List1.copy() # using the copy method creates another object # List2=List1[:] using list slicing also creates another object while copying the values List1[0]=5 After those changes, the lists‚Äô printout looks like: List1=[5,2,3,4] # took change in place, references the same object List2=[1,2,3,4] # unchanged, references previously, through copying created object Figure 2: List‚Äôs mutability on in-place change Equality and identity checks in Python As we create variables with references, some maybe shared between them, it is sometimes useful to check whether variables refer to the same object. Such a test could then be used when determining are two variables‚Äô values and their objects are equal or not. The equality is the situation when variables‚Äô values are the same; the identity comprises the fact if the variables point to the same object. Operator == tests for the equality while is operator tests for object identity. When comparing two variables, following is possible: two variables have the same values, or not, two variables have the same identities (point to the same object), or not, variables at the same time have same equality and identity, or not. Examples: A=[10, 20] B=[10, 20] A==B is true, A is B not true. Using checks on integers, there is one thing worth mentioning regarding identity check. Because Python‚Äôs optimizations, if we compare integers‚Äô identity in range -5 till 256, they will return the same object (although they shouldn‚Äôt), as such smaller integers are cached and reused: Figure 3: Identity checks in Python‚Äôs default IDLE shell However, this is not always the case as it seems that in the IDE like Visual Studio Code, interpreter still does some checks as it returns the same identity for the variables with the value of 257, so bear in mind those small grievances when testing your code: Figure 4: Identity check printout in Visual Studio Code Conclusion In this text we‚Äôve shown the behavior of mutables in the sense of their linkages to objects. Firstly, the mutables were defined and their types in Python explained. Secondly, we have explored their basic traits when reassigning their values and summarized the background changes in object‚Äôs references. It is important to test variables for equality and identity so through couple of simple examples we have highlighted some rules and exceptions to those rules, which may be of use when analyzing or debugging the code." }, { "title": "The joys of Dynamic Typing, Part One - Immutables", "url": "/posts/dynamic-typing-pt1/", "categories": "python, dynamic typing", "tags": "python, interpreter, compiler, bytecode", "date": "2022-07-31 00:00:00 +0200", "content": "In traditional, statically-typed languages there is a data type classification which needs implicit declaration to every variable, function and OOP object. If the variable‚Äôs data type is for instance an integer, it needs to be marked as one prior to the value‚Äôs assignation. It is possible to declare a variable in one line, without assigning any value to it. It is however NOT possible to declare a variable and then to re-declare it implicitly as another type, or with another value. Static typing makes things more ordered, clear and defined for the compiler, which reduces the number of needed checks improving the bug-detection rate and compiling time. Downside is the syntactic complexity of a such written code which raises the costs of the development. On the other hand, dynamically-typed languages (of which two probably most used being currently Python and JavaScript) don‚Äôt declare variables, they assign them without mentioning of any data type whatsoever. If we instead of int x=5; simply state x=5 one may ask, how does the Python know that variable x is intended to be used as integer? Names, references, objects In Python, variable is created when we assign a value to it. There is no declaration of variables prior to the assigning value (btw. proper ‚Äúpythonic‚Äù designation is name rather than variable, but we‚Äôll stick to the latter for the sake of clarity, especially in comparison with another languages). A variable only has its name, the type of it doesn‚Äôt exist as the variables are generic in nature and only refer to the particular object created in memory for designated amount of time. This may be baffling to C/C++/Java/C# developers. When a variable is being created with an assignation to some value, the Python immediately creates an object and references a variable to that object. Unassigned variables can‚Äôt exist and an attempt to do so leads to an error. More specifically, the statement: x=5 does the following in Python: interpreter checks the value, conduct some tests and make a conclusion about the type, object with proper type is created to represent the value 5, variable x is created, more specifically name, reference is created to link variable x with its object. Variables are separated from objects and stored in different parts of the memory while being linked together. Simple variables exclusively link to objects, while more complex objects (for instance lists and dictionaries) link to the objects they contain. References or links are strictly enforced; therefore, Python really uses only names for variables and for that matter a name can‚Äôt have a type, but object can. That is the answer to the question how Python knows the type of the variable: it doesn‚Äôt, but it does know the type of the object that variable is being referenced to. Objects are spaces in memory but more than that - they have two header fields: type designator, which actually identifies the type of the object, and reference counter, which holds a number of times the object is being referenced. In Python, references are pointers which point to some address in memory (similar to C pointers), but given the important fact that the Python has garbage collector, since the variables are being automatically referenced, they are automatically unreferenced on certain conditions as well ‚Äì hence, there is no way to use them in such a manner - it is pointless. Because of such structure, it is possible to change the value type of the variable: x=5 x=\"This is string\" x=1.01 Figure 1: The variable x can change values and types because in the background it only refers to various objects which are created and destroyed As described, the variable x doesn‚Äôt really change its type but name x changes it‚Äôs referenced object which changes the type. Variable, i.e., name does not have the type, it is only an internal notion, but objects through type designator recognize the particular type. What happens when we change the variable‚Äôs assignation? Whenever a name being assigned to new object, if the object referenced prior to that loses all references, it is being automatically reclaimed ‚Äì beforementioned garbage collector kicks in. x=5 x=\"This is string\" # if number of references to it equals zero, object with value 5 is being reclaimed x=1.01 # ‚ÄúThis is string\" object is reclaimed Immutability explained This discussion leads us to the notion of immutability. Broadly speaking, in programming jargon, a variable which value can‚Äôt be changed is considered immutable. In Python, immutability is achieved at object level and immutable are core types of objects (partially similar to primitives in C languages): integers, strings, tuples. They can‚Äôt be changed in place; therefore, on change of assignation we need a new object to represent the new value and/or data type. Mutable are considered following Python‚Äôs objects: lists, dictionaries, sets, most custom created OOP objects. Those can be changed in place as we can see in the part 2 of this article. Multiple (shared) references Until now, we have been assigning values with some values, which creates objects in the background. There is another possibility - assigning variable to another variable: x=5 y=x In this case, there is only one referenced object, which has the value 5. When we say y=x, there is no new object, but what happens is creation of another reference to the existing object. Object with value 5 is shared among two variables, but those variables do not have any connection whatsoever (in stark contrast to C languages); via references they point to the very same object. Let‚Äôs broaden this contemplation a bit more, by saying: x=5 y=x x=\"Hello World!\" Figure 2: If we assign one variable for another, those variables will not be linked but point to the same object. If we further assign a new value to the first variable, it will point to new object while second one will remain pointing to the previously referenced one. Here the variable x changes reference, so when the last line is typed, Python creates new object with the name ‚ÄúHello World!‚Äù and new reference to it, while previous one is being dereferenced and removed from the memory (if there are no other references linking to it). Note here that the value of the y remains the same: object with the value 5 is being still referenced by y. The same holds true if we don‚Äôt change type, for instance: x=5 y=x x=6 Figure 3: Same as Figure 2, but without change of the object‚Äôs type The object we initially created holds the value represented with integer 5, when we created new variable, we added another reference to the same object, and finally, as we assigned new value to the variable x, new object is created while first one remains referenced to variable y. Once again, variables in Python only point to objects and do not represent actual memory spaces, therefore it is impossible for immutable objects to change values. Conclusion Dynamic typing is such an important characteristic of Python. It enables the great flexibility and freedom of expression, but it does have its drawbacks as the interpreter must constantly perform numerous checks on runtime, reducing the speed of execution. This article has shown the main behavioral traits of dynamic-typed code in terms of using immutable objects and the corresponding logic behind it. I would argue that while it certainly may be cumbersome to switch from static to dynamic-typed code, but going from dynamic to static scheme is much more demanding, requiring more precision and attention to details." }, { "title": "Behind Python's lines", "url": "/posts/behind-python-lines/", "categories": "python, bytecode", "tags": "python, interpreter, compiler, bytecode", "date": "2022-07-28 00:00:00 +0200", "content": "The final product of the user‚Äôs code in Python is .py file. On its execution, the source file is passed to a Python as an argument, compiled to the intermediary byte-code and then interpreted in the Python Virtual Machine. A lot of happenings, most of which is hidden to the user. Why care, if the code runs appropriately? There are a couple of reasons: Getting the fine-grains will hone your conceptualization and coding skills. As you go deeper to the more fundamental programming concepts, you will learn a lot; open your mind to the new concepts and broaden your views. Start thinking about the code‚Äôs complexity, efficiency and optimization, even if it may sound futile as Python‚Äôs engine is already many times optimized. Those are not only theoretical aspects, but can be practical, depending on the usage. For those who will care about the program speed, there could be possibilities to accelerate your program by an order of magnitude ‚Äì by telling the Python which exactly optimization to apply (and maybe, which one to omit). Academic, professional or personal curiosity to study how things are made. Maybe you want to write your own programming language. Python blends As it turns out, Python has multiple implementations written in different languages. When we say Python, which actually is a specification, we usually think on its standard implementation called CPython and written in C and Python. It serves multiple roles: it is an interpreter, a compiler, a standard library, a parser and much more. There are many other variants, like Jython (Python implemented in Java), IronPython (.NET implementation), PyPy (fast-running Python, interpreted and compiled to optimized machine code), Cython (static-typing support, compiling to C code, really fast), Stackless (implemented in PyPy) and so on. In this text we will be considering CPython. Intermediary code In contrast to C/C++ which compile directly to machine code on a given platform and execute directly on CPU using its native instructions (therefore being fast as it gets, actually only thing that could be faster would be executing assembly instructions), Python compiles the source code into bytecode, which is assembly-alike, intermediary code with its syntax and semantics which is then interpreted in Python‚Äôs virtual machine (not on the CPU). So, Python adds one layer of abstraction between hardware and operating system, executing its bytecode internally on virtual software, achieving universal portability, but loosing speed: while C/C++ code must be compiled prior to run on any particular platform, Python (just like Java) runs platform-independent because it executes virtually. So basically, bytecode is a language hidden behind the Python‚Äôs code and developers usually do not bother with it nor the majority of users probably are aware that it exists. That is one of the reasons why Python is a high-level programming language: through automatization and optimization, it hides many steps from the developer, achieving clear, readable, human-language alike syntax which comes at the price of creating overhead and increasing complexity on its executing side which affects performance. Analyzing bytecode There is a way to disassemble bytecode by using the dis module. https://docs.python.org/3/library/dis.html The dis module disassembles Python source code into internal, individual instructions which are executed by the interpreter. So, for this basic code: def Func(a): print(a) applying dis.dis(Func) we get following output: Figure 1: Disassembling function Func() This is an example of disassembling one function, but it may be a complete .py file as well. This outputted code is the way the Python interpreter sees our code: it consists of internal instructions and their names and addresses which correspond to the original source code. import timeit import dis def fast_compute(): return 100*12*30*24*60*60 def slow_compute(): seconds_hourly=60*60 hours_monthly=30*24 months_yearly=12 years_century=100 return seconds_hourly*hours_monthly*months_yearly*years_century dis.dis(slow_compute) dis.dis(fast_compute) print(timeit.timeit(slow_compute)) print(timeit.timeit(fast_compute)) Output: Figure 2: Disassembling functions slow_compute() and fast_compute() We defined two functions which compute number of seconds in a year, one function uses variables to store the corresponding time frames, while another computes it directly in the function‚Äôs return statement. We then tested the execution time of those functions and provided their corresponding disassembly code. The time of execution for the slow_compute function was 0,23 seconds comparing to the fast_compute one of 0,07 seconds (~3x slower). The slow_compute function has 16 lines of code while fast_compute has only two! The reason for this is that the interpreter knows that product of multiplication does not need to be reevaluated and computed again, while in the slow_function we used a number of variables, each of which needs to be checked in terms of their content as Python is dynamically-typed language which takes additional time when executed in virtual machine. By knowing this we could optimize our code for a three times faster execution, which is an outstanding gain. import timeit import dis def fast_loop(): for i in range(100): pass def slow_loop(): i=0 while i&lt;100: i+=1 dis.dis(slow_loop) print() dis.dis(fast_loop) print(timeit.timeit('slow_loop()', setup='from __main__ import slow_loop')) print(timeit.timeit('fast_loop()', setup='from __main__ import fast_loop')) Output: Figure 3: Disassembling functions slow_loop() and fast_loop() It is noticeable that the for loop is much quicker and with less internal instructions: 1.36 against 4.79 seconds is a huge time difference. This is due to the fact that it is internally optimized to run much faster C code, on the other hand while loop repeats much of its instructions and is interpreted which affects performance. Down to the bytes We could go even deeper and disassemble the already disassembled intermediary code into the stream of bytecode by using the function.code. For instance: def calculate(): return 50**45+18*2/3 print(calculate.__code__.co_code) returns b'd\\x01d\\x02\\x13\\x00d\\x03\\x17\\x00S\\x00' What those bytes stand for? They are low-level specifications of the intermediate instructions we already seen, which are then being fed into the virtual machine and actually executed. So, your code is dismantled into byte representations and then run on virtual machine which does another level of abstraction by separating the bytecode from actual, native code which all CPUs need to be able to produce a valid output. Long way down from writing ‚ÄúHello world‚Äù! Conclusion This article gives a broad overview on Python‚Äôs interpreter internal dealings. It shows that Python, being a high-level language, consists of multiple layers of abstraction which hide many assembly details from the end user, providing an ease of use, readability, portability, automatic optimization and more. The only drawback is running time which, depending on one‚Äôs particular usage, may or may not be the issue." }, { "title": "Algorithms‚Äô complexity analysis - Big-O examples with explanations", "url": "/posts/big-o/", "categories": "algorithms, complexity analysis", "tags": "big-o, algorithms", "date": "2022-07-26 00:00:00 +0200", "content": "As Big-O algorithm‚Äôs time complexity is very common theoretical concept, investing some time in understanding it can pay off when implementing solution to some problem, because finding time complexity presumes deep understanding of the code behind it. Important thing though is not to overdo it, because estimating time complexity of more complex algorithms can be more resource-consuming than the coding itself. Another worthy guideline is to first devise any solution to the problem, then according to the needs and priorities of the current project contemplate about optimization, not the vice-versa. In this article we discuss the Big-O time complexity analysis of 10 algorithm implementations in C#, Java, Python and pseudocode. Example 1. for ( int a=0; i&lt;arrA.length; a++) { for (int b=0; i&lt;arrB.length; b++) { System.out.println(a+\",\"+ b); } } Here we have two loops iterating over the two different arrays. Since we don‚Äôt know anything about those arrays, the total worse time of execution is a product of A and B, O(A*B). If we had two loops iterating within the same array, we‚Äôd had O(n^2). Now consider we have two separate loops that iterate over different arrays: for(int a=0; i&lt;arrA.length; a++)) { System.out.println(a); } for(int b=0; i&lt;arrB.length; b++) { System.out.println(b); } We simply add those loops and say O(A+B). Again, there is no way to simplify under that as the details on arrays A and B are unknown. Analogous, if we‚Äôd had two loops that iterate on the same array, we could then simplify and state: O(A+A) ~ O(A) ‚Äì linear time. Example 2. def Func(n): \ti=1 \tj=1 \twhile j&lt;n: \t\ti+=1 \t\tj+=i \t\tprint(\"#\") In order to calculate the Big-O time in this example, we should ask ourselves under what conditions the while loop exits and what is the number of those steps. Value of i is incremented in each iteration and value of j is sum of previous values of i. The loop terminates when j&lt;n, meaning the loop will terminate when sum of all iterations reaches and overcomes the n, so if we label a total number of iterations as y, we can state: 1+2+‚Ä¶+y&gt;n. We further simplify the statement using the known mathematical rule that the sum of the given numbers‚Äô array is the mean of the product of its last number enlarged by one: y(y+1)/2 &gt; n =&gt; O(sq.root n). Example 3. search 7 in [1, 3, 4, 7, 10, 12, 22, 25, 30] if 7 equals middle of array (10) then return it else if 7 smaller than 10 search in [1, 3,4,7) if 7 is smaller than middle of sub-array((3+4)/2) search in [1,3] else if 7 is bigger than 3.5 search in[4,7] if 7 is smaller than 4 compare else compare to 7 return Here we examine a case of binary search which takes a sorted array (by the way, this is a fine example how sorting reduces problem‚Äôs complexity) and by comparing the search case to the middle array in the iterative turns ultimately finds it (or doesn‚Äôt). It starts with N elements to search in the first iteration, then N/2. N/4 and so on, until the value either is found (either by finding it in the middle of the array or by reducing it to the size of one element ‚Äì the one that is searched for) or isn‚Äôt. So, the total running time is a number of steps until N goes to the 1 by its division by 2: N=9 // N/2 N=4 // N/2 N=2 // N/2 N=1 // N/2 This number is same as if we reverse this and ask how many times we need to multiply 1 by 2 until we reach N: N=1 // N2 N=2 // N2 N=4 // N2 N=9 // N2 That number of times is log, as we search the k in statement 2^k=N =&gt; log(2)16=k. We will discard here the basis of the log and the fact that in this particular example we start with an odd number, both of which isn‚Äôt important for the Big-O estimation. So, in the binary search, worst execution time is O(log n). Example 4. def Func(n): count=0 for i in range(int(n/2), n): # O(n/2) time a=1 while a+n/2&lt;=n: # O(n/2) time b=1 while b&lt;=n: # O(log n) count+=1 b*=2 # because of this a+=1 print(count) Here we have three nested loops, one outer and two inner ones. First two loops execute in n/2 times, third one, while b&lt;=n loop, because of the line b*=2 which reduces its time from linear to logarithmic (halving the number of iterations), is logn. We multiply the final outcome as those are nested loops and have: n/2 ^2 * log n =&gt; O(n^2 logn). Example 5. int Func(int n) { if (n&lt;=1) { return 1; } return Func(n-1)+Func(n-1); } Calling Func(3) corresponds to: Func(3-1)+ Func(3-1)=2(Func(2) Func(2)=Func(1)+Func(1)=2 Therefore, Func(3)=2(Func(2))=4 Func(4)=8, Func(5)=16 and so on ‚Äì we have a clear example of geometric progression, i.e. this is a recursive function which generates multiple instances of itself which are progressing as a powers of 2, therefore the number of those calls defines its complexity, which is equal to 2^N =&gt; O(2^N). Example 6. def Func(n): count=0 if n&lt;=0: return 0 j=1 for i in range(0,n): # n times while j&lt;n: # n-j times const. time j+=1 count+=1 print(count) The function above executes in n*const. time =&gt; O(n). Example 7. void ArrayPrint(int[] arrayA, int[] arrayB) { for (int i= 0; i &lt; arrayA.length; i++) { for (int j = 0; j &lt; arrayB.length; j++) { for (int k = 0; k &lt; 100000; k++) { Console.WriteLine(arrayA[i] + \",\" + arrayB[j]); } } } } Here we have a variation of the example 1., with the addition of one more inner loop which iterates up to a constant number. Regardless of its size, the execution time is constant and we discard it so the total running time remains O(A,B). Example 8. def Func(n): for i in range(0, n//4): # executes n/4 times j=1 while j&lt;=n: # executes n/4 times j+=4 print(\"*\") Total execution time: O(n^2) Example 9. Which of the following statements have the time complexity of O(n)? O(n+p), where p&lt;n/2 O(2n) O(n+logn) O(n+m) Answer: Cases 1, 2 and 3 all have the O(n). Case 1: Since the p&lt;n/2, the contribution of p is guaranteed smaller than n, therefore we can exclude it. Case 2: We can exclude constant 2, as already mentioned. Case 3: logn will always be smaller than n, therefore we exclude it. Now the case 4 can‚Äôt be simplified, as we don‚Äôt know anything about n or m, so its complexity is as stated. Example 10. count=0 def Logs(n): i=1 global count while i&lt;=n: j=n while j&gt;0: j=j//2 # logn count+=1 i*=2 # logn return count Here, the outer loop executes when I doubles, which halves the number of executions, giving us the logx times, as discussed in previous examples. Similarly, the inner loop also has logarithmic time. So, the total complexity is O(log^2n). Conclusion The concept of Big-O notation may be somewhat too abstract to figure out, but through analyzing simple algorithm implementations one should get a grasp on its basic principles. Having acquainted us with a bunch of such examples, we should be in the position to draw conclusions about the worst time complexity of the given algorithm‚Äôs implementation and to compare it with another one. It is a term often found in the computer science books so developers should at least understand the basic concepts behind it." }, { "title": "Algorithms‚Äô complexity analysis - the Big-O Notation", "url": "/posts/algorithm-analysis/", "categories": "algorithms, complexity analysis", "tags": "big-o, algorithms", "date": "2022-07-23 00:00:00 +0200", "content": "As there usually are multiple ways to solve the given problem, given the multiple algorithms‚Äô implementation, one would find their comparation useful. This is what is algorithm analysis all about. Suppose we want to sum the first n integers: user inputs the number and all integers from 1 till n are summed up. We could do it via iteration like this: (pseudocode) create function sum(n) { declare integer sum=0 loop from integer I starting from 1 till n+1 in every iteration add to sum value of i return sum } Or using a mathematic shortcut that tells us the sum of first n numbers is one half of product of number n and its larger follower, n+1, like this: (pseudocode) create function sum(n) { return (n*(n+1))/2 } We don‚Äôt need some specialized knowledge in algorithm analyze to figure out that, in terms of simplicity and efficiency, second implementation will have the decisive difference: it will be faster and simpler to calculate the product of two numbers and to half that then to iterate through a list of numbers, adding each one and summing it up in the last iteration. We could easily measure the execution times in any programming language and we would realize the differences. However, it turns out that the execution time is not a good reference for comparing algorithms because it is dependable upon the computer hardware and software configuration. What we need is an algorithm comparison which is not reliant upon the given programming language, style of the coder, machine/operating system internal timer‚Äôs configuration, etc. Since we have already defined that the main reference should revolve around algorithm‚Äôs executing time, we can deepen the time analysis stating three cases: Worst case: defines the input data with which the algorithm runs with the most time (ie. slowest), Best case: the fastest algorithm‚Äôs run given the input data, and Average case: predicts the average time with the random input. If we are going to exclude the external influences like software and hardware and to focus only on the algorithm‚Äôs complexity (which directly influences running time) in relative terms, only our bet is to link inputs with outputs and to see how the output behaves when input changes. Since we can represent any algorithm with a mathematical expression (assuming here it is a function), our aim is to analyze how the execution time changes when we change the input data. If we search for an element in an array of 100 elements applying brute force through iterating all elements and comparing them with the search term (linear search), execution time will surely be shorter opposed to searching 100.000 elements in the same manner. If we take into account two search algorithms and compare their execution time on different input data (array sizes), we can then derive conclusions on how they perform in terms of their best and worst cases. In this sense, Big-O notation is the upper bound of the given function showing how function (our algorithm) behaves when the argument (input data, variables) tends to enlarge. It is represented as f(x)=O(g(x)), which in computer science translates to function f(x) that approximates given algorithm has an upper bound of output cases in a form of function g(x). That means we defined another function, g(x) which shows the maximum rate of growth for f(x) at larger values of x. Suppose we represent some algorithm with a function: f(x)=3x^2+30x+300 At the lower values of x, certainly up to 10, the constant 300 is the dominant part of the function, but when the x rises in value it tends to dwarf other parts of the equation, so the part 3x^2 becomes one with the most weight. In fact, we can actually strip the 3 and leave only the x^2 as it contributes by far for most growth. So, we can say that function f(x) has an order of magnitude f(x)=x^2, or say O(x^2). So, in the computer science, Big-O notation shows how the given algorithm will perform in the worst possible case and it can therefore be used to approximate the performance of given algorithm, which then can be compared to other one. There is no automatized way to translate algorithm into meaningful Big-O notation. One needs to analyze the parts of the algorithm counting the steps of the computation and to add their time of execution together. There are some useful guidelines in the process: Loops. The execution time of a loop is the execution time of all statements inside a loop multiplied with all iterations. So, execution time = c * x = O(x). Nested loops. Multiplication of execution times of all loops, so in case of one outer and one inner loop we have c * x * x = O(x^2). Consecutive statements. Sum the execution time of each statement in a code block. For instance: int x=10; for (int i=0; i&lt;x, i++): // execution x times { Console.WriteLine(i); // constant time } for (int i=0; i&lt;x, i++): // execution x times { for (int j=0; j&lt;x, j++): // execution x times Console.WriteLine (\"i value: \"+ i+ \" and j value: \"+ j); } which translates to x+c+x^2= O(x^2). If statements. Worst execution time translates to the if test itself plus code after if or else statement, whatever is longer to execute. For example: if x==0 // constant time { Console.WriteLine(\"Incorrect\"); else for (int i=1; i&lt;x; i++) // x times { Console.WriteLine(\"Number is \" + i) // constant time } so, total time = c+x+c=O(x). Logarithmic-complexity statements. If it takes a constant time to reduce the problem size by a fraction, then algorithm is O(logn). For instance: void Log(int x) { int i=1; while (i&lt;=x) { i*=2; Console.WriteLine(i); } } Log(100) Output: 2 4 8 16 32 64 128 The loop exits after n times so we can define 2^k=x =&gt; k=logn (base 2 rather than 10). So, total time here is O(logn). Generally, we can establish the estimates for the commonly used mathematical expressions as follows: Figure 1: Common time complexities Figure 2: Graphical representation of some time complexities One can dive more into mathematics and complexity theory to further study the best and average cases as well as amortized analysis; however, for most approximations and comparations the worst-case algorithm‚Äôs behavior depicted by the Big-O notation suffices. Interpretation of the Big-O notation‚Äôs outcome is also worth discussing. One has to bear in mind constantly that the calculated O(g(x)) is the upper bond of the notation, meaning the worst-possible case. If one algorithm gives O(x) and another O(x^2), that does not mean the first one is faster; rather that the second one will tend to have worse outcome on the large enough input. So, the comparative analysis via Big-O notation will have some drawbacks if one wants to find best running time for given algorithm, and then the analysis must be broadened by introducing another notation, Omega-notation, which we will leave for another time. Conclusion In this article we have shown that we can design self-made tests in our particular programming language to measure the executional timing of our code, however this does have some serious drawbacks as the results are not comparable but rather very dependable upon a number of circumstances. Complexity theory gives us the approximative tools to translate an algorithm into set of mathematical statements and to estimate its complexity giving estimates on each part of the code. Such results can be then compared to other algorithms‚Äô estimates within given mathematical boundaries. This analysis using Big-O notation can give a reasonably detailed picture on the worst-possible case of algorithm‚Äôs complexity." }, { "title": "Python‚Äôs (non-existing) increment/decrement operators", "url": "/posts/Python's-incerement-decrement-operators/", "categories": "python, coding", "tags": "python", "date": "2022-07-22 00:00:00 +0200", "content": "Have you tried using very basic ++/‚Äì operators in Python? Incrementing/decrementing of numeric variables should be routine, right? Actually, it turns out they do not exist, and that they are not necessary. In C(ish) languages, unary operators are commonly used in flow control as iterators, providing the ability of loop‚Äôs progression within specified limits (pre- and postfix difference will be discarded here). So simple, basic for loop in C# looks something like this: (general format) for (initialization; condition; iteration) some code (actual example) int a=5; for (int i=10; i&gt;=a;i--) { Console.WriteLine(i); } Output: 10 9 8 7 6 5 Of course, while loop is the same as for loop, just slightly reconstructed: int a=5; int i=10; while (i&gt;=a) { Console.WriteLine(i); i--; } Output: same as previous example. So unary operator ‚Äò‚Äì‚Äô decrements the integer variable by one. Attempting to use same in Python gives an error because unary operators don‚Äôt exist in Python which may be surprising to some. Sure, instead of i‚Äì in C# we could use i=i-1 or its somewhat shorter version i-=1, no problems whatsoever. Same is possible in Python as well. But, let‚Äôs go a bit deeper to see what is really going behind the scenes in Python when it comes to variables changing their values, and why the nonexistence of unary operators isn‚Äôt a big deal. Statement a+=1 is in Python known as the Augmented Assignment Operator. It (re)assigns, rather than only increments. How is that? If we create an integer, increment it and then check the hex ids, we will realize that new object is created in memory and that the integer‚Äôs reference is changed: a=1 print(id(a)) # 2636855378160 print(hex(id(a))) # 0x265f0d400f0 a+=1 print(id(a)) # 2636855378192 print(hex(id(a))) # 0x265f0d40110 Notice how the both id (unique integer) and its hex address changed after incrementation. That is consequence of the fact that integers are immutable in Python ‚Äì in other words, when we change the integer‚Äôs value, new object is created in the background and the reference now points to that object, while old one is or will be at some point garbage-collected. Both in C# and Java, int datatype is also immutable albeit the internal structure of those languages is a bit different. However, in Python the value is being reassigned to a modified, newly created object in memory. The for loop in Python somewhat differs from other languages as it does not require an iterator value because there is an option to use range() function: for (initialization) in range(condition): some code If we want to output printed integers from 10-5, as in our C# example, we would simply write: for a in range (10, 4, -1): # loop from 10 to 5 (not including 4) with -1 incrementation print(a) Output: 10 9 8 7 6 5 Equivalent while loop in Python is, given the differences in the syntax, essentially same as in C#: i=10 while(i&gt;4): print(i) i-=1 # Conclusion Due to the way of Python‚Äôs internal structure, unary operators ++/‚Äì used as incrementors/decrementors in other languages are not existent. Instead, either we use Augmented Assigned Operators += and -=, while in some cases we can achieve the same using for loop with range() function." }, { "title": "Missing ‚Äòswitch‚Äô Statement? Python 3.10 gets you covered", "url": "/posts/missing-switch-statement/", "categories": "python, coding", "tags": "python, branching", "date": "2022-07-19 00:00:00 +0200", "content": "Most languages support multiple flow control statements, vast majority of them is familiar with if and switch. So, someone coming from C/C++/Java/C# and alike languages might wonder about equivalent statement in Python. First of all, let‚Äôs clarify the subtle difference between those statements in order to see what exactly are we looking for. Consider a simple structure in Python: userInput=1 if (userInput==1): print(\"Addition\") elif (userInput==2): print(\"Subtraction\") elif (userInput==3): print(\"Multiplication\") elif(userInput==4): print(\"Division\") Depending of the value of the variable userInput, we want to achieve different outcomes. Sure, multiple if/elif/else statements are doing the job, but if we‚Äôd had quite a bit more of conditions, the code would look somewhat cumbersome. Instead, one might feel a need to use a switch statement, especially if coming from C(ish) language(s). Implementing switch in C# would look something like: userInput=1; switch (userInput) { case 1: Console.WriteLine('Discounted Price 1=40'); break; case 2: Console.WriteLine(\"Discounted Price 2=35\"); break; default: Console.WriteLine(\"Standard price: 45\") break; } Now, in Python community, the possibility of switch statement implementation was discussed at least in the last 15 years (see this: https://peps.python.org/pep-3103/), until finally, Python 3.10 released in October 2021. implemented something called Structural Pattern Matching (see what‚Äôs new in Python 3.10 here: https://docs.python.org/3/whatsnew/3.10.html) which introduced Match Case statement, so we can write now: match userInput: case 1: print(a+b) case 2: print(a-b) case 3: print(a*b) case 4: print(a/b) Notice how the Match Case does not require a break statement ‚Äì it is implicitly, internally called when the interpreter reaches the first matched case ‚Äì other ones are being ignored. If our code has a default case option ‚Äì that is the option that executes if previous cases weren‚Äôt matched ‚Äì we‚Äôd expect to use a default keyword for that, however Python neatly achieving the same using underscore char as a wildcard pattern ‚Äú_‚Äù which here detects any input and proceeds as a default case. case _: print(\"This is the default case\") We can use logical OR in the same line like this: case 5 | 6 | 7 | 8 | 9: print(\"Invalid input. Please enter the correct input value.\") Data types to match could be usual ones like integers and strings, but also floats (!), so this is possible: test=1.36 match test: case 1.00: print(\"Stock conversion\") case 1.36: print(\"Conversion kw to hp enabled\") Besides primitives, objects could be matched as well. Consider this code: class Car: def __init__(self, engineType): self.engineType=engineType def selector(self): match self.engineType: case 'petrol': print(\"Petrol engine\") case 'diesel': print(\"Diesel engine\") case 'EV' | 'electro': print(\"Electro engine\") car1=Car('petrol') car1.selector() We created a simple class with instance variable (object property) engineType, then we defined a method selector which uses matching of that instance variable. Lastly, we create an object car1 with an argument and call a method which executes its logic, finds matched case and outputs the respective string. This line: match self.engineType actually checks instance variable of the object of type class Car, in our example it checks an instance variable engineType of the object car1. Further possibility is to match multiple patterns, ie. primitives/object (arguments). This code declares method musicPlayer, which is called upon with three arguments, both of which is matched because of the asterisk ‚Äú*‚Äù sign: def musicPlayer(param): match param: case [*genres]: for genre in genres: print(f'We are playing {genre}.') musicPlayer(('rockabilly','country', 'swing')) Conclusion Switch argument was something that some Python developers sought for. Although some disputed the very need for it, as other viable solutions do the job, Python version 3.10 introduced pattern matching, which enables switch case and much more. In this article, we have shown basic usage patterns including simple primitives‚Äô case switching, as well as more advanced and to some developers‚Äô unusual features like usage of wildcards, pattern recognition and object pattern matching. Therefore, case matching is only one subset of the newly introduced features - prior to 2021. it didn‚Äôt exist in Python ecosystem. It is certainly more powerful than switch statements that we commonly use in other main languages." } ]
